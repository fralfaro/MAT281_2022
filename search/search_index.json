{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home MAT281 - Aplicaciones de la Matem\u00e1tica en la Ingenier\u00eda Material El material est\u00e1 disponible en el siguiente repositorio , para obtener el c\u00f3digo de fuente basta con que ejecutes el siguiente comando: https://github.com/fralfaro/MAT281_2022 Identificaci\u00f3n de la asignatura Asignatura : Aplicaciones a la Matem\u00e1tica Sigla : MAT281 Prerrequisitos : Ninguno Unidad Acad\u00e9mica que lo imparte : Deparatamento de matem\u00e1ticas Eje formativo :Ingenier\u00eda Aplicada e Integraci\u00f3n Descripci\u00f3n de la Asignatura El estudiante adquiere competencias/destrezas b\u00e1sicas como Data Scientist (ocupando el lenguaje de programaci\u00f3n Python). Requisitos de entrada Utiliza comprende conceptos b\u00e1sicos de C\u00e1lculo-Algebra, Probabilidad-Estad\u00edstica y Optimizaci\u00f3n. Utiliza comprende conceptos b\u00e1sicos de Python. Contenidos tem\u00e1ticos Toolkit b\u00e1sico del curso Computaci\u00f3n cient\u00edfica Manipulaci\u00f3n de datos Visualizaci\u00f3n Machine Learning Evaluaci\u00f3n Laboratorios Semanal. Individual. Notas: 0, 25, 50, 75, 100. Plazo: final del d\u00eda de clases. Entregas fuera del plazo tienen nota cero (0). Tareas Mensual. Individual. Plazo: \\(T_1\\) : 31 de Octubre 2021. \\(T_2\\) : 12 de Diciembre 2021. Entregas fuera del plazo descuentan 25 puntos por d\u00eda (parte entera). Por ejemplo, un retraso de 15 minutos cuenta como un d\u00eda y descuenta 25 puntos. Proyecto Semestral Plazo: final del curso (28 de Diciembre 2021). Grupal (grupos aleatorios). Entregas fuera del plazo descuentan 25 puntos por d\u00eda (parte entera). Nota Final Sea \\(\\bar{n_l}\\) promedio de notas del laboratorio, \\(\\bar{n_t}\\) : promedio de notas de las tareas, \\(n_p\\) : nota del proyecto. Entonces la nota final del curso ( \\(N_f\\) ) ser\u00e1: \\[ N_f = 0.25\\bar{n_l} + 0.35\\bar{n_t} + 0.4n_p \\] \u00a1Importante! : Todos los entregables se deben subir al repositorio personal del estudiante (en GitHub). Las notas se trataran de actualizar al final de cada mes.","title":"Home"},{"location":"#home","text":"MAT281 - Aplicaciones de la Matem\u00e1tica en la Ingenier\u00eda","title":"Home"},{"location":"#material","text":"El material est\u00e1 disponible en el siguiente repositorio , para obtener el c\u00f3digo de fuente basta con que ejecutes el siguiente comando: https://github.com/fralfaro/MAT281_2022","title":"Material"},{"location":"#identificacion-de-la-asignatura","text":"Asignatura : Aplicaciones a la Matem\u00e1tica Sigla : MAT281 Prerrequisitos : Ninguno Unidad Acad\u00e9mica que lo imparte : Deparatamento de matem\u00e1ticas Eje formativo :Ingenier\u00eda Aplicada e Integraci\u00f3n","title":"Identificaci\u00f3n de la asignatura"},{"location":"#descripcion-de-la-asignatura","text":"El estudiante adquiere competencias/destrezas b\u00e1sicas como Data Scientist (ocupando el lenguaje de programaci\u00f3n Python).","title":"Descripci\u00f3n de la Asignatura"},{"location":"#requisitos-de-entrada","text":"Utiliza comprende conceptos b\u00e1sicos de C\u00e1lculo-Algebra, Probabilidad-Estad\u00edstica y Optimizaci\u00f3n. Utiliza comprende conceptos b\u00e1sicos de Python.","title":"Requisitos de entrada"},{"location":"#contenidos-tematicos","text":"Toolkit b\u00e1sico del curso Computaci\u00f3n cient\u00edfica Manipulaci\u00f3n de datos Visualizaci\u00f3n Machine Learning","title":"Contenidos tem\u00e1ticos"},{"location":"#evaluacion","text":"","title":"Evaluaci\u00f3n"},{"location":"#laboratorios","text":"Semanal. Individual. Notas: 0, 25, 50, 75, 100. Plazo: final del d\u00eda de clases. Entregas fuera del plazo tienen nota cero (0).","title":"Laboratorios"},{"location":"#tareas","text":"Mensual. Individual. Plazo: \\(T_1\\) : 31 de Octubre 2021. \\(T_2\\) : 12 de Diciembre 2021. Entregas fuera del plazo descuentan 25 puntos por d\u00eda (parte entera). Por ejemplo, un retraso de 15 minutos cuenta como un d\u00eda y descuenta 25 puntos.","title":"Tareas"},{"location":"#proyecto","text":"Semestral Plazo: final del curso (28 de Diciembre 2021). Grupal (grupos aleatorios). Entregas fuera del plazo descuentan 25 puntos por d\u00eda (parte entera).","title":"Proyecto"},{"location":"#nota-final","text":"Sea \\(\\bar{n_l}\\) promedio de notas del laboratorio, \\(\\bar{n_t}\\) : promedio de notas de las tareas, \\(n_p\\) : nota del proyecto. Entonces la nota final del curso ( \\(N_f\\) ) ser\u00e1: \\[ N_f = 0.25\\bar{n_l} + 0.35\\bar{n_t} + 0.4n_p \\] \u00a1Importante! : Todos los entregables se deben subir al repositorio personal del estudiante (en GitHub). Las notas se trataran de actualizar al final de cada mes.","title":"Nota Final"},{"location":"lectures/basic_tools/basic_tools/lecture_000_configuraciones/","text":"Configuraciones Git Puede descargar el instalador en la p\u00e1gina oficial (escoger el sistema operativo correspondiente). Para validar si tu instalaci\u00f3n fue correcta, debes ejecutar en la terminal: git --version Usuarios de Windows que no agregaron Git al PATH tendr\u00e1n que utilizar la terminal Git Bash . GitHub Utilizando tu correo institucional puedes registrarte a trav\u00e9s de GitHub Student Developer Pack , con el cual puedes acceder a repositorios privados, entre otras cosas. En caso contrario, puedes crear una cuenta directamente en el sitio oficial . Virtual Enviroments Conda Seguir la instalaci\u00f3n regular de desde la documentaci\u00f3n oficial seg\u00fan tu sistema operativo. Instalar Miniconda Windows : En su men\u00fa de inicio tendr\u00e1n dos nuevos programas Anaconda Prompt y Anaconda Powershell Prompt , pueden ocupar cualquiera. Personalmente prefiero Powershell . Linux : En la instalaci\u00f3n se recomienda agregar conda al PATH . Si cada vez que inicias una terminal vez el texto (base) al comienzo, debes ejecutar conda config --set auto_activate_base false y luego cada vez que quieras utilizar conda debes ejecutar conda activate . Poetry Poetry proporciona un instalador personalizado que instalar\u00e1 poetry aislado del resto de su sistema al vender sus dependencias. Esta es la forma recomendada de instalar poes\u00eda. osx/linux/bashonwindows curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - windows powershell (Invoke-WebRequest -Uri https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py -UseBasicParsing).Content | python - Pycharm Se recomienda ocupar el IDE de Pycharm. Puede descargar el instalador en la p\u00e1gina oficial (escoger el sistema operativo correspondiente). Existen dos versiones: Professional (Pagado) y Comunnity (Gratis). Para efectos de este tutorial con la versi\u00f3n Comunnity ser\u00e1 suficiente. No es estrictamente necesario instalarlo, pero facilita bastante aspecto c\u00f3dificaci\u00f3n con Python y cosas de versionamiento. Entorno de trabajo Clonar el repositorio oficial de este tutorial: git clone https://gitlab.com/FAAM/basic_tools en alguna carpeta que estimes conveniente. Para crear el entorno virtual ejecuta el siguiente comando Conda : Crear ambiente virtual : conda env create -f environment.yml --yes Activar ambiente virtual : conda activate NAME_ENV Poetry : Crear ambiente virtual : poetry install Activar ambiente virtual : poetry shell Felicitaciones, tienes todo lo necesario para triunfar en este curso!","title":"Configuraciones"},{"location":"lectures/basic_tools/basic_tools/lecture_000_configuraciones/#configuraciones","text":"","title":"Configuraciones"},{"location":"lectures/basic_tools/basic_tools/lecture_000_configuraciones/#git","text":"Puede descargar el instalador en la p\u00e1gina oficial (escoger el sistema operativo correspondiente). Para validar si tu instalaci\u00f3n fue correcta, debes ejecutar en la terminal: git --version Usuarios de Windows que no agregaron Git al PATH tendr\u00e1n que utilizar la terminal Git Bash .","title":"Git"},{"location":"lectures/basic_tools/basic_tools/lecture_000_configuraciones/#github","text":"Utilizando tu correo institucional puedes registrarte a trav\u00e9s de GitHub Student Developer Pack , con el cual puedes acceder a repositorios privados, entre otras cosas. En caso contrario, puedes crear una cuenta directamente en el sitio oficial .","title":"GitHub"},{"location":"lectures/basic_tools/basic_tools/lecture_000_configuraciones/#virtual-enviroments","text":"","title":"Virtual Enviroments"},{"location":"lectures/basic_tools/basic_tools/lecture_000_configuraciones/#conda","text":"Seguir la instalaci\u00f3n regular de desde la documentaci\u00f3n oficial seg\u00fan tu sistema operativo. Instalar Miniconda Windows : En su men\u00fa de inicio tendr\u00e1n dos nuevos programas Anaconda Prompt y Anaconda Powershell Prompt , pueden ocupar cualquiera. Personalmente prefiero Powershell . Linux : En la instalaci\u00f3n se recomienda agregar conda al PATH . Si cada vez que inicias una terminal vez el texto (base) al comienzo, debes ejecutar conda config --set auto_activate_base false y luego cada vez que quieras utilizar conda debes ejecutar conda activate .","title":"Conda"},{"location":"lectures/basic_tools/basic_tools/lecture_000_configuraciones/#poetry","text":"Poetry proporciona un instalador personalizado que instalar\u00e1 poetry aislado del resto de su sistema al vender sus dependencias. Esta es la forma recomendada de instalar poes\u00eda. osx/linux/bashonwindows curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - windows powershell (Invoke-WebRequest -Uri https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py -UseBasicParsing).Content | python -","title":"Poetry"},{"location":"lectures/basic_tools/basic_tools/lecture_000_configuraciones/#pycharm","text":"Se recomienda ocupar el IDE de Pycharm. Puede descargar el instalador en la p\u00e1gina oficial (escoger el sistema operativo correspondiente). Existen dos versiones: Professional (Pagado) y Comunnity (Gratis). Para efectos de este tutorial con la versi\u00f3n Comunnity ser\u00e1 suficiente. No es estrictamente necesario instalarlo, pero facilita bastante aspecto c\u00f3dificaci\u00f3n con Python y cosas de versionamiento.","title":"Pycharm"},{"location":"lectures/basic_tools/basic_tools/lecture_000_configuraciones/#entorno-de-trabajo","text":"Clonar el repositorio oficial de este tutorial: git clone https://gitlab.com/FAAM/basic_tools en alguna carpeta que estimes conveniente. Para crear el entorno virtual ejecuta el siguiente comando Conda : Crear ambiente virtual : conda env create -f environment.yml --yes Activar ambiente virtual : conda activate NAME_ENV Poetry : Crear ambiente virtual : poetry install Activar ambiente virtual : poetry shell Felicitaciones, tienes todo lo necesario para triunfar en este curso!","title":"Entorno de trabajo"},{"location":"lectures/basic_tools/basic_tools/lecture_000_intro/","text":"Introducci\u00f3n Sistema Operativo Personalmente recomiendo Linux , en particular distribuciones como Ubuntu, Mint o Fedora por su facilidad a la hora de instalar. En ocasiones las implementaciones en Windows no est\u00e1n completamente integradas e inclusive en ocasiones no est\u00e1n disponibles. Una alternativa es Windows Subsystem for Linux , pero lamentablemente no se asegura un 100% de compatibilidad. En el caso que poseas un equipo con macOS no deber\u00eda haber problema. Interfaz de L\u00ednea de Comandos ( Command Line Interface / CLI) Es un m\u00e9todo que permite a los usuarios interactuar con alg\u00fan programa inform\u00e1tico por medio de l\u00edneas de texto. T\u00edpicamente se hace uso de una terminal/ shell (ver imagen). En el d\u00eda a d\u00eda dentro de la oficina facilita flujo de trabajo. Permite moverse entre manipular directorios y ficheros, instalar/actualizar herramientas, aplicaciones, softwares, etc. Screenshot of a sample bash session in GNOME Terminal 3, Fedora 15. Wikipedia Python Python es un lenguaje de programaci\u00f3n interpretado cuya filosof\u00eda hace hincapi\u00e9 en la legibilidad de su c\u00f3digo. Se trata de un lenguaje de programaci\u00f3n multiparadigma, ya que soporta orientaci\u00f3n a objetos, programaci\u00f3n imperativa y, en menor medida, programaci\u00f3n funcional. Es un lenguaje interpretado, din\u00e1mico y multiplataforma. Las principales librer\u00edas cient\u00edficas a instalar y que ocuparemos durante el curso son: Numpy : Computaci\u00f3n cient\u00edfica. Pandas : An\u00e1lisis de datos. Matplotlib : Visualizaci\u00f3n. Scikit-Learn : Machine Learning Durante el curso se ocupar\u00e1n m\u00e1s librer\u00edas a modo de complementaci\u00f3n (ejemplo, scipy, seaborn, statsmodels ,etc.) Entorno Virtual Problemas recurrentes: - Dependencias de librer\u00edas ( packages ) incompatibles. - Dificultad a la hora de compartir y reproducir resultados, e.g. no conocer las versiones de las librer\u00edas instaladas. - Tener una m\u00e1quina virtual para cada desarrollo es tedioso y costoso. - Miedo constante a instalar algo nuevo y tu script vuelva a funcionar. Soluci\u00f3n Aislar el desarrollo con tal de mejorar la compatibilidad y reproducibilidad de resultados. Para el curso (es recomendable) Package, dependency and environment management for any language\u2014Python, R, Ruby, Lua, Scala, Java, JavaScript, C/ C++, FORTRAN. (Link) \u00bfPor qu\u00e9 Conda? Open Source Gestor de librer\u00edas y entornos virtuales. Compatible con Linux, Windows y macOS. Es agn\u00f3stico al lenguaje de programaci\u00f3n (inicialmente fue desarrollado para Python). Es de f\u00e1cil instalaci\u00f3n y uso. Otras alternativas pip + virtualenv : el primero es el gestor favorito de librer\u00edas de Python y el segundo es un gestos de entornos virtuales, el contra es que es exclusivo de Python. Pipenv o Poetry : librer\u00edas enfocadas al manejo de dependencias (muy recomendables!) Entorno de desarrollo integrado Un entorno de desarrollo integrado , en ingl\u00e9s Integrated Development Environment (IDE) , es una aplicaci\u00f3n inform\u00e1tica que proporciona servicios integrales para facilitarle al desarrollador o programador el desarrollo de software. Normalmente, un IDE consiste de un editor de c\u00f3digo fuente, herramientas de construcci\u00f3n autom\u00e1ticas y un depurador. La mayor\u00eda de los IDE tienen auto-completado inteligente de c\u00f3digo (IntelliSense). Algunos IDE contienen un compilador, un int\u00e9rprete, o ambos, tales como NetBeans y Eclipse; otros no, tales como SharpDevelop y Lazarus. Existen varios IDE populares que sirven para varios lenguajes de programaci+on. En python, el m\u00e1s recomendable es Pycharm . Pycharm PyCharm es un IDE para desarrolladores profesionales. Fue creado por JetBrains, una empresa conocida por crear excelentes herramientas de desarrollo de software. Hay dos versiones de PyCharm: Community : versi\u00f3n gratuita de c\u00f3digo abierto, ligera, buena para Python y desarrollo cient\u00edfico Professional : versi\u00f3n de pago, IDE con todas las funciones con soporte para desarrollo web tambi\u00e9n Observaci\u00f3n Se recomienda que puedan descargar Pycharm (en su versi\u00f3n gratuita) para poder familiarizarse con este tipo de herramientas, aunque el curso est\u00e1 orientado a trabajar sobre la terminal y con jupyter notebook. Project Jupyter Project Jupyter exists to develop open-source software, open-standards, and services for interactive computing across dozens of programming languages.* Jupyter Notebook Es una aplicaci\u00f3n web que permite crear y compartir documentos que contienen c\u00f3digo, ecuaciones, visualizaciones y texto. Entre sus usos se encuentra: Limpieza de datos Transformaci\u00f3n de datos Simulaciones num\u00e9ricas Modelamiendo Estad\u00edstico Visualizaci\u00f3n de Datos Machine Learning Mucho m\u00e1s. Jupyter Lab Es la siguiente generaci\u00f3n de la interfaz de usuario de Project Jupyter . Similar a Jupyter Notebook cuenta con la facilidad de editar archivos .ipynb (notebooks) y heramientas como una terminal, editor de texto, explorador de archivos, etc. Eventualmente Jupyter Lab reemplazar\u00e1 a Jupyter Notebok (aunque la versi\u00f3n estable fue liberada hace algunos meses). Cuenta con una serie de extensiones que puedes instalar (y desarrollar inclurisve. M\u00e1s informaci\u00f3n en: https://github.com/jupyterlab/jupyterlab-demo Puedes probar Jupyter Lab con solo dos clicks! Ingresar a este link: https://github.com/jupyterlab/jupyterlab-demo Hacer click en el icono de binder: Otros Proyectos Entre los m\u00e1s conocidos se encuentran: JupyterHub : Distribuir Jupyter Noterbooks a m\u00faltiples usuarios. nbviewer : Compartir Jupyter Notebooks. Jupyter Book : Construir y publicar libros de t\u00f3picos computacionales. Jupyter Docker Stacks : Im\u00e1genes de Jupyter para utilizar en Docker. Versionamiento de C\u00f3digo Permite compartir el c\u00f3digo fuente de nuestros desarrollos y a la vez mantener un registro de los cambios por los que va pasando. Herramienta m\u00e1s importante y fundamental dentro del desarrollo. Tipos de versionadores de c\u00f3digo: Sistemas Centralizados : Son los m\u00e1s \"tradicionales\", por ejemplo SVN, CVS, etc. Sistemas Distribuidos : son los que est\u00e1n en auge actualmente como: Git, Mercurial, Bazaar, etc. Git Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. Es importante comprender que Git es la herramienta que permite versionar tus proyectos, sin embargo, a la hora de querer aprovechar m\u00e1s funcionalidades, como compartir o sincronizar tus trabajos se hace necesario utilizar servicios externos. Los m\u00e1s famosos son: GitHub GitLab Bitbucket Piensa lo siguiente, cualquiera podr\u00eda implementar un correo electr\u00f3nico entre dos computadoras conectadas entre ellas por LAN pero no conectadas a Internet. Sin embargo la gente utiliza servicios como Gmail, Outlook, etc. con tal de aprovechar de mejor manera las funcionalidades que ofrece la tecnolog\u00eda del correo electr\u00f3nico. Esta es una analog\u00eda perfecta entre las diferencias de Git y los servicios como GitHub o GitLab. GitHub GitHub is a development platform inspired by the way you work. From open source to business, you can host and review code, manage projects, and build software alongside 30 million developers. Gitlab Gitlab is an open source end-to-end software development platform with built-in version control, issue tracking, code review, CI/CD, and more. Self-host GitLab on your own servers, in a container, or on a cloud provider. Bitbucket Bitbucket is more than just Git code management. Bitbucket gives teams one place to plan projects, collaborate on code, test, and deploy. . Resumen Sistema operativo: Cualquiera, sin embargo se recomiendan alternativas basadas en Unix. Lenguaje de programaci\u00f3n: Python Entorno virtual: Conda, preferentemetne a trav\u00e9s de miniconda. Entorno de trabajo: Jupyter Lab. Versionamiento: Git & GitHub.","title":"Introducci\u00f3n"},{"location":"lectures/basic_tools/basic_tools/lecture_000_intro/#introduccion","text":"","title":"Introducci\u00f3n"},{"location":"lectures/basic_tools/basic_tools/lecture_000_intro/#sistema-operativo","text":"Personalmente recomiendo Linux , en particular distribuciones como Ubuntu, Mint o Fedora por su facilidad a la hora de instalar. En ocasiones las implementaciones en Windows no est\u00e1n completamente integradas e inclusive en ocasiones no est\u00e1n disponibles. Una alternativa es Windows Subsystem for Linux , pero lamentablemente no se asegura un 100% de compatibilidad. En el caso que poseas un equipo con macOS no deber\u00eda haber problema.","title":"Sistema Operativo"},{"location":"lectures/basic_tools/basic_tools/lecture_000_intro/#interfaz-de-linea-de-comandos-command-line-interface-cli","text":"Es un m\u00e9todo que permite a los usuarios interactuar con alg\u00fan programa inform\u00e1tico por medio de l\u00edneas de texto. T\u00edpicamente se hace uso de una terminal/ shell (ver imagen). En el d\u00eda a d\u00eda dentro de la oficina facilita flujo de trabajo. Permite moverse entre manipular directorios y ficheros, instalar/actualizar herramientas, aplicaciones, softwares, etc. Screenshot of a sample bash session in GNOME Terminal 3, Fedora 15. Wikipedia","title":"Interfaz de L\u00ednea de Comandos (Command Line Interface / CLI)"},{"location":"lectures/basic_tools/basic_tools/lecture_000_intro/#python","text":"Python es un lenguaje de programaci\u00f3n interpretado cuya filosof\u00eda hace hincapi\u00e9 en la legibilidad de su c\u00f3digo. Se trata de un lenguaje de programaci\u00f3n multiparadigma, ya que soporta orientaci\u00f3n a objetos, programaci\u00f3n imperativa y, en menor medida, programaci\u00f3n funcional. Es un lenguaje interpretado, din\u00e1mico y multiplataforma. Las principales librer\u00edas cient\u00edficas a instalar y que ocuparemos durante el curso son: Numpy : Computaci\u00f3n cient\u00edfica. Pandas : An\u00e1lisis de datos. Matplotlib : Visualizaci\u00f3n. Scikit-Learn : Machine Learning Durante el curso se ocupar\u00e1n m\u00e1s librer\u00edas a modo de complementaci\u00f3n (ejemplo, scipy, seaborn, statsmodels ,etc.)","title":"Python"},{"location":"lectures/basic_tools/basic_tools/lecture_000_intro/#entorno-virtual","text":"Problemas recurrentes: - Dependencias de librer\u00edas ( packages ) incompatibles. - Dificultad a la hora de compartir y reproducir resultados, e.g. no conocer las versiones de las librer\u00edas instaladas. - Tener una m\u00e1quina virtual para cada desarrollo es tedioso y costoso. - Miedo constante a instalar algo nuevo y tu script vuelva a funcionar. Soluci\u00f3n Aislar el desarrollo con tal de mejorar la compatibilidad y reproducibilidad de resultados. Para el curso (es recomendable) Package, dependency and environment management for any language\u2014Python, R, Ruby, Lua, Scala, Java, JavaScript, C/ C++, FORTRAN. (Link) \u00bfPor qu\u00e9 Conda? Open Source Gestor de librer\u00edas y entornos virtuales. Compatible con Linux, Windows y macOS. Es agn\u00f3stico al lenguaje de programaci\u00f3n (inicialmente fue desarrollado para Python). Es de f\u00e1cil instalaci\u00f3n y uso. Otras alternativas pip + virtualenv : el primero es el gestor favorito de librer\u00edas de Python y el segundo es un gestos de entornos virtuales, el contra es que es exclusivo de Python. Pipenv o Poetry : librer\u00edas enfocadas al manejo de dependencias (muy recomendables!)","title":"Entorno Virtual"},{"location":"lectures/basic_tools/basic_tools/lecture_000_intro/#entorno-de-desarrollo-integrado","text":"Un entorno de desarrollo integrado , en ingl\u00e9s Integrated Development Environment (IDE) , es una aplicaci\u00f3n inform\u00e1tica que proporciona servicios integrales para facilitarle al desarrollador o programador el desarrollo de software. Normalmente, un IDE consiste de un editor de c\u00f3digo fuente, herramientas de construcci\u00f3n autom\u00e1ticas y un depurador. La mayor\u00eda de los IDE tienen auto-completado inteligente de c\u00f3digo (IntelliSense). Algunos IDE contienen un compilador, un int\u00e9rprete, o ambos, tales como NetBeans y Eclipse; otros no, tales como SharpDevelop y Lazarus. Existen varios IDE populares que sirven para varios lenguajes de programaci+on. En python, el m\u00e1s recomendable es Pycharm .","title":"Entorno de desarrollo integrado"},{"location":"lectures/basic_tools/basic_tools/lecture_000_intro/#pycharm","text":"PyCharm es un IDE para desarrolladores profesionales. Fue creado por JetBrains, una empresa conocida por crear excelentes herramientas de desarrollo de software. Hay dos versiones de PyCharm: Community : versi\u00f3n gratuita de c\u00f3digo abierto, ligera, buena para Python y desarrollo cient\u00edfico Professional : versi\u00f3n de pago, IDE con todas las funciones con soporte para desarrollo web tambi\u00e9n","title":"Pycharm"},{"location":"lectures/basic_tools/basic_tools/lecture_000_intro/#observacion","text":"Se recomienda que puedan descargar Pycharm (en su versi\u00f3n gratuita) para poder familiarizarse con este tipo de herramientas, aunque el curso est\u00e1 orientado a trabajar sobre la terminal y con jupyter notebook.","title":"Observaci\u00f3n"},{"location":"lectures/basic_tools/basic_tools/lecture_000_intro/#project-jupyter","text":"Project Jupyter exists to develop open-source software, open-standards, and services for interactive computing across dozens of programming languages.*","title":"Project Jupyter"},{"location":"lectures/basic_tools/basic_tools/lecture_000_intro/#jupyter-notebook","text":"Es una aplicaci\u00f3n web que permite crear y compartir documentos que contienen c\u00f3digo, ecuaciones, visualizaciones y texto. Entre sus usos se encuentra: Limpieza de datos Transformaci\u00f3n de datos Simulaciones num\u00e9ricas Modelamiendo Estad\u00edstico Visualizaci\u00f3n de Datos Machine Learning Mucho m\u00e1s.","title":"Jupyter Notebook"},{"location":"lectures/basic_tools/basic_tools/lecture_000_intro/#jupyter-lab","text":"Es la siguiente generaci\u00f3n de la interfaz de usuario de Project Jupyter . Similar a Jupyter Notebook cuenta con la facilidad de editar archivos .ipynb (notebooks) y heramientas como una terminal, editor de texto, explorador de archivos, etc. Eventualmente Jupyter Lab reemplazar\u00e1 a Jupyter Notebok (aunque la versi\u00f3n estable fue liberada hace algunos meses). Cuenta con una serie de extensiones que puedes instalar (y desarrollar inclurisve. M\u00e1s informaci\u00f3n en: https://github.com/jupyterlab/jupyterlab-demo Puedes probar Jupyter Lab con solo dos clicks! Ingresar a este link: https://github.com/jupyterlab/jupyterlab-demo Hacer click en el icono de binder:","title":"Jupyter Lab"},{"location":"lectures/basic_tools/basic_tools/lecture_000_intro/#otros-proyectos","text":"Entre los m\u00e1s conocidos se encuentran: JupyterHub : Distribuir Jupyter Noterbooks a m\u00faltiples usuarios. nbviewer : Compartir Jupyter Notebooks. Jupyter Book : Construir y publicar libros de t\u00f3picos computacionales. Jupyter Docker Stacks : Im\u00e1genes de Jupyter para utilizar en Docker.","title":"Otros Proyectos"},{"location":"lectures/basic_tools/basic_tools/lecture_000_intro/#versionamiento-de-codigo","text":"Permite compartir el c\u00f3digo fuente de nuestros desarrollos y a la vez mantener un registro de los cambios por los que va pasando. Herramienta m\u00e1s importante y fundamental dentro del desarrollo. Tipos de versionadores de c\u00f3digo: Sistemas Centralizados : Son los m\u00e1s \"tradicionales\", por ejemplo SVN, CVS, etc. Sistemas Distribuidos : son los que est\u00e1n en auge actualmente como: Git, Mercurial, Bazaar, etc.","title":"Versionamiento de C\u00f3digo"},{"location":"lectures/basic_tools/basic_tools/lecture_000_intro/#git","text":"Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. Es importante comprender que Git es la herramienta que permite versionar tus proyectos, sin embargo, a la hora de querer aprovechar m\u00e1s funcionalidades, como compartir o sincronizar tus trabajos se hace necesario utilizar servicios externos. Los m\u00e1s famosos son: GitHub GitLab Bitbucket Piensa lo siguiente, cualquiera podr\u00eda implementar un correo electr\u00f3nico entre dos computadoras conectadas entre ellas por LAN pero no conectadas a Internet. Sin embargo la gente utiliza servicios como Gmail, Outlook, etc. con tal de aprovechar de mejor manera las funcionalidades que ofrece la tecnolog\u00eda del correo electr\u00f3nico. Esta es una analog\u00eda perfecta entre las diferencias de Git y los servicios como GitHub o GitLab.","title":"Git"},{"location":"lectures/basic_tools/basic_tools/lecture_000_intro/#github","text":"GitHub is a development platform inspired by the way you work. From open source to business, you can host and review code, manage projects, and build software alongside 30 million developers.","title":"GitHub"},{"location":"lectures/basic_tools/basic_tools/lecture_000_intro/#gitlab","text":"Gitlab is an open source end-to-end software development platform with built-in version control, issue tracking, code review, CI/CD, and more. Self-host GitLab on your own servers, in a container, or on a cloud provider.","title":"Gitlab"},{"location":"lectures/basic_tools/basic_tools/lecture_000_intro/#bitbucket","text":"Bitbucket is more than just Git code management. Bitbucket gives teams one place to plan projects, collaborate on code, test, and deploy. .","title":"Bitbucket"},{"location":"lectures/basic_tools/basic_tools/lecture_000_intro/#resumen","text":"Sistema operativo: Cualquiera, sin embargo se recomiendan alternativas basadas en Unix. Lenguaje de programaci\u00f3n: Python Entorno virtual: Conda, preferentemetne a trav\u00e9s de miniconda. Entorno de trabajo: Jupyter Lab. Versionamiento: Git & GitHub.","title":"Resumen"},{"location":"lectures/basic_tools/basic_tools/lecture_011_os/","text":"Linux Introducci\u00f3n GNU/Linux es la denominaci\u00f3n t\u00e9cnica y generalizada que reciben una serie de sistemas operativos de tipo Unix, que tambi\u00e9n suelen ser de c\u00f3digo abierto, multiplataforma, multiusuario y multitarea.Estos sistemas operativos est\u00e1n formados mediante la combinaci\u00f3n de varios proyectos, entre los cuales destaca el entorno GNU, encabezado por el programador estadounidense Richard Stallman junto a la Free Software Foundation, una fundaci\u00f3n cuyo prop\u00f3sito es difundir el software libre (Open source), as\u00ed como tambi\u00e9n el n\u00facleo de sistema operativo conocido como \u00abLinux\u00bb , encabezado por el programador finland\u00e9s Linus Torvalds . Comandos B\u00e1sicos (por l\u00ednea de comando) comando pwd Usa el comando pwd para encontrar la ruta del directorio (carpeta) de trabajo actual en el que te encuentras. El comando devolver\u00e1 una ruta absoluta (completa), que es b\u00e1sicamente una ruta de todos los directorios que comienzan con una barra diagonal (/) Un ejemplo de una ruta absoluta es /home/nombredeusuario . !pwd comando cd Para navegar por los archivos y directorios de Linux, usa el comando cd. Te pedir\u00e1 la ruta completa o el nombre del directorio, dependiendo del directorio de trabajo actual en el que te encuentres. Supongamos que est\u00e1s en /home/nombredeusuario/Documentos y deseas ir a Fotos, un subdirectorio de Documentos. Para hacerlo, simplemente escribe el siguiente comando: cd Fotos. Otro escenario es si deseas ir a un directorio completamente nuevo, por ejemplo, /home/nombredeusuario/Peliculas . En este caso, debes escribir cd seguido de la ruta absoluta del directorio: cd /home/nombredeusuario/Peliculas . Hay algunos atajos para ayudarte a navegar r\u00e1pidamente: cd .. (con dos puntos) para ir un directorio hacia arriba cd para ir directamente a la carpeta de inicio Como nota al margen, el shell de Linux distingue entre may\u00fasculas y min\u00fasculas. Por lo tanto, debes escribir el nombre del directorio de forma exacta. !cd /home/fralfaro/PycharmProjects && pwd !cd .. && pwd !cd && pwd comando ls El comando ls se usa para ver el contenido de un directorio. Por defecto, este comando mostrar\u00e1 el contenido de tu directorio de trabajo actual. Si deseas ver el contenido de otros directorios, escribe ls y luego la ruta del directorio. Por ejemplo, ingresa ls /home/nombredeusuario/Documentos para ver el contenido de Documentos. Hay variaciones que puedes usar con el comando ls: ls -R tambi\u00e9n listar\u00e1 todos los archivos en los subdirectorios ls -a mostrar\u00e1 los archivos ocultos ls -al listar\u00e1 los archivos y directorios con informaci\u00f3n detallada como los permisos, el tama\u00f1o, el propietario, etc. !ls !ls -R !ls -a !ls -al comando cat Se utiliza para listar el contenido de un archivo en la salida est\u00e1ndar (sdout). Para ejecutar este comando, escribe cat seguido del nombre del archivo y su extensi\u00f3n. Por ejemplo: cat archivo.txt. Aqu\u00ed hay otras formas de usar el comando cat: cat > nombredearchivo crea un nuevo archivo. cat nombredearchivo1 nombredearchivo2>nombredearchivo3 une dos archivos (1 y 2) y almacena la salida de ellos en un nuevo archivo (3) convertir un archivo a may\u00fasculas o min\u00fasculas, cat nombredearchivo | tr a-z A-Z> salida.txt !cat archivo.txt comando cp Usa el comando cp para copiar archivos del directorio actual a un directorio diferente. Por ejemplo: !cp archivo.txt archivo_02.txt comando mv El uso principal del comando mv es mover archivos, aunque tambi\u00e9n se puede usar para cambiar el nombre de los archivos. Los argumentos en mv son similares al comando cp. Debes escribir mv, el nombre del archivo y el directorio destino. !mv archivo_02.txt linux Para cambiar el nombre de los archivos, el comando de Linux es mv nombreviejo.ext nombrenuevo.ext comando mkdir Usa el comando mkdir para crear un nuevo directorio: si escribes mkdir Musica, crear\u00e1 un directorio llamado Musica. Tambi\u00e9n hay comandos adicionales de mkdir: Para generar un nuevo directorio dentro de otro directorio, usa este comando b\u00e1sico de Linux mkdir Musica/Nuevoarchivo Usa la opci\u00f3n p (padres) para crear un directorio entre dos directorios existentes. Por ejemplo, mkdir -p Musica/2020/Nuevoarchivo crear\u00e1 el nuevo archivo \u00ab2020\u00bb. !mkdir Carpeta !mkdir Carpeta/nueva_carpeta comando touch El comando touch te permite crear un nuevo archivo en blanco a trav\u00e9s de la l\u00ednea de comando de Linux. Como ejemplo, ingresa touch /home/nombredeusuario/Documentos/Web.html para crear un archivo HTML titulado Web en el directorio Documentos. !touch Web.html !ls comando rmdir Si necesitas eliminar un directorio, usa el comando rmdir. Sin embargo, rmdir solo te permite eliminar directorios vac\u00edos. !rmdir Carpeta !rmdir Carpeta/nueva_carpeta comando rm El comando rm se usa para eliminar directorios y el contenido dentro de ellos. Si solo deseas eliminar el directorio, como alternativa a rmdir, usa rm -r. Nota : Ten mucho cuidado con este comando y verifica en qu\u00e9 directorio te encuentras. Este comando elimina todo y no se puede deshacer. !rm -r Carpeta !rm Web.html linux/archivo_02.txt Referencia The Ultimate Linux Command Line Guide - Full Bash Tutorial 35 comandos b\u00e1sicos de Linux que todo usuario debe saber","title":"Linux"},{"location":"lectures/basic_tools/basic_tools/lecture_011_os/#linux","text":"","title":"Linux"},{"location":"lectures/basic_tools/basic_tools/lecture_011_os/#introduccion","text":"GNU/Linux es la denominaci\u00f3n t\u00e9cnica y generalizada que reciben una serie de sistemas operativos de tipo Unix, que tambi\u00e9n suelen ser de c\u00f3digo abierto, multiplataforma, multiusuario y multitarea.Estos sistemas operativos est\u00e1n formados mediante la combinaci\u00f3n de varios proyectos, entre los cuales destaca el entorno GNU, encabezado por el programador estadounidense Richard Stallman junto a la Free Software Foundation, una fundaci\u00f3n cuyo prop\u00f3sito es difundir el software libre (Open source), as\u00ed como tambi\u00e9n el n\u00facleo de sistema operativo conocido como \u00abLinux\u00bb , encabezado por el programador finland\u00e9s Linus Torvalds .","title":"Introducci\u00f3n"},{"location":"lectures/basic_tools/basic_tools/lecture_011_os/#comandos-basicos-por-linea-de-comando","text":"","title":"Comandos B\u00e1sicos (por l\u00ednea de comando)"},{"location":"lectures/basic_tools/basic_tools/lecture_011_os/#comando-pwd","text":"Usa el comando pwd para encontrar la ruta del directorio (carpeta) de trabajo actual en el que te encuentras. El comando devolver\u00e1 una ruta absoluta (completa), que es b\u00e1sicamente una ruta de todos los directorios que comienzan con una barra diagonal (/) Un ejemplo de una ruta absoluta es /home/nombredeusuario . !pwd","title":"comando pwd"},{"location":"lectures/basic_tools/basic_tools/lecture_011_os/#comando-cd","text":"Para navegar por los archivos y directorios de Linux, usa el comando cd. Te pedir\u00e1 la ruta completa o el nombre del directorio, dependiendo del directorio de trabajo actual en el que te encuentres. Supongamos que est\u00e1s en /home/nombredeusuario/Documentos y deseas ir a Fotos, un subdirectorio de Documentos. Para hacerlo, simplemente escribe el siguiente comando: cd Fotos. Otro escenario es si deseas ir a un directorio completamente nuevo, por ejemplo, /home/nombredeusuario/Peliculas . En este caso, debes escribir cd seguido de la ruta absoluta del directorio: cd /home/nombredeusuario/Peliculas . Hay algunos atajos para ayudarte a navegar r\u00e1pidamente: cd .. (con dos puntos) para ir un directorio hacia arriba cd para ir directamente a la carpeta de inicio Como nota al margen, el shell de Linux distingue entre may\u00fasculas y min\u00fasculas. Por lo tanto, debes escribir el nombre del directorio de forma exacta. !cd /home/fralfaro/PycharmProjects && pwd !cd .. && pwd !cd && pwd","title":"comando cd"},{"location":"lectures/basic_tools/basic_tools/lecture_011_os/#comando-ls","text":"El comando ls se usa para ver el contenido de un directorio. Por defecto, este comando mostrar\u00e1 el contenido de tu directorio de trabajo actual. Si deseas ver el contenido de otros directorios, escribe ls y luego la ruta del directorio. Por ejemplo, ingresa ls /home/nombredeusuario/Documentos para ver el contenido de Documentos. Hay variaciones que puedes usar con el comando ls: ls -R tambi\u00e9n listar\u00e1 todos los archivos en los subdirectorios ls -a mostrar\u00e1 los archivos ocultos ls -al listar\u00e1 los archivos y directorios con informaci\u00f3n detallada como los permisos, el tama\u00f1o, el propietario, etc. !ls !ls -R !ls -a !ls -al","title":"comando ls"},{"location":"lectures/basic_tools/basic_tools/lecture_011_os/#comando-cat","text":"Se utiliza para listar el contenido de un archivo en la salida est\u00e1ndar (sdout). Para ejecutar este comando, escribe cat seguido del nombre del archivo y su extensi\u00f3n. Por ejemplo: cat archivo.txt. Aqu\u00ed hay otras formas de usar el comando cat: cat > nombredearchivo crea un nuevo archivo. cat nombredearchivo1 nombredearchivo2>nombredearchivo3 une dos archivos (1 y 2) y almacena la salida de ellos en un nuevo archivo (3) convertir un archivo a may\u00fasculas o min\u00fasculas, cat nombredearchivo | tr a-z A-Z> salida.txt !cat archivo.txt","title":"comando cat"},{"location":"lectures/basic_tools/basic_tools/lecture_011_os/#comando-cp","text":"Usa el comando cp para copiar archivos del directorio actual a un directorio diferente. Por ejemplo: !cp archivo.txt archivo_02.txt","title":"comando cp"},{"location":"lectures/basic_tools/basic_tools/lecture_011_os/#comando-mv","text":"El uso principal del comando mv es mover archivos, aunque tambi\u00e9n se puede usar para cambiar el nombre de los archivos. Los argumentos en mv son similares al comando cp. Debes escribir mv, el nombre del archivo y el directorio destino. !mv archivo_02.txt linux Para cambiar el nombre de los archivos, el comando de Linux es mv nombreviejo.ext nombrenuevo.ext","title":"comando mv"},{"location":"lectures/basic_tools/basic_tools/lecture_011_os/#comando-mkdir","text":"Usa el comando mkdir para crear un nuevo directorio: si escribes mkdir Musica, crear\u00e1 un directorio llamado Musica. Tambi\u00e9n hay comandos adicionales de mkdir: Para generar un nuevo directorio dentro de otro directorio, usa este comando b\u00e1sico de Linux mkdir Musica/Nuevoarchivo Usa la opci\u00f3n p (padres) para crear un directorio entre dos directorios existentes. Por ejemplo, mkdir -p Musica/2020/Nuevoarchivo crear\u00e1 el nuevo archivo \u00ab2020\u00bb. !mkdir Carpeta !mkdir Carpeta/nueva_carpeta","title":"comando mkdir"},{"location":"lectures/basic_tools/basic_tools/lecture_011_os/#comando-touch","text":"El comando touch te permite crear un nuevo archivo en blanco a trav\u00e9s de la l\u00ednea de comando de Linux. Como ejemplo, ingresa touch /home/nombredeusuario/Documentos/Web.html para crear un archivo HTML titulado Web en el directorio Documentos. !touch Web.html !ls","title":"comando touch"},{"location":"lectures/basic_tools/basic_tools/lecture_011_os/#comando-rmdir","text":"Si necesitas eliminar un directorio, usa el comando rmdir. Sin embargo, rmdir solo te permite eliminar directorios vac\u00edos. !rmdir Carpeta !rmdir Carpeta/nueva_carpeta","title":"comando rmdir"},{"location":"lectures/basic_tools/basic_tools/lecture_011_os/#comando-rm","text":"El comando rm se usa para eliminar directorios y el contenido dentro de ellos. Si solo deseas eliminar el directorio, como alternativa a rmdir, usa rm -r. Nota : Ten mucho cuidado con este comando y verifica en qu\u00e9 directorio te encuentras. Este comando elimina todo y no se puede deshacer. !rm -r Carpeta !rm Web.html linux/archivo_02.txt","title":"comando rm"},{"location":"lectures/basic_tools/basic_tools/lecture_011_os/#referencia","text":"The Ultimate Linux Command Line Guide - Full Bash Tutorial 35 comandos b\u00e1sicos de Linux que todo usuario debe saber","title":"Referencia"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/","text":"Python Nomenclatura b\u00e1sica Hola mundo! Escribamos nuestro primer programa de Python, \"\u00a1Hola, mundo!\". Es un programa simple que imprime Hello World! en el dispositivo de salida est\u00e1ndar (pantalla). # imprimir \"Hola Mundo!\" print ( \"Hola Mundo!\" ); Variables Una variable es una ubicaci\u00f3n con nombre utilizada para almacenar datos en la memoria. Aqu\u00ed hay un ejemplo: # crear e imprimir variables a = 5 print ( \"a =\" , 5 ) a = \"cinco\" print ( \"a =\" , a ) Operadores b\u00e1sico Los operadores son s\u00edmbolos especiales que realizan operaciones en operandos (variables y valores). Hablemos de operadores aritm\u00e9ticos y de asignaci\u00f3n en esta parte. # operaciones basicas x = 14 y = 4 # suma print ( 'x + y =' , x + y ) # Output: x + y = 18 # resta print ( 'x - y =' , x - y ) # Output: x - y = 10 # multiplicacion print ( 'x * y =' , x * y ) # Output: x * y = 56 # division print ( 'x / y =' , x / y ) # Output: x / y = 3.5 # cuociente print ( 'x // y =' , x // y ) # Output: x // y = 3 # resto print ( 'x % y =' , x % y ) # Output: x % y = 2 Operadores de asignaci\u00f3n Los operadores de asignaci\u00f3n se utilizan para asignar valores a las variables. Ya has visto el uso de = operator. Probemos algunos operadores de asignaci\u00f3n m\u00e1s. # operadores de asignacion x = 5 # x += 5 ----> x = x + 5 x += 5 print ( x ) # Output: 10 # x /= 5 ----> x = x / 5 x /= 5 print ( x ) # Output: 2.0 Inputs por el usuario En Python, puede usar la funci\u00f3n input() para tomar la entrada del usuario. Por ejemplo: # inputs por el usuario inputString = input ( 'Escriba una oracion:' ) print ( 'Su oracion es:' , inputString ) Conversi\u00f3n de tipo The process of converting the value of one data type (integer, string, float, etc.) to another is called type conversion. Python has two types of type conversion. Impl\u00edcita # correcto num_int = 123 # integer type num_flo = 1.23 # float type num_new = num_int + num_flo print ( \"Valor de num_new:\" , num_new ) print ( \"tipo de datos de num_new:\" , type ( num_new )) # incorrecto num_int = 123 # int type num_str = \"456\" # str type print ( num_int + num_str ) Expl\u00edcito num_int = 123 # int type num_str = \"456\" # str type # explicitly converted to int type num_str = int ( num_str ) print ( num_int + num_str ) Python tipos num\u00e9ricos Python admite enteros, n\u00fameros de coma flotante y n\u00fameros complejos. Se definen como int, float y clase compleja en Python. Adem\u00e1s de eso, los booleanos: verdadero y falso son un subtipo de enteros. print ( type ( 5 )) # Output: <class> print ( type ( 5.0 )) # <class> c = 5 + 3 j print ( type ( c )) Manejo de excepciones de Python Los errores que se producen en tiempo de ejecuci\u00f3n se denominan excepciones. Ocurren, por ejemplo, cuando un archivo que intentamos abrir no existe FileNotFoundError , dividiendo un n\u00famero por cero ZeroDivisionError , etc. Si no se manejan las excepciones, se escupe un mensaje de error y nuestro programa se detiene repentinamente e inesperadamente. En Python, las excepciones se pueden manejar usando la declaraci\u00f3n try. Cuando se detectan excepciones, depende de usted qu\u00e9 operador realizar. # importar modulo sys import sys lista_random = [ 'a' , 0 , 2 ] for elemento in lista_random : try : print ( \"La entrada es \" , elemento ) r = 1 / int ( elemento ) break except : print ( \"Oops! ocurrio un\" , sys . exc_info ()[ 0 ]) print ( \"Siguiente entrada.\" ) print () print ( \"El reciproco de\" , elemento , \"es\" , r ) Algunas Excepciones Comunes NameError : Esta excepci\u00f3n es levantada cuando el programa no puede encontrar un nombre local o global. El nombre que podr\u00eda no ser encontrado est\u00e1 incluido en el mensaje de error. TypeError : Esta excepci\u00f3n es levantada cuando una funci\u00f3n se le pasa un objeto del tipo inapropiado como su argumento. M\u00e1s detalles sobre el tipo incorrecto son proporcionados en el mensaje de error. ValueError : Esta excepci\u00f3n ocurre cuando un argumento de funci\u00f3n tiene el tipo correcto pero un valor inapropiado. NotImplementedError : Esta excepci\u00f3n es levantada cuando se supone que un objeto apoye una operaci\u00f3n pero no ha sido implementado a\u00fan. No deber\u00edas usar este error cuando la funci\u00f3n dada no deba apoyar al tipo de argumento de entrada. En esas situaciones, levantar una excepci\u00f3n TypeError es m\u00e1s apropiado. ZeroDivisionError : Esta excepci\u00f3n es levantada cuando proporcionas el segundo argumento para una operaci\u00f3n de divisi\u00f3n o m\u00f3dulo como cero. FileNotFoundError : Esta excepci\u00f3n es levantada cuando el archivo o diccionario que el programa solicit\u00f3 no existe. Aserciones en python Las aserciones son expresiones booleanas que comprueban si las condiciones devuelven verdaderas o no. Si es cierto, el programa no hace nada y pasa a la siguiente l\u00ednea de c\u00f3digo. Sin embargo, si es falso, el programa se detiene y arroja un error. Las aserciones son importantes al momento de realizar tests unitarios o asegurar que un resultado siempre sea el mismo. # definir funcion def suma ( x , y ): return x + y # ejemplo correcto assert suma ( 1 , 1 ) == 2 , \"ejemplo invalido\" # ejemplo incorrecto assert suma ( 1 , 1 ) == 3 , \"ejemplo invalido\" Estructura de datos Listas Se crea una lista colocando todos los elementos (elementos) dentro de un corchete [] separados por comas. # lista vacia lista = [] # lista de enteros lista = [ 1 , 2 , 3 ] # lista mixta lista = [ 1 , \"hola\" , 3.4 ] Tambi\u00e9n, se pueden acceder a cada uno de sus eleentos numeros = [ \"uno\" , \"dos\" , \"tres\" , \"cuatro\" ] # acceder al primer elemento print ( numeros [ 0 ]) # acceder al cuarto elemento print ( numeros [ 1 : 3 ]) Tuplas La tupla es similar a una lista, excepto que no puede cambiar los elementos de una tupla una vez que est\u00e1 definida. Mientras que en una lista, los elementos se pueden modificar. numeros = ( \"uno\" , \"dos\" , \"tres\" , \"cuatro\" ) print ( numeros ) numeros = ( \"uno\" , \"dos\" , \"tres\" , \"cuatro\" ) print ( numeros [ 1 ]) #Output: dos print ( numeros [ 3 ]) #Output: cuatro print ( numeros [ - 1 ]) # Output: cuatro Conjuntos Un conjunto es una colecci\u00f3n desordenada de elementos donde cada elemento es \u00fanico (sin duplicados). # conjunto de enteros conjunto = { 1 , 2 , 3 } print ( conjunto ) # conjunto mixto conjunto = { 1.0 , \"hola\" , ( 1 , 2 , 3 )} print ( conjunto ) # conjunto de enteros conjunto = { 1 , 2 , 3 } conjunto . add ( 4 ) print ( conjunto ) # Output: {1, 2, 3, 4} conjunto . add ( 2 ) print ( conjunto ) # Output: {1, 2, 3, 4} conjunto . update ([ 3 , 4 , 5 ]) print ( conjunto ) # Output: {1, 2, 3, 4, 5} conjunto . remove ( 4 ) print ( conjunto ) # Output: {1, 2, 3, 5} Diccionarios Los diccionarios en Python son un tipo de estructuras de datos que permite guardar un conjunto no ordenado de pares clave-valor, siendo las claves \u00fanicas dentro de un mismo diccionario (es decir que no pueden existir dos elementos con una misma clave) # diccionario vacio dct = {} # diccionario de enteros dct = { 1 : 'apple' , 2 : 'ball' } # diccionario dde llaves mixtas dct = { 'name' : 'John' , 1 : [ 2 , 4 , 3 ]} dct = { 'nombre' : 'Jack' , 'edad' : 26 , 'salario' : 4534.2 } print ( dct [ 'edad' ]) # Output: 26 dct = { 'nombre' : 'Jack' , 'edad' : 26 } # cambiar edad dct [ 'edad' ] = 36 print ( dct ) # Output: {'name': 'Jack', 'age': 36} # adherir llave salario dct [ 'salario' ] = 4342.4 print ( dct ) # Output: {'name': 'Jack', 'age': 36, 'salary': 4342.4} # borrar llave edad del dct [ 'edad' ] print ( dct ) # Output: {'name': 'Jack', 'salary': 4342.4} # borrar diccionario del dct Python range() range() devuelve una secuencia inmutable de n\u00fameros entre el entero de inicio dado al entero de parada. print ( range ( 1 , 10 )) # Output: range(1, 10) El resultado es iterable y puede convertirlo en list, tuple, set, etc. Por ejemplo: numeros = range ( 1 , 6 ) print ( list ( numeros )) # Output: [1, 2, 3, 4, 5] print ( tuple ( numeros )) # Output: (1, 2, 3, 4, 5) print ( set ( numeros )) # Output: {1, 2, 3, 4, 5} # Output: {1: 99, 2: 99, 3: 99, 4: 99, 5: 99} print ( dict . fromkeys ( numeros , 99 )) Hemos omitido el par\u00e1metro de step opcional para range() en los ejemplos anteriores. Cuando se omite, el paso predeterminado es 1. Probemos algunos ejemplos con el par\u00e1metro de paso. numero1 = range ( 1 , 6 , 1 ) print ( list ( numero1 )) # Output: [1, 2, 3, 4, 5] numero2 = range ( 1 , 6 , 2 ) print ( list ( numero2 )) # Output: [1, 3, 5] numero3 = range ( 5 , 0 , - 1 ) print ( list ( numero3 )) # Output: [5, 4, 3, 2, 1] Control de Flujo if, elif,..., else La instrucci\u00f3n if ... else se usa si desea realizar una acci\u00f3n diferente (ejecutar un c\u00f3digo diferente) en diferentes condiciones. Por ejemplo: num = - 1 if num > 0 : print ( \"numero positivo\" ) elif num == 0 : print ( \"cero\" ) else : print ( \"numero negativo\" ) # Output: numero negativo Puede haber cero o m\u00e1s partes elif, y la parte else es opcional. La mayor\u00eda de los lenguajes de programaci\u00f3n usan {} para especificar el bloque de c\u00f3digo. Python usa sangr\u00eda. Un bloque de c\u00f3digo comienza con sangr\u00eda y termina con la primera l\u00ednea sin sangr\u00eda. La cantidad de sangr\u00eda depende de usted, pero debe ser consistente a lo largo de ese bloque. En general, se utilizan cuatro espacios en blanco para la sangr\u00eda y se prefieren a las pesta\u00f1as. Probemos con otro ejemplo: if False : print ( \"Hola\" ) print ( \"mundo\" ) print ( \"!!!\" ) # Output: !!! While Loop Al igual que la mayor\u00eda de los lenguajes de programaci\u00f3n, el loop se usa para iterar sobre un bloque de c\u00f3digo siempre que la expresi\u00f3n de prueba (condici\u00f3n) sea verdadera. Aqu\u00ed hay un ejemplo para encontrar la suma de n\u00fameros naturales: n = 100 # inicializar contador sum = 0 i = 1 while i <= n : sum = sum + i i = i + 1 # actualizar contador print ( \"La suma es\" , sum ) # Output: La suma es For Loop En Python, for loop se usa para iterar sobre una secuencia (lista, tupla, cadena) u otros objetos iterables. Iterar sobre una secuencia se llama transversal. Aqu\u00ed hay un ejemplo para encontrar la suma de todos los n\u00fameros almacenados en una lista. numbers = [ 6 , 5 , 3 , 8 , 4 , 2 ] sum = 0 # iterar sobre la lista for val in numbers : sum = sum + val print ( \"La suma es\" , sum ) # Output: La suma es 28 Break La declaraci\u00f3n break termina el ciclo que lo contiene. El control del programa fluye a la declaraci\u00f3n inmediatamente despu\u00e9s del cuerpo del bucle. Por ejemplo: for val in \"string\" : if val == \"r\" : break print ( val ) print ( \"Fin\" ) Continue La instrucci\u00f3n continue se usa para omitir el resto del c\u00f3digo dentro de un bucle solo para la iteraci\u00f3n actual. El bucle no termina pero contin\u00faa con la siguiente iteraci\u00f3n. Por ejemplo: for val in \"string\" : if val == \"r\" : continue print ( val ) print ( \"Fin\" ) Pass Supongamos que tiene un bucle o una funci\u00f3n que a\u00fan no est\u00e1 implementada, pero desea implementarla en el futuro. No pueden tener un cuerpo vac\u00edo. El int\u00e9rprete se quejar\u00eda. Por lo tanto, utiliza la instrucci\u00f3n pass para construir un cuerpo que no hace nada. sequence = { 'p' , 'a' , 's' , 's' } for val in sequence : pass Funciones Una funci\u00f3n es un grupo de declaraciones relacionadas que realizan una tarea espec\u00edfica. Utiliza la palabra clave def para crear funciones en Python. def imprimir_lineas (): print ( \"linea 1\" ) print ( \"linea 2\" ) # llamar funcion imprimir_lineas () Una funci\u00f3n puede aceptar argumentos. def sumar ( a , b ): sum = a + b return sum resultado = sumar ( 4 , 5 ) print ( resultado ) # Output: 9 Recursion Una funci\u00f3n que se llama a s\u00ed misma se conoce como funci\u00f3n recursiva y este proceso se llama recursividad. Cada funci\u00f3n recursiva debe tener una condici\u00f3n base que detenga la recursividad o, de lo contrario, la funci\u00f3n se llama a s\u00ed misma infinitamente. # funcion factorial (recursivo) def factorial ( x ): if x == 1 : return 1 else : return ( x * factorial ( x - 1 )) num = 6 print ( \"El factorial de \" , num , \"es\" , factorial ( num )) # Output: El factorial de 6 es 720 Lambda En Python, puede definir funciones sin nombre. Estas funciones se denominan lambda o funci\u00f3n an\u00f3nima. Para crear una funci\u00f3n lambda , se utiliza la palabra clave lambda . cuadrado = lambda x : x ** 2 print ( cuadrado ( 5 )) # Output: 25 M\u00f3dulos Los m\u00f3dulos se refieren a un archivo que contiene declaraciones y definiciones de Python. Un archivo que contiene c\u00f3digo Python, por ejemplo: modulo_01.py , se llama m\u00f3dulo y su nombre de m\u00f3dulo ser\u00eda un ejemplo. Perm\u00edtanos crearlo y guardarlo como modulo_01.py. %% writefile modulo_01 . py def sumar ( a , b ): return a + b Para usar este m\u00f3dulo, usamos la palabra clave import . # importar modulo import modulo_01 # acceder a las funciones del modulo modulo_01 . sumar ( 4 , 5.5 ) Tambi\u00e9n se pueden exportar m\u00f3dulos nativos de python. import math resultado = math . log2 ( 5 ) # retorna logaritmo base 2 print ( resultado ) # Output: 2.321928094887362 Python tiene una tonelada de m\u00f3dulos est\u00e1ndar f\u00e1cilmente disponibles para su uso. Por ejemplo: from math import pi print ( \"El valor de pi es\" , pi ) # Output: The value of pi is 3.141592653589793 Programaci\u00f3n orientada a objetos (OOP) Todo en Python es un objeto que incluye enteros, flotantes, funciones, clases y Ninguno. No nos centremos en por qu\u00e9 todo en Python es un objeto. Para eso, visite esta p\u00e1gina. M\u00e1s bien, esta secci\u00f3n se enfoca en crear sus propias clases y objetos. Clase y objetos El objeto es simplemente una colecci\u00f3n de datos (variables) y m\u00e9todos (funciones) que act\u00faan sobre los datos. Y, la clase es un modelo para el objeto. Tan pronto como defina una clase, se crea un nuevo objeto de clase con el mismo nombre. Este objeto de clase nos permite acceder a los diferentes atributos, as\u00ed como crear instancias de nuevos objetos de esa clase. class Mi_clase : \"Esta es mi clase\" a = 10 def func ( self ): print ( 'hola' ) # Output: 10 print ( Mi_clase . a ) # Output: <function 0x0000000003079bf8=\"\" at=\"\" myclass.func=\"\"> print ( Mi_clase . func ) # Output: 'Esta es mi clase' print ( Mi_clase . __doc__ ) Es posible que haya notado el par\u00e1metro self en la definici\u00f3n de la funci\u00f3n dentro de la clase, pero llamamos al m\u00e9todo simplemente como ob.func() sin ning\u00fan argumento. A\u00fan funcion\u00f3. Esto se debe a que, cada vez que un objeto llama a su m\u00e9todo, el objeto mismo se pasa como primer argumento. Entonces, ob.func() se traduce en Mi_clase.func(ob) . Creando objetos Tambi\u00e9n puede crear objetos de la clase usted mismo. class Mi_clase : \"Esta es mi clase\" a = 10 def func ( self ): print ( 'hola' ) obj1 = Mi_clase () print ( obj1 . a ) # Output: 10 obj2 = Mi_clase () print ( obj1 . a + 5 ) # Output: 15 Constructores de Python En Python, un m\u00e9todo con el nombre init () es un constructor. Este m\u00e9todo se llama autom\u00e1ticamente cuando se instancia un objeto. class NumerosComplejos : def __init__ ( self , r = 0 , i = 0 ): # constructor self . real = r self . imag = i def obtener_datos ( self ): print ( \" {0} + {1} j\" . format ( self . real , self . imag )) c1 = NumerosComplejos ( 2 , 3 ) # crear el objeto NumerosComplejos c1 . obtener_datos () # Output: 2+3j c2 = NumerosComplejos () # crear un nuevo objeto NumerosComplejos c2 . obtener_datos () # Output: 0+0j Herencia de Python La herencia se refiere a definir una nueva clase con poca o ninguna modificaci\u00f3n a una clase existente. Tomemos un ejemplo: class Mamifero : def caracteristicas ( self ): print ( 'Mam\u00edfero es un animal de sangre caliente' ) Derivemos una nueva clase Perro de esta clase Mamifero . class Mamifero : def caracteristicas_mamifero ( self ): print ( 'Mam\u00edfero es un animal de sangre caliente' ) class Perro ( Mamifero ): def caracteristicas_perro ( self ): print ( 'El perro ladra' ) d = Perro () d . caracteristicas_perro () d . caracteristicas_mamifero () Decoradores Python tiene una caracter\u00edstica interesante llamada decoradores para agregar funcionalidad a un c\u00f3digo existente. Esto tambi\u00e9n se llama metaprogramaci\u00f3n ya que una parte del programa intenta modificar otra parte del programa en tiempo de compilaci\u00f3n. def debug ( f ): def nueva_funcion ( a , b ): print ( \"La funcion Sumar es llamada!!!\" ) return f ( a , b ) return nueva_funcion @debug # decorador def Sumar ( a , b ): return a + b print ( Sumar ( 7 , 5 )) Referencia Introducci\u00f3n al lenguaje Python Introducci\u00f3n \u2014 Tutorial de Python 3.6.3 documentation","title":"Python"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#python","text":"","title":"Python"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#nomenclatura-basica","text":"","title":"Nomenclatura b\u00e1sica"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#hola-mundo","text":"Escribamos nuestro primer programa de Python, \"\u00a1Hola, mundo!\". Es un programa simple que imprime Hello World! en el dispositivo de salida est\u00e1ndar (pantalla). # imprimir \"Hola Mundo!\" print ( \"Hola Mundo!\" );","title":"Hola mundo!"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#variables","text":"Una variable es una ubicaci\u00f3n con nombre utilizada para almacenar datos en la memoria. Aqu\u00ed hay un ejemplo: # crear e imprimir variables a = 5 print ( \"a =\" , 5 ) a = \"cinco\" print ( \"a =\" , a )","title":"Variables"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#operadores-basico","text":"Los operadores son s\u00edmbolos especiales que realizan operaciones en operandos (variables y valores). Hablemos de operadores aritm\u00e9ticos y de asignaci\u00f3n en esta parte. # operaciones basicas x = 14 y = 4 # suma print ( 'x + y =' , x + y ) # Output: x + y = 18 # resta print ( 'x - y =' , x - y ) # Output: x - y = 10 # multiplicacion print ( 'x * y =' , x * y ) # Output: x * y = 56 # division print ( 'x / y =' , x / y ) # Output: x / y = 3.5 # cuociente print ( 'x // y =' , x // y ) # Output: x // y = 3 # resto print ( 'x % y =' , x % y ) # Output: x % y = 2","title":"Operadores b\u00e1sico"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#operadores-de-asignacion","text":"Los operadores de asignaci\u00f3n se utilizan para asignar valores a las variables. Ya has visto el uso de = operator. Probemos algunos operadores de asignaci\u00f3n m\u00e1s. # operadores de asignacion x = 5 # x += 5 ----> x = x + 5 x += 5 print ( x ) # Output: 10 # x /= 5 ----> x = x / 5 x /= 5 print ( x ) # Output: 2.0","title":"Operadores de asignaci\u00f3n"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#inputs-por-el-usuario","text":"En Python, puede usar la funci\u00f3n input() para tomar la entrada del usuario. Por ejemplo: # inputs por el usuario inputString = input ( 'Escriba una oracion:' ) print ( 'Su oracion es:' , inputString )","title":"Inputs por el usuario"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#conversion-de-tipo","text":"The process of converting the value of one data type (integer, string, float, etc.) to another is called type conversion. Python has two types of type conversion.","title":"Conversi\u00f3n de tipo"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#implicita","text":"# correcto num_int = 123 # integer type num_flo = 1.23 # float type num_new = num_int + num_flo print ( \"Valor de num_new:\" , num_new ) print ( \"tipo de datos de num_new:\" , type ( num_new )) # incorrecto num_int = 123 # int type num_str = \"456\" # str type print ( num_int + num_str )","title":"Impl\u00edcita"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#explicito","text":"num_int = 123 # int type num_str = \"456\" # str type # explicitly converted to int type num_str = int ( num_str ) print ( num_int + num_str )","title":"Expl\u00edcito"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#python-tipos-numericos","text":"Python admite enteros, n\u00fameros de coma flotante y n\u00fameros complejos. Se definen como int, float y clase compleja en Python. Adem\u00e1s de eso, los booleanos: verdadero y falso son un subtipo de enteros. print ( type ( 5 )) # Output: <class> print ( type ( 5.0 )) # <class> c = 5 + 3 j print ( type ( c ))","title":"Python tipos num\u00e9ricos"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#manejo-de-excepciones-de-python","text":"Los errores que se producen en tiempo de ejecuci\u00f3n se denominan excepciones. Ocurren, por ejemplo, cuando un archivo que intentamos abrir no existe FileNotFoundError , dividiendo un n\u00famero por cero ZeroDivisionError , etc. Si no se manejan las excepciones, se escupe un mensaje de error y nuestro programa se detiene repentinamente e inesperadamente. En Python, las excepciones se pueden manejar usando la declaraci\u00f3n try. Cuando se detectan excepciones, depende de usted qu\u00e9 operador realizar. # importar modulo sys import sys lista_random = [ 'a' , 0 , 2 ] for elemento in lista_random : try : print ( \"La entrada es \" , elemento ) r = 1 / int ( elemento ) break except : print ( \"Oops! ocurrio un\" , sys . exc_info ()[ 0 ]) print ( \"Siguiente entrada.\" ) print () print ( \"El reciproco de\" , elemento , \"es\" , r ) Algunas Excepciones Comunes NameError : Esta excepci\u00f3n es levantada cuando el programa no puede encontrar un nombre local o global. El nombre que podr\u00eda no ser encontrado est\u00e1 incluido en el mensaje de error. TypeError : Esta excepci\u00f3n es levantada cuando una funci\u00f3n se le pasa un objeto del tipo inapropiado como su argumento. M\u00e1s detalles sobre el tipo incorrecto son proporcionados en el mensaje de error. ValueError : Esta excepci\u00f3n ocurre cuando un argumento de funci\u00f3n tiene el tipo correcto pero un valor inapropiado. NotImplementedError : Esta excepci\u00f3n es levantada cuando se supone que un objeto apoye una operaci\u00f3n pero no ha sido implementado a\u00fan. No deber\u00edas usar este error cuando la funci\u00f3n dada no deba apoyar al tipo de argumento de entrada. En esas situaciones, levantar una excepci\u00f3n TypeError es m\u00e1s apropiado. ZeroDivisionError : Esta excepci\u00f3n es levantada cuando proporcionas el segundo argumento para una operaci\u00f3n de divisi\u00f3n o m\u00f3dulo como cero. FileNotFoundError : Esta excepci\u00f3n es levantada cuando el archivo o diccionario que el programa solicit\u00f3 no existe.","title":"Manejo de excepciones de Python"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#aserciones-en-python","text":"Las aserciones son expresiones booleanas que comprueban si las condiciones devuelven verdaderas o no. Si es cierto, el programa no hace nada y pasa a la siguiente l\u00ednea de c\u00f3digo. Sin embargo, si es falso, el programa se detiene y arroja un error. Las aserciones son importantes al momento de realizar tests unitarios o asegurar que un resultado siempre sea el mismo. # definir funcion def suma ( x , y ): return x + y # ejemplo correcto assert suma ( 1 , 1 ) == 2 , \"ejemplo invalido\" # ejemplo incorrecto assert suma ( 1 , 1 ) == 3 , \"ejemplo invalido\"","title":"Aserciones en python"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#estructura-de-datos","text":"","title":"Estructura de datos"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#listas","text":"Se crea una lista colocando todos los elementos (elementos) dentro de un corchete [] separados por comas. # lista vacia lista = [] # lista de enteros lista = [ 1 , 2 , 3 ] # lista mixta lista = [ 1 , \"hola\" , 3.4 ] Tambi\u00e9n, se pueden acceder a cada uno de sus eleentos numeros = [ \"uno\" , \"dos\" , \"tres\" , \"cuatro\" ] # acceder al primer elemento print ( numeros [ 0 ]) # acceder al cuarto elemento print ( numeros [ 1 : 3 ])","title":"Listas"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#tuplas","text":"La tupla es similar a una lista, excepto que no puede cambiar los elementos de una tupla una vez que est\u00e1 definida. Mientras que en una lista, los elementos se pueden modificar. numeros = ( \"uno\" , \"dos\" , \"tres\" , \"cuatro\" ) print ( numeros ) numeros = ( \"uno\" , \"dos\" , \"tres\" , \"cuatro\" ) print ( numeros [ 1 ]) #Output: dos print ( numeros [ 3 ]) #Output: cuatro print ( numeros [ - 1 ]) # Output: cuatro","title":"Tuplas"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#conjuntos","text":"Un conjunto es una colecci\u00f3n desordenada de elementos donde cada elemento es \u00fanico (sin duplicados). # conjunto de enteros conjunto = { 1 , 2 , 3 } print ( conjunto ) # conjunto mixto conjunto = { 1.0 , \"hola\" , ( 1 , 2 , 3 )} print ( conjunto ) # conjunto de enteros conjunto = { 1 , 2 , 3 } conjunto . add ( 4 ) print ( conjunto ) # Output: {1, 2, 3, 4} conjunto . add ( 2 ) print ( conjunto ) # Output: {1, 2, 3, 4} conjunto . update ([ 3 , 4 , 5 ]) print ( conjunto ) # Output: {1, 2, 3, 4, 5} conjunto . remove ( 4 ) print ( conjunto ) # Output: {1, 2, 3, 5}","title":"Conjuntos"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#diccionarios","text":"Los diccionarios en Python son un tipo de estructuras de datos que permite guardar un conjunto no ordenado de pares clave-valor, siendo las claves \u00fanicas dentro de un mismo diccionario (es decir que no pueden existir dos elementos con una misma clave) # diccionario vacio dct = {} # diccionario de enteros dct = { 1 : 'apple' , 2 : 'ball' } # diccionario dde llaves mixtas dct = { 'name' : 'John' , 1 : [ 2 , 4 , 3 ]} dct = { 'nombre' : 'Jack' , 'edad' : 26 , 'salario' : 4534.2 } print ( dct [ 'edad' ]) # Output: 26 dct = { 'nombre' : 'Jack' , 'edad' : 26 } # cambiar edad dct [ 'edad' ] = 36 print ( dct ) # Output: {'name': 'Jack', 'age': 36} # adherir llave salario dct [ 'salario' ] = 4342.4 print ( dct ) # Output: {'name': 'Jack', 'age': 36, 'salary': 4342.4} # borrar llave edad del dct [ 'edad' ] print ( dct ) # Output: {'name': 'Jack', 'salary': 4342.4} # borrar diccionario del dct","title":"Diccionarios"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#python-range","text":"range() devuelve una secuencia inmutable de n\u00fameros entre el entero de inicio dado al entero de parada. print ( range ( 1 , 10 )) # Output: range(1, 10) El resultado es iterable y puede convertirlo en list, tuple, set, etc. Por ejemplo: numeros = range ( 1 , 6 ) print ( list ( numeros )) # Output: [1, 2, 3, 4, 5] print ( tuple ( numeros )) # Output: (1, 2, 3, 4, 5) print ( set ( numeros )) # Output: {1, 2, 3, 4, 5} # Output: {1: 99, 2: 99, 3: 99, 4: 99, 5: 99} print ( dict . fromkeys ( numeros , 99 )) Hemos omitido el par\u00e1metro de step opcional para range() en los ejemplos anteriores. Cuando se omite, el paso predeterminado es 1. Probemos algunos ejemplos con el par\u00e1metro de paso. numero1 = range ( 1 , 6 , 1 ) print ( list ( numero1 )) # Output: [1, 2, 3, 4, 5] numero2 = range ( 1 , 6 , 2 ) print ( list ( numero2 )) # Output: [1, 3, 5] numero3 = range ( 5 , 0 , - 1 ) print ( list ( numero3 )) # Output: [5, 4, 3, 2, 1]","title":"Python range()"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#control-de-flujo","text":"","title":"Control de Flujo"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#if-elif-else","text":"La instrucci\u00f3n if ... else se usa si desea realizar una acci\u00f3n diferente (ejecutar un c\u00f3digo diferente) en diferentes condiciones. Por ejemplo: num = - 1 if num > 0 : print ( \"numero positivo\" ) elif num == 0 : print ( \"cero\" ) else : print ( \"numero negativo\" ) # Output: numero negativo Puede haber cero o m\u00e1s partes elif, y la parte else es opcional. La mayor\u00eda de los lenguajes de programaci\u00f3n usan {} para especificar el bloque de c\u00f3digo. Python usa sangr\u00eda. Un bloque de c\u00f3digo comienza con sangr\u00eda y termina con la primera l\u00ednea sin sangr\u00eda. La cantidad de sangr\u00eda depende de usted, pero debe ser consistente a lo largo de ese bloque. En general, se utilizan cuatro espacios en blanco para la sangr\u00eda y se prefieren a las pesta\u00f1as. Probemos con otro ejemplo: if False : print ( \"Hola\" ) print ( \"mundo\" ) print ( \"!!!\" ) # Output: !!!","title":"if, elif,..., else"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#while-loop","text":"Al igual que la mayor\u00eda de los lenguajes de programaci\u00f3n, el loop se usa para iterar sobre un bloque de c\u00f3digo siempre que la expresi\u00f3n de prueba (condici\u00f3n) sea verdadera. Aqu\u00ed hay un ejemplo para encontrar la suma de n\u00fameros naturales: n = 100 # inicializar contador sum = 0 i = 1 while i <= n : sum = sum + i i = i + 1 # actualizar contador print ( \"La suma es\" , sum ) # Output: La suma es","title":"While Loop"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#for-loop","text":"En Python, for loop se usa para iterar sobre una secuencia (lista, tupla, cadena) u otros objetos iterables. Iterar sobre una secuencia se llama transversal. Aqu\u00ed hay un ejemplo para encontrar la suma de todos los n\u00fameros almacenados en una lista. numbers = [ 6 , 5 , 3 , 8 , 4 , 2 ] sum = 0 # iterar sobre la lista for val in numbers : sum = sum + val print ( \"La suma es\" , sum ) # Output: La suma es 28","title":"For Loop"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#break","text":"La declaraci\u00f3n break termina el ciclo que lo contiene. El control del programa fluye a la declaraci\u00f3n inmediatamente despu\u00e9s del cuerpo del bucle. Por ejemplo: for val in \"string\" : if val == \"r\" : break print ( val ) print ( \"Fin\" )","title":"Break"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#continue","text":"La instrucci\u00f3n continue se usa para omitir el resto del c\u00f3digo dentro de un bucle solo para la iteraci\u00f3n actual. El bucle no termina pero contin\u00faa con la siguiente iteraci\u00f3n. Por ejemplo: for val in \"string\" : if val == \"r\" : continue print ( val ) print ( \"Fin\" )","title":"Continue"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#pass","text":"Supongamos que tiene un bucle o una funci\u00f3n que a\u00fan no est\u00e1 implementada, pero desea implementarla en el futuro. No pueden tener un cuerpo vac\u00edo. El int\u00e9rprete se quejar\u00eda. Por lo tanto, utiliza la instrucci\u00f3n pass para construir un cuerpo que no hace nada. sequence = { 'p' , 'a' , 's' , 's' } for val in sequence : pass","title":"Pass"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#funciones","text":"Una funci\u00f3n es un grupo de declaraciones relacionadas que realizan una tarea espec\u00edfica. Utiliza la palabra clave def para crear funciones en Python. def imprimir_lineas (): print ( \"linea 1\" ) print ( \"linea 2\" ) # llamar funcion imprimir_lineas () Una funci\u00f3n puede aceptar argumentos. def sumar ( a , b ): sum = a + b return sum resultado = sumar ( 4 , 5 ) print ( resultado ) # Output: 9","title":"Funciones"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#recursion","text":"Una funci\u00f3n que se llama a s\u00ed misma se conoce como funci\u00f3n recursiva y este proceso se llama recursividad. Cada funci\u00f3n recursiva debe tener una condici\u00f3n base que detenga la recursividad o, de lo contrario, la funci\u00f3n se llama a s\u00ed misma infinitamente. # funcion factorial (recursivo) def factorial ( x ): if x == 1 : return 1 else : return ( x * factorial ( x - 1 )) num = 6 print ( \"El factorial de \" , num , \"es\" , factorial ( num )) # Output: El factorial de 6 es 720","title":"Recursion"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#lambda","text":"En Python, puede definir funciones sin nombre. Estas funciones se denominan lambda o funci\u00f3n an\u00f3nima. Para crear una funci\u00f3n lambda , se utiliza la palabra clave lambda . cuadrado = lambda x : x ** 2 print ( cuadrado ( 5 )) # Output: 25","title":"Lambda"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#modulos","text":"Los m\u00f3dulos se refieren a un archivo que contiene declaraciones y definiciones de Python. Un archivo que contiene c\u00f3digo Python, por ejemplo: modulo_01.py , se llama m\u00f3dulo y su nombre de m\u00f3dulo ser\u00eda un ejemplo. Perm\u00edtanos crearlo y guardarlo como modulo_01.py. %% writefile modulo_01 . py def sumar ( a , b ): return a + b Para usar este m\u00f3dulo, usamos la palabra clave import . # importar modulo import modulo_01 # acceder a las funciones del modulo modulo_01 . sumar ( 4 , 5.5 ) Tambi\u00e9n se pueden exportar m\u00f3dulos nativos de python. import math resultado = math . log2 ( 5 ) # retorna logaritmo base 2 print ( resultado ) # Output: 2.321928094887362 Python tiene una tonelada de m\u00f3dulos est\u00e1ndar f\u00e1cilmente disponibles para su uso. Por ejemplo: from math import pi print ( \"El valor de pi es\" , pi ) # Output: The value of pi is 3.141592653589793","title":"M\u00f3dulos"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#programacion-orientada-a-objetos-oop","text":"Todo en Python es un objeto que incluye enteros, flotantes, funciones, clases y Ninguno. No nos centremos en por qu\u00e9 todo en Python es un objeto. Para eso, visite esta p\u00e1gina. M\u00e1s bien, esta secci\u00f3n se enfoca en crear sus propias clases y objetos.","title":"Programaci\u00f3n orientada a objetos (OOP)"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#clase-y-objetos","text":"El objeto es simplemente una colecci\u00f3n de datos (variables) y m\u00e9todos (funciones) que act\u00faan sobre los datos. Y, la clase es un modelo para el objeto. Tan pronto como defina una clase, se crea un nuevo objeto de clase con el mismo nombre. Este objeto de clase nos permite acceder a los diferentes atributos, as\u00ed como crear instancias de nuevos objetos de esa clase. class Mi_clase : \"Esta es mi clase\" a = 10 def func ( self ): print ( 'hola' ) # Output: 10 print ( Mi_clase . a ) # Output: <function 0x0000000003079bf8=\"\" at=\"\" myclass.func=\"\"> print ( Mi_clase . func ) # Output: 'Esta es mi clase' print ( Mi_clase . __doc__ ) Es posible que haya notado el par\u00e1metro self en la definici\u00f3n de la funci\u00f3n dentro de la clase, pero llamamos al m\u00e9todo simplemente como ob.func() sin ning\u00fan argumento. A\u00fan funcion\u00f3. Esto se debe a que, cada vez que un objeto llama a su m\u00e9todo, el objeto mismo se pasa como primer argumento. Entonces, ob.func() se traduce en Mi_clase.func(ob) .","title":"Clase y objetos"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#creando-objetos","text":"Tambi\u00e9n puede crear objetos de la clase usted mismo. class Mi_clase : \"Esta es mi clase\" a = 10 def func ( self ): print ( 'hola' ) obj1 = Mi_clase () print ( obj1 . a ) # Output: 10 obj2 = Mi_clase () print ( obj1 . a + 5 ) # Output: 15","title":"Creando objetos"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#constructores-de-python","text":"En Python, un m\u00e9todo con el nombre init () es un constructor. Este m\u00e9todo se llama autom\u00e1ticamente cuando se instancia un objeto. class NumerosComplejos : def __init__ ( self , r = 0 , i = 0 ): # constructor self . real = r self . imag = i def obtener_datos ( self ): print ( \" {0} + {1} j\" . format ( self . real , self . imag )) c1 = NumerosComplejos ( 2 , 3 ) # crear el objeto NumerosComplejos c1 . obtener_datos () # Output: 2+3j c2 = NumerosComplejos () # crear un nuevo objeto NumerosComplejos c2 . obtener_datos () # Output: 0+0j","title":"Constructores de Python"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#herencia-de-python","text":"La herencia se refiere a definir una nueva clase con poca o ninguna modificaci\u00f3n a una clase existente. Tomemos un ejemplo: class Mamifero : def caracteristicas ( self ): print ( 'Mam\u00edfero es un animal de sangre caliente' ) Derivemos una nueva clase Perro de esta clase Mamifero . class Mamifero : def caracteristicas_mamifero ( self ): print ( 'Mam\u00edfero es un animal de sangre caliente' ) class Perro ( Mamifero ): def caracteristicas_perro ( self ): print ( 'El perro ladra' ) d = Perro () d . caracteristicas_perro () d . caracteristicas_mamifero ()","title":"Herencia de Python"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#decoradores","text":"Python tiene una caracter\u00edstica interesante llamada decoradores para agregar funcionalidad a un c\u00f3digo existente. Esto tambi\u00e9n se llama metaprogramaci\u00f3n ya que una parte del programa intenta modificar otra parte del programa en tiempo de compilaci\u00f3n. def debug ( f ): def nueva_funcion ( a , b ): print ( \"La funcion Sumar es llamada!!!\" ) return f ( a , b ) return nueva_funcion @debug # decorador def Sumar ( a , b ): return a + b print ( Sumar ( 7 , 5 ))","title":"Decoradores"},{"location":"lectures/basic_tools/basic_tools/lecture_012_python/#referencia","text":"Introducci\u00f3n al lenguaje Python Introducci\u00f3n \u2014 Tutorial de Python 3.6.3 documentation","title":"Referencia"},{"location":"lectures/basic_tools/basic_tools/lecture_013_ide/","text":"IDE's Introducci\u00f3n Un IDE (Integrated Development Environment) es un software que ayuda a los desarrolladores a escribir c\u00f3digos para desarrollar aplicaciones. Tiene una interfaz f\u00e1cil de usar que consta de un editor y un compilador. Podemos escribir el c\u00f3digo en la ventana del editor y compilarlo usando el compilador. En consecuencia, podemos ejecutarlo para verificar la salida del programa en el terminal de salida. Un IDE facilita a los desarrolladores con muchas funcionalidades programar en varios idiomas. As\u00ed, se ha convertido en una de las herramientas imprescindibles para el desarrollo de diversas aplicaciones. Herramientas Text Editor Escribimos programas en la ventana del editor de texto. Aqu\u00ed podemos editar, depurar y comentar nuestros programas. Tambi\u00e9n proporciona funciones como el predictor de texto que ayuda a corregir errores. Project Editor Un proyecto consta de diferentes archivos para diversas funcionalidades. Almacenamos estos archivos esenciales de un proyecto de software en la ventana del Editor de proyectos. Output Terminal El terminal de salida en un IDE muestra el resultado del programa. Toolbar Una barra de herramientas es un conjunto de opciones que est\u00e1n vinculadas a funcionalidades espec\u00edficas como ver, ejecutar, debugging, etc. Ayuda a realizar varias operaciones en el programa que creamos. La necesidad de un IDE La velocidad y la eficiencia son los dos factores que afectan a todas las industrias. En esta era competitiva, se ha convertido en un desaf\u00edo crear software r\u00e1pido y escalable para los clientes. Aqu\u00ed es donde un IDE inteligente gana el juego. Un IDE nos ayuda con las siguientes caracter\u00edsticas: Autocompletar Esta funci\u00f3n nos ayuda a corregir errores de sintaxis de inmediato. Sugiere las variables, palabras clave, funciones, bibliotecas y paquetes correctos para usar al escribir el c\u00f3digo. Adem\u00e1s, el esquema de color de un IDE resulta ser un gran activo. Ayuda a comprender el flujo de un programa. Podemos distinguir f\u00e1cilmente entre variables y texto y entre funciones y clases, y as\u00ed comprender la l\u00f3gica del c\u00f3digo. Debugging El debugging es el paso m\u00e1s importante para crear un software exitoso. El debugging del c\u00f3digo implica probar y borrar todos los errores para que nuestro programa no se bloquee. Gesti\u00f3n de archivos Un proyecto consta de diferentes archivos con diferentes funcionalidades. Se convierte en una tarea necesaria colocar estos archivos en ubicaciones adecuadas para la ejecuci\u00f3n del programa. Un IDE hace que la ubicaci\u00f3n de todos los archivos de recursos de un proyecto sea m\u00e1s visible Tipos de IDE La creaci\u00f3n de aplicaciones utilizando diferentes lenguajes de programaci\u00f3n requiere varios tipos de IDE compatibles con los respectivos lenguajes de programaci\u00f3n. En esta secci\u00f3n, veremos los diferentes tipos de IDE utilizados para el desarrollo en varios lenguajes. Existen diferentes categor\u00edas de IDE, por ejemplo, usamos IDE para m\u00faltiples lenguajes de programaci\u00f3n, desarrollo m\u00f3vil y para lenguajes de programaci\u00f3n particulares. IDE para varios lenguajes Hay muchos IDE disponibles que ayudan a programar en diferentes lenguajes. Podemos utilizar un \u00fanico IDE para programar en varios lenguajes como C, C ++ o Java. Algunos de estos IDE se mencionan a continuaci\u00f3n: NetBeans Es un IDE de c\u00f3digo abierto que consta de varios paquetes y m\u00f3dulos \u00fatiles para el desarrollo. NetBeans admite varios idiomas como se indica a continuaci\u00f3n: C/C++ Java Python Ruby PHP JavaScript Groovy Eclipse Tambi\u00e9n es un software de c\u00f3digo abierto utilizado para varios marcos de desarrollo. Se utiliza principalmente para el desarrollo de aplicaciones con Java. Los idiomas admitidos por Eclipse son: Java Python C/C++ PHP Perl Ruby COBOL Haskell Fortran Aptana Es un IDE popular utilizado para el desarrollo de aplicaciones web. Los lenguajes de programaci\u00f3n admitidos por Aptana son: Python Ruby on Rails Ajax HTML CSS JavaScript IDE de desarrollo m\u00f3vil Hay varios IDE que se utilizan para el desarrollo de aplicaciones m\u00f3viles. Algunos de ellos se enumeran a continuaci\u00f3n: PhoneGap Android Studio IntelliJ IDEA Visual Studio Appcelerator IDE para lenguajes particulares Para programar exclusivamente en un idioma espec\u00edfico, hay varios IDE disponibles. Son los siguientes: Java: JCreator Python: IDLE C/C++: CodeLite R: RStudio Referencia What is an IDE?","title":"IDE"},{"location":"lectures/basic_tools/basic_tools/lecture_013_ide/#ides","text":"","title":"IDE's"},{"location":"lectures/basic_tools/basic_tools/lecture_013_ide/#introduccion","text":"Un IDE (Integrated Development Environment) es un software que ayuda a los desarrolladores a escribir c\u00f3digos para desarrollar aplicaciones. Tiene una interfaz f\u00e1cil de usar que consta de un editor y un compilador. Podemos escribir el c\u00f3digo en la ventana del editor y compilarlo usando el compilador. En consecuencia, podemos ejecutarlo para verificar la salida del programa en el terminal de salida. Un IDE facilita a los desarrolladores con muchas funcionalidades programar en varios idiomas. As\u00ed, se ha convertido en una de las herramientas imprescindibles para el desarrollo de diversas aplicaciones.","title":"Introducci\u00f3n"},{"location":"lectures/basic_tools/basic_tools/lecture_013_ide/#herramientas","text":"","title":"Herramientas"},{"location":"lectures/basic_tools/basic_tools/lecture_013_ide/#text-editor","text":"Escribimos programas en la ventana del editor de texto. Aqu\u00ed podemos editar, depurar y comentar nuestros programas. Tambi\u00e9n proporciona funciones como el predictor de texto que ayuda a corregir errores.","title":"Text Editor"},{"location":"lectures/basic_tools/basic_tools/lecture_013_ide/#project-editor","text":"Un proyecto consta de diferentes archivos para diversas funcionalidades. Almacenamos estos archivos esenciales de un proyecto de software en la ventana del Editor de proyectos.","title":"Project Editor"},{"location":"lectures/basic_tools/basic_tools/lecture_013_ide/#output-terminal","text":"El terminal de salida en un IDE muestra el resultado del programa.","title":"Output Terminal"},{"location":"lectures/basic_tools/basic_tools/lecture_013_ide/#toolbar","text":"Una barra de herramientas es un conjunto de opciones que est\u00e1n vinculadas a funcionalidades espec\u00edficas como ver, ejecutar, debugging, etc. Ayuda a realizar varias operaciones en el programa que creamos.","title":"Toolbar"},{"location":"lectures/basic_tools/basic_tools/lecture_013_ide/#la-necesidad-de-un-ide","text":"La velocidad y la eficiencia son los dos factores que afectan a todas las industrias. En esta era competitiva, se ha convertido en un desaf\u00edo crear software r\u00e1pido y escalable para los clientes. Aqu\u00ed es donde un IDE inteligente gana el juego. Un IDE nos ayuda con las siguientes caracter\u00edsticas:","title":"La necesidad de un IDE"},{"location":"lectures/basic_tools/basic_tools/lecture_013_ide/#autocompletar","text":"Esta funci\u00f3n nos ayuda a corregir errores de sintaxis de inmediato. Sugiere las variables, palabras clave, funciones, bibliotecas y paquetes correctos para usar al escribir el c\u00f3digo. Adem\u00e1s, el esquema de color de un IDE resulta ser un gran activo. Ayuda a comprender el flujo de un programa. Podemos distinguir f\u00e1cilmente entre variables y texto y entre funciones y clases, y as\u00ed comprender la l\u00f3gica del c\u00f3digo.","title":"Autocompletar"},{"location":"lectures/basic_tools/basic_tools/lecture_013_ide/#debugging","text":"El debugging es el paso m\u00e1s importante para crear un software exitoso. El debugging del c\u00f3digo implica probar y borrar todos los errores para que nuestro programa no se bloquee.","title":"Debugging"},{"location":"lectures/basic_tools/basic_tools/lecture_013_ide/#gestion-de-archivos","text":"Un proyecto consta de diferentes archivos con diferentes funcionalidades. Se convierte en una tarea necesaria colocar estos archivos en ubicaciones adecuadas para la ejecuci\u00f3n del programa. Un IDE hace que la ubicaci\u00f3n de todos los archivos de recursos de un proyecto sea m\u00e1s visible","title":"Gesti\u00f3n de archivos"},{"location":"lectures/basic_tools/basic_tools/lecture_013_ide/#tipos-de-ide","text":"La creaci\u00f3n de aplicaciones utilizando diferentes lenguajes de programaci\u00f3n requiere varios tipos de IDE compatibles con los respectivos lenguajes de programaci\u00f3n. En esta secci\u00f3n, veremos los diferentes tipos de IDE utilizados para el desarrollo en varios lenguajes. Existen diferentes categor\u00edas de IDE, por ejemplo, usamos IDE para m\u00faltiples lenguajes de programaci\u00f3n, desarrollo m\u00f3vil y para lenguajes de programaci\u00f3n particulares.","title":"Tipos de IDE"},{"location":"lectures/basic_tools/basic_tools/lecture_013_ide/#ide-para-varios-lenguajes","text":"Hay muchos IDE disponibles que ayudan a programar en diferentes lenguajes. Podemos utilizar un \u00fanico IDE para programar en varios lenguajes como C, C ++ o Java. Algunos de estos IDE se mencionan a continuaci\u00f3n:","title":"IDE para varios lenguajes"},{"location":"lectures/basic_tools/basic_tools/lecture_013_ide/#netbeans","text":"Es un IDE de c\u00f3digo abierto que consta de varios paquetes y m\u00f3dulos \u00fatiles para el desarrollo. NetBeans admite varios idiomas como se indica a continuaci\u00f3n: C/C++ Java Python Ruby PHP JavaScript Groovy","title":"NetBeans"},{"location":"lectures/basic_tools/basic_tools/lecture_013_ide/#eclipse","text":"Tambi\u00e9n es un software de c\u00f3digo abierto utilizado para varios marcos de desarrollo. Se utiliza principalmente para el desarrollo de aplicaciones con Java. Los idiomas admitidos por Eclipse son: Java Python C/C++ PHP Perl Ruby COBOL Haskell Fortran","title":"Eclipse"},{"location":"lectures/basic_tools/basic_tools/lecture_013_ide/#aptana","text":"Es un IDE popular utilizado para el desarrollo de aplicaciones web. Los lenguajes de programaci\u00f3n admitidos por Aptana son: Python Ruby on Rails Ajax HTML CSS JavaScript","title":"Aptana"},{"location":"lectures/basic_tools/basic_tools/lecture_013_ide/#ide-de-desarrollo-movil","text":"Hay varios IDE que se utilizan para el desarrollo de aplicaciones m\u00f3viles. Algunos de ellos se enumeran a continuaci\u00f3n: PhoneGap Android Studio IntelliJ IDEA Visual Studio Appcelerator","title":"IDE de desarrollo m\u00f3vil"},{"location":"lectures/basic_tools/basic_tools/lecture_013_ide/#ide-para-lenguajes-particulares","text":"Para programar exclusivamente en un idioma espec\u00edfico, hay varios IDE disponibles. Son los siguientes: Java: JCreator Python: IDLE C/C++: CodeLite R: RStudio","title":"IDE para lenguajes particulares"},{"location":"lectures/basic_tools/basic_tools/lecture_013_ide/#referencia","text":"What is an IDE?","title":"Referencia"},{"location":"lectures/basic_tools/basic_tools/lecture_014_jupyter/","text":"Jupyter Notebook Funciones B\u00e1sicas Jupyter Notebook es una herramienta de desarrollo para facilitar la programaci\u00f3n. Est\u00e1 orientada a la Computaci\u00f3n Cient\u00edfica y a la Ciencia de Datos. Es un entorno de computaci\u00f3n interactivo agn\u00f3stico, se puede ocupar en R, python o Julia. Jupyter notebook ofrece muchas herramientas que ser\u00e1n de gran utilidad a lo largo de este curso. Toolbox Jupyter notebook nos ofrece el siguiente toolbox: File : En \u00e9l, puede crear un nuevo cuaderno o abrir uno preexistente. Aqu\u00ed es tambi\u00e9n a donde ir\u00eda para cambiar el nombre de un Cuaderno. Creo que el elemento de men\u00fa m\u00e1s interesante es la opci\u00f3n Guardar y Checkpoint. Esto le permite crear puntos de control a los que puede retroceder si lo necesita. Edit : Aqu\u00ed puede cortar, copiar y pegar celdas. Aqu\u00ed tambi\u00e9n es donde ir\u00edas si quisieras eliminar, dividir o fusionar una celda. Puede reordenar celdas aqu\u00ed tambi\u00e9n. View : es \u00fatil para alternar la visibilidad del encabezado y la barra de herramientas. Tambi\u00e9n puede activar o desactivar los n\u00fameros de l\u00ednea dentro de las celdas. Aqu\u00ed tambi\u00e9n es donde ir\u00edas si quieres meterte con la barra de herramientas de la celda. Insert : es solo para insertar celdas encima o debajo de la celda seleccionada actualmente. Cell : le permite ejecutar una celda, un grupo de celdas o todas las celdas. Tambi\u00e9n puede ir aqu\u00ed para cambiar el tipo de celda, aunque personalmente considero que la barra de herramientas es m\u00e1s intuitiva para eso. Kernel : es para trabajar con el kernel que se ejecuta en segundo plano. Aqu\u00ed puede reiniciar el kernel, volver a conectarlo, apagarlo o incluso cambiar el kernel que est\u00e1 utilizando su computadora port\u00e1til. Widgets : es para guardar y borrar el estado del widget. Los widgets son b\u00e1sicamente widgets de JavaScript que puede agregar a sus celdas para crear contenido din\u00e1mico utilizando Python (u otro Kernel). Help : es donde debe aprender sobre los atajos de teclado del Notebook, un recorrido por la interfaz de usuario y mucho material de referencia. A medida que se avance en el cuso se entrar\u00e1 en detalle en cada una de estas herramientas. Ahora se ense\u00f1ar\u00e1 las virtudes de los notebooks en aspectos de presentaci\u00f3n tipo latex (markdown) y algunos atajos importantes que nos ayudar\u00e1n en el desarrollo de c\u00f3digos. Markdown Jupyter Notebook permite que escribamos texto formateado, es decir, texto con cursiva, negritas, t\u00edtulos de distintos tama\u00f1os, etc., de forma simple. Para ello Jupyter nos permite usar Markdown , que es un lenguaje de marcado (markup) muy popular. Los lenguajes de markup son lenguajes ideados para procesar texto, algunos de los m\u00e1s conocidos son HTML y \\(\\LaTeX\\) . Markdown tiene como objetivo ser un lenguaje de sintaxis minimalista, simple de aprender y usar; de esa forma uno puede dar formato al texto pero sin perder demasiado tiempo en los detalles. La cantidad de tutoriales en la red sobre Markdown es inmenso, por lo que nos centraremos en indicar las opciones que m\u00e1s se utilizan. Texto en negrita/cursiva : El texto en negrita se indica entre dos pares de asteriscos. De este modo **palabra** aparecer\u00e1 como palabra . Por otro lado, el texto en cursiva se indica entre dos asteriscos simples; es decir *palabra* aparecer\u00e1 como palabra . Listas : Las listas en Markdown se realizan indicando un asterisco o un n\u00famero seguido de un punto si se desean listas numeradas. Markdown organiza autom\u00e1ticamente los items asign\u00e1ndoles el n\u00famero correcto. Inclusi\u00f3n de im\u00e1genes : La sintaxis para incluir im\u00e1genes en Markdown es ![nombre alternativo](direcci\u00f3n de la imagen) en donde el nombre alternativo aparecer\u00e1 en caso de que no se pueda cargar la im\u00e1gen y la direcci\u00f3n puede referirse a una imagen local o un enlace en Internet. Inclusi\u00f3n de c\u00f3digo HTML : El lenguaje Markdown es un subconjunto del lenguaje HTML y en donde se necesite un mayor control del formato, se puede incluir directamente el c\u00f3digo HTML. Enlaces : Las celdas de texto pueden contener enlaces, tanto a otras partes del documento, como a p\u00e1ginas en internet u otros archivos locales. Su sintaxis es [texto](direcci\u00f3n del enlace) . F\u00f3rmulas matem\u00e1ticas : Gracias al uso de MathJax , se puede incluir c\u00f3digo en \\(\\LaTeX\\) para mostrar todo tipo de f\u00f3rmulas y expresiones matem\u00e1ticas. Las f\u00f3rmulas dentro de una l\u00ednea de texto se escriben entre s\u00edmbolos de d\u00f3lar $...$ , mientras que las expresiones separadas del texto utilizan s\u00edmbolos de d\u00f3lar dobles $$...$$ . Los siguientes son ejemplos de f\u00f3rmulas matem\u00e1ticas escritas en \\(\\LaTeX\\) : \\[p(x) = 3x^2 + 5y^2 + x^2y^2\\] \\[e^{\\pi i} - 1 = 0\\] \\[\\lim_{x \\rightarrow \\infty} 3x+1\\] \\[\\sum_{n=1}^\\infty\\frac{1}{n^2}\\] \\[\\int_0^\\infty\\frac{\\sin x}{x}\\,\\mathrm{d}x=\\frac{\\pi}{2}\\] \\[R^2 = \\begin{pmatrix} c & s \\end{pmatrix} \\begin{pmatrix} 1 & 0\\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} c \\\\ s \\end{pmatrix} = c^2 + s^2\\] Algunos atajos importantes Completado mediantes Tabs. La completaci\u00f3n mediante tabs, especialmente para los atributos, es una forma conveniente de explorar la estructura de cualquier objeto con el que est\u00e9 tratando. Simplemente escriba object_name. <TAB> para ver los atributos del objeto. Adem\u00e1s de los objetos y palabras clave de Python, la finalizaci\u00f3n de pesta\u00f1as tambi\u00e9n funciona en nombres de archivos y directorios. import collections collections . # aprete la tecla <\ud835\udc47\ud835\udc34\ud835\udc35> Buscando ayuda En caso de necesitar ayuda sobre cualquier comando de Python , Jupyter nos ofrece una funci\u00f3n llamada help . En resumen, \u00a1suele ser m\u00e1s importante saber como buscar informaci\u00f3n que memorizarla! Por todo esto, Jupyter nos ofrece ayuda sobre cualquier comando agregando un signo de interrogaci\u00f3n ? luego del nombre del comando (y luego ejecutar la celda con la combinaci\u00f3n de teclas SHIFT + ENTER). import numpy as np np . sum ? Magias (magics). Jupyter posee varias funciones m\u00e1gicas predefinidas que sirven para simplificar tareas comunes. Hay dos tipos de magias : Magias por linea (line magics): son comandos que empiezan con el caracter % y que toman como argumentos valores escritos en la misma l\u00ednea. Magias por celda (cell magics): son comandos que empiezan con los caracteres %% , y que reciben argumentos en la misma l\u00ednea y en toda la celda. En general solo se puede usar una sola m\u00e1gias por celda en cada celda y debe ser escrita en la primer linea de la celda . Un buen ejemplo de m\u00e1gia es %lsmagic que lista todas las magias disponibles: % lsmagic Available line magics: %alias %alias_magic %autoawait %autocall %automagic %autosave %bookmark %cd %clear %cls %colors %conda %config %connect_info %copy %ddir %debug %dhist %dirs %doctest_mode %echo %ed %edit %env %gui %hist %history %killbgscripts %ldir %less %load %load_ext %loadpy %logoff %logon %logstart %logstate %logstop %ls %lsmagic %macro %magic %matplotlib %mkdir %more %notebook %page %pastebin %pdb %pdef %pdoc %pfile %pinfo %pinfo2 %pip %popd %pprint %precision %prun %psearch %psource %pushd %pwd %pycat %pylab %qtconsole %quickref %recall %rehashx %reload_ext %ren %rep %rerun %reset %reset_selective %rmdir %run %save %sc %set_env %store %sx %system %tb %time %timeit %unalias %unload_ext %who %who_ls %whos %xdel %xmode Available cell magics: %%! %%HTML %%SVG %%bash %%capture %%cmd %%debug %%file %%html %%javascript %%js %%latex %%markdown %%perl %%prun %%pypy %%python %%python2 %%python3 %%ruby %%script %%sh %%svg %%sx %%system %%time %%timeit %%writefile Automagic is ON, % prefix IS NOT needed for line magics. Tiempo por celda En varias situaciones resulta necesario medir el tiempo de ejecuci\u00f3n de una porci\u00f3n de c\u00f3digo. Para ello podemos usar la magia %timeit . Esta magia est\u00e1 disponible tanto para l\u00ednea como para celda: %% timeit 1 + 1 # timeit repite (adaptativamente) la medici\u00f3n a fin de reducir el error. 15.3 ns \u00b1 0.578 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000000 loops each) Comandos bash Jupyter permite tambi\u00e9n mezclar varios lenguajes de programaci\u00f3n en una misma notebook. Por ejemplo, podr\u00edamos escribir en bash lo siguiente: %%bash for i in { 3 ..1 } ; do echo $i done echo \"Hola desde $BASH \" 3 2 1 Hola desde /bin/bash Referencia Notebook Basics Running the Notebook","title":"Jupyter Notebook"},{"location":"lectures/basic_tools/basic_tools/lecture_014_jupyter/#jupyter-notebook","text":"","title":"Jupyter Notebook"},{"location":"lectures/basic_tools/basic_tools/lecture_014_jupyter/#funciones-basicas","text":"Jupyter Notebook es una herramienta de desarrollo para facilitar la programaci\u00f3n. Est\u00e1 orientada a la Computaci\u00f3n Cient\u00edfica y a la Ciencia de Datos. Es un entorno de computaci\u00f3n interactivo agn\u00f3stico, se puede ocupar en R, python o Julia. Jupyter notebook ofrece muchas herramientas que ser\u00e1n de gran utilidad a lo largo de este curso.","title":"Funciones B\u00e1sicas"},{"location":"lectures/basic_tools/basic_tools/lecture_014_jupyter/#toolbox","text":"Jupyter notebook nos ofrece el siguiente toolbox: File : En \u00e9l, puede crear un nuevo cuaderno o abrir uno preexistente. Aqu\u00ed es tambi\u00e9n a donde ir\u00eda para cambiar el nombre de un Cuaderno. Creo que el elemento de men\u00fa m\u00e1s interesante es la opci\u00f3n Guardar y Checkpoint. Esto le permite crear puntos de control a los que puede retroceder si lo necesita. Edit : Aqu\u00ed puede cortar, copiar y pegar celdas. Aqu\u00ed tambi\u00e9n es donde ir\u00edas si quisieras eliminar, dividir o fusionar una celda. Puede reordenar celdas aqu\u00ed tambi\u00e9n. View : es \u00fatil para alternar la visibilidad del encabezado y la barra de herramientas. Tambi\u00e9n puede activar o desactivar los n\u00fameros de l\u00ednea dentro de las celdas. Aqu\u00ed tambi\u00e9n es donde ir\u00edas si quieres meterte con la barra de herramientas de la celda. Insert : es solo para insertar celdas encima o debajo de la celda seleccionada actualmente. Cell : le permite ejecutar una celda, un grupo de celdas o todas las celdas. Tambi\u00e9n puede ir aqu\u00ed para cambiar el tipo de celda, aunque personalmente considero que la barra de herramientas es m\u00e1s intuitiva para eso. Kernel : es para trabajar con el kernel que se ejecuta en segundo plano. Aqu\u00ed puede reiniciar el kernel, volver a conectarlo, apagarlo o incluso cambiar el kernel que est\u00e1 utilizando su computadora port\u00e1til. Widgets : es para guardar y borrar el estado del widget. Los widgets son b\u00e1sicamente widgets de JavaScript que puede agregar a sus celdas para crear contenido din\u00e1mico utilizando Python (u otro Kernel). Help : es donde debe aprender sobre los atajos de teclado del Notebook, un recorrido por la interfaz de usuario y mucho material de referencia. A medida que se avance en el cuso se entrar\u00e1 en detalle en cada una de estas herramientas. Ahora se ense\u00f1ar\u00e1 las virtudes de los notebooks en aspectos de presentaci\u00f3n tipo latex (markdown) y algunos atajos importantes que nos ayudar\u00e1n en el desarrollo de c\u00f3digos.","title":"Toolbox"},{"location":"lectures/basic_tools/basic_tools/lecture_014_jupyter/#markdown","text":"Jupyter Notebook permite que escribamos texto formateado, es decir, texto con cursiva, negritas, t\u00edtulos de distintos tama\u00f1os, etc., de forma simple. Para ello Jupyter nos permite usar Markdown , que es un lenguaje de marcado (markup) muy popular. Los lenguajes de markup son lenguajes ideados para procesar texto, algunos de los m\u00e1s conocidos son HTML y \\(\\LaTeX\\) . Markdown tiene como objetivo ser un lenguaje de sintaxis minimalista, simple de aprender y usar; de esa forma uno puede dar formato al texto pero sin perder demasiado tiempo en los detalles. La cantidad de tutoriales en la red sobre Markdown es inmenso, por lo que nos centraremos en indicar las opciones que m\u00e1s se utilizan. Texto en negrita/cursiva : El texto en negrita se indica entre dos pares de asteriscos. De este modo **palabra** aparecer\u00e1 como palabra . Por otro lado, el texto en cursiva se indica entre dos asteriscos simples; es decir *palabra* aparecer\u00e1 como palabra . Listas : Las listas en Markdown se realizan indicando un asterisco o un n\u00famero seguido de un punto si se desean listas numeradas. Markdown organiza autom\u00e1ticamente los items asign\u00e1ndoles el n\u00famero correcto. Inclusi\u00f3n de im\u00e1genes : La sintaxis para incluir im\u00e1genes en Markdown es ![nombre alternativo](direcci\u00f3n de la imagen) en donde el nombre alternativo aparecer\u00e1 en caso de que no se pueda cargar la im\u00e1gen y la direcci\u00f3n puede referirse a una imagen local o un enlace en Internet. Inclusi\u00f3n de c\u00f3digo HTML : El lenguaje Markdown es un subconjunto del lenguaje HTML y en donde se necesite un mayor control del formato, se puede incluir directamente el c\u00f3digo HTML. Enlaces : Las celdas de texto pueden contener enlaces, tanto a otras partes del documento, como a p\u00e1ginas en internet u otros archivos locales. Su sintaxis es [texto](direcci\u00f3n del enlace) . F\u00f3rmulas matem\u00e1ticas : Gracias al uso de MathJax , se puede incluir c\u00f3digo en \\(\\LaTeX\\) para mostrar todo tipo de f\u00f3rmulas y expresiones matem\u00e1ticas. Las f\u00f3rmulas dentro de una l\u00ednea de texto se escriben entre s\u00edmbolos de d\u00f3lar $...$ , mientras que las expresiones separadas del texto utilizan s\u00edmbolos de d\u00f3lar dobles $$...$$ . Los siguientes son ejemplos de f\u00f3rmulas matem\u00e1ticas escritas en \\(\\LaTeX\\) : \\[p(x) = 3x^2 + 5y^2 + x^2y^2\\] \\[e^{\\pi i} - 1 = 0\\] \\[\\lim_{x \\rightarrow \\infty} 3x+1\\] \\[\\sum_{n=1}^\\infty\\frac{1}{n^2}\\] \\[\\int_0^\\infty\\frac{\\sin x}{x}\\,\\mathrm{d}x=\\frac{\\pi}{2}\\] \\[R^2 = \\begin{pmatrix} c & s \\end{pmatrix} \\begin{pmatrix} 1 & 0\\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} c \\\\ s \\end{pmatrix} = c^2 + s^2\\]","title":"Markdown"},{"location":"lectures/basic_tools/basic_tools/lecture_014_jupyter/#algunos-atajos-importantes","text":"","title":"Algunos atajos importantes"},{"location":"lectures/basic_tools/basic_tools/lecture_014_jupyter/#completado-mediantes-tabs","text":"La completaci\u00f3n mediante tabs, especialmente para los atributos, es una forma conveniente de explorar la estructura de cualquier objeto con el que est\u00e9 tratando. Simplemente escriba object_name. <TAB> para ver los atributos del objeto. Adem\u00e1s de los objetos y palabras clave de Python, la finalizaci\u00f3n de pesta\u00f1as tambi\u00e9n funciona en nombres de archivos y directorios. import collections collections . # aprete la tecla <\ud835\udc47\ud835\udc34\ud835\udc35>","title":"Completado mediantes Tabs."},{"location":"lectures/basic_tools/basic_tools/lecture_014_jupyter/#buscando-ayuda","text":"En caso de necesitar ayuda sobre cualquier comando de Python , Jupyter nos ofrece una funci\u00f3n llamada help . En resumen, \u00a1suele ser m\u00e1s importante saber como buscar informaci\u00f3n que memorizarla! Por todo esto, Jupyter nos ofrece ayuda sobre cualquier comando agregando un signo de interrogaci\u00f3n ? luego del nombre del comando (y luego ejecutar la celda con la combinaci\u00f3n de teclas SHIFT + ENTER). import numpy as np np . sum ?","title":"Buscando ayuda"},{"location":"lectures/basic_tools/basic_tools/lecture_014_jupyter/#magias-magics","text":"Jupyter posee varias funciones m\u00e1gicas predefinidas que sirven para simplificar tareas comunes. Hay dos tipos de magias : Magias por linea (line magics): son comandos que empiezan con el caracter % y que toman como argumentos valores escritos en la misma l\u00ednea. Magias por celda (cell magics): son comandos que empiezan con los caracteres %% , y que reciben argumentos en la misma l\u00ednea y en toda la celda. En general solo se puede usar una sola m\u00e1gias por celda en cada celda y debe ser escrita en la primer linea de la celda . Un buen ejemplo de m\u00e1gia es %lsmagic que lista todas las magias disponibles: % lsmagic Available line magics: %alias %alias_magic %autoawait %autocall %automagic %autosave %bookmark %cd %clear %cls %colors %conda %config %connect_info %copy %ddir %debug %dhist %dirs %doctest_mode %echo %ed %edit %env %gui %hist %history %killbgscripts %ldir %less %load %load_ext %loadpy %logoff %logon %logstart %logstate %logstop %ls %lsmagic %macro %magic %matplotlib %mkdir %more %notebook %page %pastebin %pdb %pdef %pdoc %pfile %pinfo %pinfo2 %pip %popd %pprint %precision %prun %psearch %psource %pushd %pwd %pycat %pylab %qtconsole %quickref %recall %rehashx %reload_ext %ren %rep %rerun %reset %reset_selective %rmdir %run %save %sc %set_env %store %sx %system %tb %time %timeit %unalias %unload_ext %who %who_ls %whos %xdel %xmode Available cell magics: %%! %%HTML %%SVG %%bash %%capture %%cmd %%debug %%file %%html %%javascript %%js %%latex %%markdown %%perl %%prun %%pypy %%python %%python2 %%python3 %%ruby %%script %%sh %%svg %%sx %%system %%time %%timeit %%writefile Automagic is ON, % prefix IS NOT needed for line magics.","title":"Magias (magics)."},{"location":"lectures/basic_tools/basic_tools/lecture_014_jupyter/#tiempo-por-celda","text":"En varias situaciones resulta necesario medir el tiempo de ejecuci\u00f3n de una porci\u00f3n de c\u00f3digo. Para ello podemos usar la magia %timeit . Esta magia est\u00e1 disponible tanto para l\u00ednea como para celda: %% timeit 1 + 1 # timeit repite (adaptativamente) la medici\u00f3n a fin de reducir el error. 15.3 ns \u00b1 0.578 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000000 loops each)","title":"Tiempo por celda"},{"location":"lectures/basic_tools/basic_tools/lecture_014_jupyter/#comandos-bash","text":"Jupyter permite tambi\u00e9n mezclar varios lenguajes de programaci\u00f3n en una misma notebook. Por ejemplo, podr\u00edamos escribir en bash lo siguiente: %%bash for i in { 3 ..1 } ; do echo $i done echo \"Hola desde $BASH \" 3 2 1 Hola desde /bin/bash","title":"Comandos bash"},{"location":"lectures/basic_tools/basic_tools/lecture_014_jupyter/#referencia","text":"Notebook Basics Running the Notebook","title":"Referencia"},{"location":"lectures/basic_tools/basic_tools/lecture_015_git/","text":"Git Introducci\u00f3n Git es un software de control de versiones dise\u00f1ado por Linus Torvalds, pensando en la eficiencia, la confiabilidad y compatibilidad del mantenimiento de versiones de aplicaciones cuando estas tienen un gran n\u00famero de archivos de c\u00f3digo fuente. Su prop\u00f3sito es llevar registro de los cambios en archivos de computadora incluyendo coordinar el trabajo que varias personas realizan sobre archivos compartidos en un repositorio de c\u00f3digo. Al principio, Git se pens\u00f3 como un motor de bajo nivel sobre el cual otros pudieran escribir la interfaz de usuario o front end como Cogito o StGIT. Sin embargo, Git se ha convertido desde entonces en un sistema de control de versiones con funcionalidad plena. Hay algunos proyectos de mucha relevancia que ya usan Git, en particular, el grupo de programaci\u00f3n del n\u00facleo Linux. El mantenimiento del software Git est\u00e1 actualmente (2009) supervisado por Junio Hamano, quien recibe contribuciones al c\u00f3digo de alrededor de 280 programadores. En cuanto a derechos de autor Git es un software libre distribuible bajo los t\u00e9rminos de la versi\u00f3n 2 de la Licencia P\u00fablica General de GNU. Comandos b\u00e1sicos Configuraci\u00f3n inicial Abra su terminal de Git para comenzar con la ejecuci\u00f3n de comandos, por ejemplo, abrir\u00e1 el programa Git bash en Windows para ingresar a la l\u00ednea de comandos de este programa. Una vez que ingrese, use el siguiente comando para establecer el nombre de usuario de git: git config --global user.name \"tunombre\" ! git config -- global user . name Recuerde sustituir el texto entre comillas por su nombre real. Ahora indique el correo electr\u00f3nico del usuario para git: git config --global user.email \"tuemail\" ! git config -- global user . email crea un repositorio nuevo Crea un directorio nuevo, \u00e1brelo y ejecuta git init para crear un nuevo repositorio de git. hacer checkout a un repositorio Crea una copia local del repositorio ejecutando: git clone /path/to/repository Si utilizas un servidor remoto, ejecuta: git clone username@host:/path/to/repository flujo de trabajo Tu repositorio local esta compuesto por tres \"\u00e1rboles\" administrados por git. El primero es tu Directorio de trabajo que contiene los archivos, el segundo es el Index que actua como una zona intermedia, y el \u00faltimo es el HEAD que apunta al \u00faltimo commit realizado. add & commit Puedes registrar cambios (a\u00f1adirlos al Index) usando git add <filename> git add . Este es el primer paso en el flujo de trabajo b\u00e1sico. Para hacer commit a estos cambios usa git commit -m \"Commit message\" Ahora el archivo esta inclu\u00eddo en el HEAD , pero a\u00fan no en tu repositorio remoto. ramas (branches) Las ramas son utilizadas para desarrollar funcionalidades aisladas unas de otras. La rama master es la rama \"por defecto\" cuando creas un repositorio. Crea nuevas ramas durante el desarrollo y fusi\u00f3nalas a la rama principal cuando termines. Crea una nueva rama llamada \"feature_x\" y c\u00e1mbiate a ella usando git checkout -b feature_x vuelve a la rama principal git checkout master y borra la rama git branch -d feature_x Una rama nueva no estar\u00e1 disponible para los dem\u00e1s a menos que subas (push) la rama a tu repositorio remoto git push origin <branch> actualiza & fusiona (pull & merge) Para actualizar tu repositorio local al commit m\u00e1s nuevo, ejecuta git pull en tu directorio de trabajo para bajar y fusionar los cambios remotos. Para fusionar otra rama a tu rama activa (por ejemplo master), utiliza git merge <branch> en ambos casos git intentar\u00e1 fusionar autom\u00e1ticamente los cambios. Desafortunadamente, no siempre ser\u00e1 posible y se podr\u00e1n producir conflictos. T\u00fa eres responsable de fusionar esos conflictos manualmente al editar los archivos mostrados por git. Despu\u00e9s de modificarlos, necesitas marcarlos como fusionados con git add <filename> Antes de fusionar los cambios, puedes revisarlos usando git diff <source_branch> <target_branch>``` gitignore El archivo .gitignore , es un archivo de texto que le dice a Git qu\u00e9 archivos o carpetas ignorar en un proyecto. Un archivo local .gitignore generalmente se coloca en el directorio ra\u00edz de un proyecto. Tambi\u00e9n puedes crear un archivo global .gitignore , y cualquier entrada en ese archivo se ignorar\u00e1 en todos tus repositorios de Git. Las entradas de este archivo tambi\u00e9n pueden seguir un patr\u00f3n coincidente: * se utiliza como una coincidencia comod\u00edn. / se usa para ignorar las rutas relativas al archivo .gitignore. # es usado para agregar comentarios reemplaza cambios locales En caso de que hagas algo mal (lo que seguramente nunca suceda ;) puedes reemplazar cambios locales usando el comando git checkout -- <filename> Este comando reemplaza los cambios en tu directorio de trabajo con el \u00faltimo contenido de HEAD . Los cambios que ya han sido agregados al Index, as\u00ed como tambi\u00e9n los nuevos archivos, se mantendr\u00e1n sin cambio. Por otro lado, si quieres deshacer todos los cambios locales y commits, puedes traer la \u00faltima versi\u00f3n del servidor y apuntar a tu copia local principal de esta forma git fetch origin git reset --hard origin/master Referencia La gu\u00eda sensilla","title":"Git"},{"location":"lectures/basic_tools/basic_tools/lecture_015_git/#git","text":"","title":"Git"},{"location":"lectures/basic_tools/basic_tools/lecture_015_git/#introduccion","text":"Git es un software de control de versiones dise\u00f1ado por Linus Torvalds, pensando en la eficiencia, la confiabilidad y compatibilidad del mantenimiento de versiones de aplicaciones cuando estas tienen un gran n\u00famero de archivos de c\u00f3digo fuente. Su prop\u00f3sito es llevar registro de los cambios en archivos de computadora incluyendo coordinar el trabajo que varias personas realizan sobre archivos compartidos en un repositorio de c\u00f3digo. Al principio, Git se pens\u00f3 como un motor de bajo nivel sobre el cual otros pudieran escribir la interfaz de usuario o front end como Cogito o StGIT. Sin embargo, Git se ha convertido desde entonces en un sistema de control de versiones con funcionalidad plena. Hay algunos proyectos de mucha relevancia que ya usan Git, en particular, el grupo de programaci\u00f3n del n\u00facleo Linux. El mantenimiento del software Git est\u00e1 actualmente (2009) supervisado por Junio Hamano, quien recibe contribuciones al c\u00f3digo de alrededor de 280 programadores. En cuanto a derechos de autor Git es un software libre distribuible bajo los t\u00e9rminos de la versi\u00f3n 2 de la Licencia P\u00fablica General de GNU.","title":"Introducci\u00f3n"},{"location":"lectures/basic_tools/basic_tools/lecture_015_git/#comandos-basicos","text":"","title":"Comandos b\u00e1sicos"},{"location":"lectures/basic_tools/basic_tools/lecture_015_git/#configuracion-inicial","text":"Abra su terminal de Git para comenzar con la ejecuci\u00f3n de comandos, por ejemplo, abrir\u00e1 el programa Git bash en Windows para ingresar a la l\u00ednea de comandos de este programa. Una vez que ingrese, use el siguiente comando para establecer el nombre de usuario de git: git config --global user.name \"tunombre\" ! git config -- global user . name Recuerde sustituir el texto entre comillas por su nombre real. Ahora indique el correo electr\u00f3nico del usuario para git: git config --global user.email \"tuemail\" ! git config -- global user . email","title":"Configuraci\u00f3n inicial"},{"location":"lectures/basic_tools/basic_tools/lecture_015_git/#crea-un-repositorio-nuevo","text":"Crea un directorio nuevo, \u00e1brelo y ejecuta git init para crear un nuevo repositorio de git.","title":"crea un repositorio nuevo"},{"location":"lectures/basic_tools/basic_tools/lecture_015_git/#hacer-checkout-a-un-repositorio","text":"Crea una copia local del repositorio ejecutando: git clone /path/to/repository Si utilizas un servidor remoto, ejecuta: git clone username@host:/path/to/repository","title":"hacer checkout a un repositorio"},{"location":"lectures/basic_tools/basic_tools/lecture_015_git/#flujo-de-trabajo","text":"Tu repositorio local esta compuesto por tres \"\u00e1rboles\" administrados por git. El primero es tu Directorio de trabajo que contiene los archivos, el segundo es el Index que actua como una zona intermedia, y el \u00faltimo es el HEAD que apunta al \u00faltimo commit realizado.","title":"flujo de trabajo"},{"location":"lectures/basic_tools/basic_tools/lecture_015_git/#add-commit","text":"Puedes registrar cambios (a\u00f1adirlos al Index) usando git add <filename> git add . Este es el primer paso en el flujo de trabajo b\u00e1sico. Para hacer commit a estos cambios usa git commit -m \"Commit message\" Ahora el archivo esta inclu\u00eddo en el HEAD , pero a\u00fan no en tu repositorio remoto.","title":"add &amp; commit"},{"location":"lectures/basic_tools/basic_tools/lecture_015_git/#ramas-branches","text":"Las ramas son utilizadas para desarrollar funcionalidades aisladas unas de otras. La rama master es la rama \"por defecto\" cuando creas un repositorio. Crea nuevas ramas durante el desarrollo y fusi\u00f3nalas a la rama principal cuando termines. Crea una nueva rama llamada \"feature_x\" y c\u00e1mbiate a ella usando git checkout -b feature_x vuelve a la rama principal git checkout master y borra la rama git branch -d feature_x Una rama nueva no estar\u00e1 disponible para los dem\u00e1s a menos que subas (push) la rama a tu repositorio remoto git push origin <branch>","title":"ramas (branches)"},{"location":"lectures/basic_tools/basic_tools/lecture_015_git/#actualiza-fusiona-pull-merge","text":"Para actualizar tu repositorio local al commit m\u00e1s nuevo, ejecuta git pull en tu directorio de trabajo para bajar y fusionar los cambios remotos. Para fusionar otra rama a tu rama activa (por ejemplo master), utiliza git merge <branch> en ambos casos git intentar\u00e1 fusionar autom\u00e1ticamente los cambios. Desafortunadamente, no siempre ser\u00e1 posible y se podr\u00e1n producir conflictos. T\u00fa eres responsable de fusionar esos conflictos manualmente al editar los archivos mostrados por git. Despu\u00e9s de modificarlos, necesitas marcarlos como fusionados con git add <filename> Antes de fusionar los cambios, puedes revisarlos usando git diff <source_branch> <target_branch>```","title":"actualiza &amp; fusiona (pull &amp; merge)"},{"location":"lectures/basic_tools/basic_tools/lecture_015_git/#gitignore","text":"El archivo .gitignore , es un archivo de texto que le dice a Git qu\u00e9 archivos o carpetas ignorar en un proyecto. Un archivo local .gitignore generalmente se coloca en el directorio ra\u00edz de un proyecto. Tambi\u00e9n puedes crear un archivo global .gitignore , y cualquier entrada en ese archivo se ignorar\u00e1 en todos tus repositorios de Git. Las entradas de este archivo tambi\u00e9n pueden seguir un patr\u00f3n coincidente: * se utiliza como una coincidencia comod\u00edn. / se usa para ignorar las rutas relativas al archivo .gitignore. # es usado para agregar comentarios","title":"gitignore"},{"location":"lectures/basic_tools/basic_tools/lecture_015_git/#reemplaza-cambios-locales","text":"En caso de que hagas algo mal (lo que seguramente nunca suceda ;) puedes reemplazar cambios locales usando el comando git checkout -- <filename> Este comando reemplaza los cambios en tu directorio de trabajo con el \u00faltimo contenido de HEAD . Los cambios que ya han sido agregados al Index, as\u00ed como tambi\u00e9n los nuevos archivos, se mantendr\u00e1n sin cambio. Por otro lado, si quieres deshacer todos los cambios locales y commits, puedes traer la \u00faltima versi\u00f3n del servidor y apuntar a tu copia local principal de esta forma git fetch origin git reset --hard origin/master","title":"reemplaza cambios locales"},{"location":"lectures/basic_tools/basic_tools/lecture_015_git/#referencia","text":"La gu\u00eda sensilla","title":"Referencia"},{"location":"lectures/basic_tools/basic_tools/lecture_016_colab/","text":"Google Colab Si ya conoces Colab, echa un vistazo a este v\u00eddeo para obtener informaci\u00f3n sobre las tablas interactivas, la vista del historial de c\u00f3digo ejecutado y la paleta de comandos. \u00bfQu\u00e9 es Colaboratory? Colab, tambi\u00e9n conocido como \"Colaboratory\", te permite programar y ejecutar Python en tu navegador con las siguientes ventajas: - No requiere configuraci\u00f3n - Da acceso gratuito a GPUs - Permite compartir contenido f\u00e1cilmente Colab puede facilitar tu trabajo, ya seas estudiante , cient\u00edfico de datos o investigador de IA . No te pierdas el v\u00eddeo de Introducci\u00f3n a Colab para obtener m\u00e1s informaci\u00f3n. O simplemente empieza con los pasos descritos m\u00e1s abajo. Nota : Para ocupar un notebook de Google Colab, debes preseionar el siguiente icono (a menudo se encuentra en la cabezera del proyecto): Primeros pasos El documento que est\u00e1s leyendo no es una p\u00e1gina web est\u00e1tica, sino un entorno interactivo denominado cuaderno de Colab que te permite escribir y ejecutar c\u00f3digo. Por ejemplo, a continuaci\u00f3n se muestra una celda de c\u00f3digo con una breve secuencia de comandos de Python que calcula un valor, lo almacena en una variable e imprime el resultado: seconds_in_a_day = 24 * 60 * 60 seconds_in_a_day 86400 Si quieres ejecutar el c\u00f3digo de la celda anterior, haz clic para seleccionarlo y pulsa el bot\u00f3n de reproducir situado a la izquierda del c\u00f3digo o usa la combinaci\u00f3n de teclas \"Comando/Ctrl + Intro\". Para editar el c\u00f3digo, solo tienes que hacer clic en la celda. Las variables que definas en una celda se pueden usar despu\u00e9s en otras celdas: seconds_in_a_week = 7 * seconds_in_a_day seconds_in_a_week 604800 Los cuadernos de Colab te permiten combinar c\u00f3digo ejecutable y texto enriquecido en un mismo documento, adem\u00e1s de im\u00e1genes , HTML , LaTeX y mucho m\u00e1s. Los cuadernos que creas en Colab se almacenan en tu cuenta de Google Drive. Puedes compartir tus cuadernos de Colab f\u00e1cilmente con compa\u00f1eros de trabajo o amigos, lo que les permite comentarlos o incluso editarlos. Consulta m\u00e1s informaci\u00f3n en Informaci\u00f3n general sobre Colab . Para crear un cuaderno de Colab, puedes usar el men\u00fa Archivo que aparece arriba o bien acceder al enlace para crear un cuaderno de Colab . Los cuadernos de Colab son cuadernos de Jupyter alojados en Colab. Para obtener m\u00e1s informaci\u00f3n sobre el proyecto Jupyter, visita jupyter.org . Ciencia de datos Con Colab, puedes aprovechar toda la potencia de las bibliotecas m\u00e1s populares de Python para analizar y visualizar datos. La celda de c\u00f3digo de abajo utiliza NumPy para generar datos aleatorios y Matplotlib para visualizarlos. Para editar el c\u00f3digo, solo tienes que hacer clic en la celda. import numpy as np from matplotlib import pyplot as plt ys = 200 + np . random . randn ( 100 ) x = [ x for x in range ( len ( ys ))] plt . plot ( x , ys , '-' ) plt . fill_between ( x , ys , 195 , where = ( ys > 195 ), facecolor = 'g' , alpha = 0.6 ) plt . title ( \"Sample Visualization\" ) plt . show () Puedes importar tus propios datos a los cuadernos de Colab desde tu cuenta de Google Drive, incluidas las hojas de c\u00e1lculo, y tambi\u00e9n desde GitHub y muchas fuentes m\u00e1s. Para obtener m\u00e1s informaci\u00f3n sobre c\u00f3mo importar datos y c\u00f3mo se puede usar Colab en la ciencia de datos, consulta los enlaces que aparecen en la secci\u00f3n Trabajar con datos m\u00e1s abajo. Aprendizaje autom\u00e1tico Con Colab, puedes importar un conjunto de datos de im\u00e1genes, entrenar un clasificador de im\u00e1genes con dicho conjunto de datos y evaluar el modelo con tan solo usar unas pocas l\u00edneas de c\u00f3digo . Los cuadernos de Colab ejecutan c\u00f3digo en los servidores en la nube de Google, lo que te permite aprovechar la potencia del hardware de Google, incluidas las GPU y TPU , independientemente de la potencia de tu equipo. Lo \u00fanico que necesitas es un navegador. Colab es una herramienta muy utilizada en la comunidad de aprendizaje autom\u00e1tico. Estos son algunos ejemplos de las aplicaciones que tiene Colab: - Dar los primeros pasos con TensorFlow - Desarrollar y entrenar redes neuronales - Experimentar con TPUs - Divulgar datos de investigaci\u00f3n sobre IA - Crear tutoriales Para ver cuadernos de Colab que demuestran las aplicaciones del aprendizaje autom\u00e1tico, consulta los ejemplos de aprendizaje autom\u00e1tico de abajo. M\u00e1s recursos Trabajar con cuadernos en Colab Informaci\u00f3n general sobre Colaboratory Gu\u00eda de Markdown Importar bibliotecas e instalar dependencias Guardar y cargar cuadernos en GitHub Formularios interactivos Widgets interactivos TensorFlow 2 en Colab ( Nuevo ) Trabajar con datos Cargar datos: Drive, Hojas de c\u00e1lculo y Google Cloud Storage Gr\u00e1ficos: visualizaci\u00f3n de datos Primeros pasos con BigQuery Curso intensivo de aprendizaje autom\u00e1tico A continuaci\u00f3n, se muestran algunos cuadernos del curso online de Google sobre aprendizaje autom\u00e1tico. Para obtener m\u00e1s informaci\u00f3n, consulta el sitio web del curso completo . - Introducci\u00f3n a Pandas DataFrame - Regresi\u00f3n lineal con tf.keras usando datos sint\u00e9ticos Uso de hardware acelerado TensorFlow con GPUs TensorFlow con TPUs Ejemplos destacados Reemplaza voces con NeMo : usa NeMo, el kit de herramientas de IA conversacional de Nvidia, para sustituir una voz de un fragmento de audio por otra generada por ordenador. Reentrenamiento de un clasificador de im\u00e1genes : crea un modelo de Keras sobre un clasificador de im\u00e1genes preparado previamente para que distinga flores. Clasificaci\u00f3n de textos : clasifica las rese\u00f1as de pel\u00edculas de IMDb en positivas o negativas . Transferencia de estilo : utiliza el aprendizaje profundo para transferir el estilo de una imagen a otra. Codificador universal de frases multiling\u00fce para preguntas y respuestas : utiliza un modelo de aprendizaje autom\u00e1tico para contestar preguntas con el conjunto de datos SQuAD. Interpolaci\u00f3n de v\u00eddeo : predice lo que ocurre entre el primer y el \u00faltimo fotograma de un v\u00eddeo.","title":"Google Colab"},{"location":"lectures/basic_tools/basic_tools/lecture_016_colab/#google-colab","text":"Si ya conoces Colab, echa un vistazo a este v\u00eddeo para obtener informaci\u00f3n sobre las tablas interactivas, la vista del historial de c\u00f3digo ejecutado y la paleta de comandos.","title":"Google Colab"},{"location":"lectures/basic_tools/basic_tools/lecture_016_colab/#que-es-colaboratory","text":"Colab, tambi\u00e9n conocido como \"Colaboratory\", te permite programar y ejecutar Python en tu navegador con las siguientes ventajas: - No requiere configuraci\u00f3n - Da acceso gratuito a GPUs - Permite compartir contenido f\u00e1cilmente Colab puede facilitar tu trabajo, ya seas estudiante , cient\u00edfico de datos o investigador de IA . No te pierdas el v\u00eddeo de Introducci\u00f3n a Colab para obtener m\u00e1s informaci\u00f3n. O simplemente empieza con los pasos descritos m\u00e1s abajo. Nota : Para ocupar un notebook de Google Colab, debes preseionar el siguiente icono (a menudo se encuentra en la cabezera del proyecto):","title":"\u00bfQu\u00e9 es Colaboratory?"},{"location":"lectures/basic_tools/basic_tools/lecture_016_colab/#primeros-pasos","text":"El documento que est\u00e1s leyendo no es una p\u00e1gina web est\u00e1tica, sino un entorno interactivo denominado cuaderno de Colab que te permite escribir y ejecutar c\u00f3digo. Por ejemplo, a continuaci\u00f3n se muestra una celda de c\u00f3digo con una breve secuencia de comandos de Python que calcula un valor, lo almacena en una variable e imprime el resultado: seconds_in_a_day = 24 * 60 * 60 seconds_in_a_day 86400 Si quieres ejecutar el c\u00f3digo de la celda anterior, haz clic para seleccionarlo y pulsa el bot\u00f3n de reproducir situado a la izquierda del c\u00f3digo o usa la combinaci\u00f3n de teclas \"Comando/Ctrl + Intro\". Para editar el c\u00f3digo, solo tienes que hacer clic en la celda. Las variables que definas en una celda se pueden usar despu\u00e9s en otras celdas: seconds_in_a_week = 7 * seconds_in_a_day seconds_in_a_week 604800 Los cuadernos de Colab te permiten combinar c\u00f3digo ejecutable y texto enriquecido en un mismo documento, adem\u00e1s de im\u00e1genes , HTML , LaTeX y mucho m\u00e1s. Los cuadernos que creas en Colab se almacenan en tu cuenta de Google Drive. Puedes compartir tus cuadernos de Colab f\u00e1cilmente con compa\u00f1eros de trabajo o amigos, lo que les permite comentarlos o incluso editarlos. Consulta m\u00e1s informaci\u00f3n en Informaci\u00f3n general sobre Colab . Para crear un cuaderno de Colab, puedes usar el men\u00fa Archivo que aparece arriba o bien acceder al enlace para crear un cuaderno de Colab . Los cuadernos de Colab son cuadernos de Jupyter alojados en Colab. Para obtener m\u00e1s informaci\u00f3n sobre el proyecto Jupyter, visita jupyter.org .","title":"Primeros pasos"},{"location":"lectures/basic_tools/basic_tools/lecture_016_colab/#ciencia-de-datos","text":"Con Colab, puedes aprovechar toda la potencia de las bibliotecas m\u00e1s populares de Python para analizar y visualizar datos. La celda de c\u00f3digo de abajo utiliza NumPy para generar datos aleatorios y Matplotlib para visualizarlos. Para editar el c\u00f3digo, solo tienes que hacer clic en la celda. import numpy as np from matplotlib import pyplot as plt ys = 200 + np . random . randn ( 100 ) x = [ x for x in range ( len ( ys ))] plt . plot ( x , ys , '-' ) plt . fill_between ( x , ys , 195 , where = ( ys > 195 ), facecolor = 'g' , alpha = 0.6 ) plt . title ( \"Sample Visualization\" ) plt . show () Puedes importar tus propios datos a los cuadernos de Colab desde tu cuenta de Google Drive, incluidas las hojas de c\u00e1lculo, y tambi\u00e9n desde GitHub y muchas fuentes m\u00e1s. Para obtener m\u00e1s informaci\u00f3n sobre c\u00f3mo importar datos y c\u00f3mo se puede usar Colab en la ciencia de datos, consulta los enlaces que aparecen en la secci\u00f3n Trabajar con datos m\u00e1s abajo.","title":"Ciencia de datos"},{"location":"lectures/basic_tools/basic_tools/lecture_016_colab/#aprendizaje-automatico","text":"Con Colab, puedes importar un conjunto de datos de im\u00e1genes, entrenar un clasificador de im\u00e1genes con dicho conjunto de datos y evaluar el modelo con tan solo usar unas pocas l\u00edneas de c\u00f3digo . Los cuadernos de Colab ejecutan c\u00f3digo en los servidores en la nube de Google, lo que te permite aprovechar la potencia del hardware de Google, incluidas las GPU y TPU , independientemente de la potencia de tu equipo. Lo \u00fanico que necesitas es un navegador. Colab es una herramienta muy utilizada en la comunidad de aprendizaje autom\u00e1tico. Estos son algunos ejemplos de las aplicaciones que tiene Colab: - Dar los primeros pasos con TensorFlow - Desarrollar y entrenar redes neuronales - Experimentar con TPUs - Divulgar datos de investigaci\u00f3n sobre IA - Crear tutoriales Para ver cuadernos de Colab que demuestran las aplicaciones del aprendizaje autom\u00e1tico, consulta los ejemplos de aprendizaje autom\u00e1tico de abajo.","title":"Aprendizaje autom\u00e1tico"},{"location":"lectures/basic_tools/basic_tools/lecture_016_colab/#mas-recursos","text":"","title":"M\u00e1s recursos"},{"location":"lectures/basic_tools/basic_tools/lecture_016_colab/#trabajar-con-cuadernos-en-colab","text":"Informaci\u00f3n general sobre Colaboratory Gu\u00eda de Markdown Importar bibliotecas e instalar dependencias Guardar y cargar cuadernos en GitHub Formularios interactivos Widgets interactivos TensorFlow 2 en Colab ( Nuevo )","title":"Trabajar con cuadernos en Colab"},{"location":"lectures/basic_tools/basic_tools/lecture_016_colab/#trabajar-con-datos","text":"Cargar datos: Drive, Hojas de c\u00e1lculo y Google Cloud Storage Gr\u00e1ficos: visualizaci\u00f3n de datos Primeros pasos con BigQuery","title":"Trabajar con datos"},{"location":"lectures/basic_tools/basic_tools/lecture_016_colab/#curso-intensivo-de-aprendizaje-automatico","text":"A continuaci\u00f3n, se muestran algunos cuadernos del curso online de Google sobre aprendizaje autom\u00e1tico. Para obtener m\u00e1s informaci\u00f3n, consulta el sitio web del curso completo . - Introducci\u00f3n a Pandas DataFrame - Regresi\u00f3n lineal con tf.keras usando datos sint\u00e9ticos","title":"Curso intensivo de aprendizaje autom\u00e1tico"},{"location":"lectures/basic_tools/basic_tools/lecture_016_colab/#uso-de-hardware-acelerado","text":"TensorFlow con GPUs TensorFlow con TPUs","title":"Uso de hardware acelerado"},{"location":"lectures/basic_tools/basic_tools/lecture_016_colab/#ejemplos-destacados","text":"Reemplaza voces con NeMo : usa NeMo, el kit de herramientas de IA conversacional de Nvidia, para sustituir una voz de un fragmento de audio por otra generada por ordenador. Reentrenamiento de un clasificador de im\u00e1genes : crea un modelo de Keras sobre un clasificador de im\u00e1genes preparado previamente para que distinga flores. Clasificaci\u00f3n de textos : clasifica las rese\u00f1as de pel\u00edculas de IMDb en positivas o negativas . Transferencia de estilo : utiliza el aprendizaje profundo para transferir el estilo de una imagen a otra. Codificador universal de frases multiling\u00fce para preguntas y respuestas : utiliza un modelo de aprendizaje autom\u00e1tico para contestar preguntas con el conjunto de datos SQuAD. Interpolaci\u00f3n de v\u00eddeo : predice lo que ocurre entre el primer y el \u00faltimo fotograma de un v\u00eddeo.","title":"Ejemplos destacados"},{"location":"lectures/data_manipulation/data_manipulation/base_datos/","text":"Base de datos \u00bfTe imaginas como las grandes compa\u00f1\u00edas o gobiernos almacenan sus datos?. No, no es en un excel gigante en un pendrive. Es importante saber como modificar o crear valor a trav\u00e9s de distintas tablas de datos, por lo que esta clase se centrar\u00e1 en hacer esto, motivando a partir del uso de bases de datos relacionales. Una Base de Datos es un conjunto de datos almacenados en una computadora (generalmente un servidor, m\u00e1quina virtual, etc.) y que poseen una estructura tal que sean de f\u00e1cil acceso. Base de Datos Relacional Es el tipo de base de datos m\u00e1s ampliamente utilizado, aunque existen otros tipos de bases de datos para fines espec\u00edficos. Utiliza una estructura tal que es posible identificar y acceder a datos relacionados entre si. Generalmente una base de datos relacional est\u00e1 organizada en tablas . Las tablas est\u00e1n conformadas de filas y columnas. Cada columna posee un nombre y tiene un tipo de dato espec\u00edfico, mientras que las filas son registros almacenados. Por ejemplo, la siguiente tabla tiene tres columnas y cuatro registros. En particular, la columna age tiene tipo INTEGER y las otras dos tipo STRING . \u00bfEste formato de datos te parece familar? \u00bfQu\u00e9 es SQL? Sus siglas significan Structured Query Language (Lenguaje de Consulta Estructurada) es un lenguaje de programaci\u00f3n utilizado para comunicarse con datos almacenados en un Sistema de Gesti\u00f3n de Bases de Datos Relacionales ( Relational Database Management System o RDBMS). Posee una sintaxis muy similar al idioma ingl\u00e9s, con lo cual se hace relativamente f\u00e1cil de escribir, leer e interpretar. Hay distintos RDBMS entre los cuales la sintaxis de SQL difiere ligeramente. Los m\u00e1s populares son: SQLite MySQL / MariaDB PostgreSQL Oracle DB SQL Server En una empresa de tecnolog\u00eda hay cargos especialmente destinados a todo lo que tenga que ver con bases de datos, por ejemplo: creaci\u00f3n, mantenci\u00f3n, actualizaci\u00f3n, obtenci\u00f3n de datos, transformaci\u00f3n, seguridad y un largo etc. Los matem\u00e1ticos en la industria suelen tener cargos como Data Scientist , Data Analyst , Data Statistician , Data X (reemplace X con algo fancy tal de formar un cargo que quede bien en Linkedin), en donde lo importante es otorgar valor a estos datos. Por ende, lo m\u00ednimo que deben satisfacer es: Entendimiento casi total del modelo de datos (tablas, relaciones, tipos, etc.) Seleccionar datos a medida ( queries ). Modelo de datos Es la forma en que se organizan los datos. En las bases de datos incluso es posible conocer las relaciones entre tablas. A menudo se presentan gr\u00e1ficamente como en la imagen de abajo (esta ser\u00e1 la base de datos que utilizaremos en los ejericios del d\u00eda de Esta base de datos se conoce con el nombre de chinook database . La descripci\u00f3n y las im\u00e1genes se pueden encontrar aqu\u00ed . En la figura anterior, existen algunas columnas especiales con una llave al lado de su nombre. \u00bfQu\u00e9 crees que significan? Las 11 tablas se definen de la siguiente forma (en ingl\u00e9s): employees table stores employees data such as employee id, last name, first name, etc. It also has a field named ReportsTo to specify who reports to whom. customers table stores customers data. invoices & invoice_items tables: these two tables store invoice data. The invoices table stores invoice header data and the invoice_items table stores the invoice line items data. artists table stores artists data. It is a simple table that contains only artist id and name. albums table stores data about a list of tracks. Each album belongs to one artist. However, one artist may have multiple albums. media_types table stores media types such as MPEG audio file, ACC audio file, etc. genres table stores music types such as rock, jazz, metal, etc. tracks table store the data of songs. Each track belongs to one album. playlists & playlist_track tables : playlists table store data about playlists. Each playlist contains a list of tracks. Each track may belong to multiple playlists. The relationship between the playlists table and tracks table is many-to-many. The playlist_track table is used to reflect this relationship.","title":"Introducci\u00f3n"},{"location":"lectures/data_manipulation/data_manipulation/base_datos/#base-de-datos","text":"\u00bfTe imaginas como las grandes compa\u00f1\u00edas o gobiernos almacenan sus datos?. No, no es en un excel gigante en un pendrive. Es importante saber como modificar o crear valor a trav\u00e9s de distintas tablas de datos, por lo que esta clase se centrar\u00e1 en hacer esto, motivando a partir del uso de bases de datos relacionales. Una Base de Datos es un conjunto de datos almacenados en una computadora (generalmente un servidor, m\u00e1quina virtual, etc.) y que poseen una estructura tal que sean de f\u00e1cil acceso.","title":"Base de datos"},{"location":"lectures/data_manipulation/data_manipulation/base_datos/#base-de-datos-relacional","text":"Es el tipo de base de datos m\u00e1s ampliamente utilizado, aunque existen otros tipos de bases de datos para fines espec\u00edficos. Utiliza una estructura tal que es posible identificar y acceder a datos relacionados entre si. Generalmente una base de datos relacional est\u00e1 organizada en tablas . Las tablas est\u00e1n conformadas de filas y columnas. Cada columna posee un nombre y tiene un tipo de dato espec\u00edfico, mientras que las filas son registros almacenados. Por ejemplo, la siguiente tabla tiene tres columnas y cuatro registros. En particular, la columna age tiene tipo INTEGER y las otras dos tipo STRING . \u00bfEste formato de datos te parece familar?","title":"Base de Datos Relacional"},{"location":"lectures/data_manipulation/data_manipulation/base_datos/#que-es-sql","text":"Sus siglas significan Structured Query Language (Lenguaje de Consulta Estructurada) es un lenguaje de programaci\u00f3n utilizado para comunicarse con datos almacenados en un Sistema de Gesti\u00f3n de Bases de Datos Relacionales ( Relational Database Management System o RDBMS). Posee una sintaxis muy similar al idioma ingl\u00e9s, con lo cual se hace relativamente f\u00e1cil de escribir, leer e interpretar. Hay distintos RDBMS entre los cuales la sintaxis de SQL difiere ligeramente. Los m\u00e1s populares son: SQLite MySQL / MariaDB PostgreSQL Oracle DB SQL Server En una empresa de tecnolog\u00eda hay cargos especialmente destinados a todo lo que tenga que ver con bases de datos, por ejemplo: creaci\u00f3n, mantenci\u00f3n, actualizaci\u00f3n, obtenci\u00f3n de datos, transformaci\u00f3n, seguridad y un largo etc. Los matem\u00e1ticos en la industria suelen tener cargos como Data Scientist , Data Analyst , Data Statistician , Data X (reemplace X con algo fancy tal de formar un cargo que quede bien en Linkedin), en donde lo importante es otorgar valor a estos datos. Por ende, lo m\u00ednimo que deben satisfacer es: Entendimiento casi total del modelo de datos (tablas, relaciones, tipos, etc.) Seleccionar datos a medida ( queries ).","title":"\u00bfQu\u00e9 es SQL?"},{"location":"lectures/data_manipulation/data_manipulation/base_datos/#modelo-de-datos","text":"Es la forma en que se organizan los datos. En las bases de datos incluso es posible conocer las relaciones entre tablas. A menudo se presentan gr\u00e1ficamente como en la imagen de abajo (esta ser\u00e1 la base de datos que utilizaremos en los ejericios del d\u00eda de Esta base de datos se conoce con el nombre de chinook database . La descripci\u00f3n y las im\u00e1genes se pueden encontrar aqu\u00ed . En la figura anterior, existen algunas columnas especiales con una llave al lado de su nombre. \u00bfQu\u00e9 crees que significan? Las 11 tablas se definen de la siguiente forma (en ingl\u00e9s): employees table stores employees data such as employee id, last name, first name, etc. It also has a field named ReportsTo to specify who reports to whom. customers table stores customers data. invoices & invoice_items tables: these two tables store invoice data. The invoices table stores invoice header data and the invoice_items table stores the invoice line items data. artists table stores artists data. It is a simple table that contains only artist id and name. albums table stores data about a list of tracks. Each album belongs to one artist. However, one artist may have multiple albums. media_types table stores media types such as MPEG audio file, ACC audio file, etc. genres table stores music types such as rock, jazz, metal, etc. tracks table store the data of songs. Each track belongs to one album. playlists & playlist_track tables : playlists table store data about playlists. Each playlist contains a list of tracks. Each track may belong to multiple playlists. The relationship between the playlists table and tracks table is many-to-many. The playlist_track table is used to reflect this relationship.","title":"Modelo de datos"},{"location":"lectures/data_manipulation/data_manipulation/groupby/","text":"Groupby Groupby es un concepto bastante simple. Podemos crear una agrupaci\u00f3n de categor\u00edas y aplicar una funci\u00f3n a las categor\u00edas. El proceso de groupby se puede resumiren los siguientes pasos: Divisi\u00f3n : es un proceso en el que dividimos los datos en grupos aplicando algunas condiciones en los conjuntos de datos. Aplicaci\u00f3n : es un proceso en el que aplicamos una funci\u00f3n a cada grupo de forma independiente Combinaci\u00f3n : es un proceso en el que combinamos diferentes conjuntos de datos despu\u00e9s de aplicar groupby y resultados en una estructura de datos Despu\u00e9s de dividir los datos en un grupo, aplicamos una funci\u00f3n a cada grupo para realizar algunas operaciones que son: Agregaci\u00f3n : es un proceso en el que calculamos una estad\u00edstica resumida (o estad\u00edstica) sobre cada grupo. Por ejemplo, Calcular sumas de grupo o medios Transformaci\u00f3n : es un proceso en el que realizamos algunos c\u00e1lculos espec\u00edficos del grupo y devolvemos un \u00edndice similar. Por ejemplo, llenar NA dentro de grupos con un valor derivado de cada grupo Filtraci\u00f3n : es un proceso en el cual descartamos algunos grupos, de acuerdo con un c\u00e1lculo grupal que eval\u00faa Verdadero o Falso. Por ejemplo, Filtrar datos en funci\u00f3n de la suma o media grupal Aplicaci\u00f3n Para comprender mejor el concepto de agrupaci\u00f3n de tablas, se realiza un ejercicio simple sobre el conjunto de datos pokemon.csv # libreria import pandas as pd import numpy as np import os # cargar datos pokemon_data = pd . read_csv ( os . path . join ( \"data\" , \"pokemon.csv\" ), sep = \",\" ) pokemon_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # Name Type 1 Type 2 HP Attack Defense Sp. Atk Sp. Def Speed Generation Legendary 0 1 Bulbasaur Grass Poison 45 49 49 65 65 45 1 False 1 2 Ivysaur Grass Poison 60 62 63 80 80 60 1 False 2 3 Venusaur Grass Poison 80 82 83 100 100 80 1 False 3 4 Mega Venusaur Grass Poison 80 100 123 122 120 80 1 False 4 5 Charmander Fire NaN 39 52 43 60 50 65 1 False # renombrar columnas pokemon_data . columns = pokemon_data . columns . str . lower () . str . replace ( '.' , '_' ) . str . replace ( ' ' , '_' ) #change into upper case pokemon_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # name type_1 type_2 hp attack defense sp__atk sp__def speed generation legendary 0 1 Bulbasaur Grass Poison 45 49 49 65 65 45 1 False 1 2 Ivysaur Grass Poison 60 62 63 80 80 60 1 False 2 3 Venusaur Grass Poison 80 82 83 100 100 80 1 False 3 4 Mega Venusaur Grass Poison 80 100 123 122 120 80 1 False 4 5 Charmander Fire NaN 39 52 43 60 50 65 1 False N\u00famero de pokemones por generaci\u00f3n grupo = pokemon_data . groupby ( 'generation' ) grupo [[ 'name' ]] . count () . reset_index () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } generation name 0 1 165 1 2 106 2 3 160 3 4 121 4 5 165 5 6 82 N\u00famero de pokemones por par Tipo I y Tipo II grupo = pokemon_data . groupby ([ 'type_1' , 'type_2' ]) grupo [ 'name' ] . count () . reset_index () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type_1 type_2 name 0 Bug Electric 2 1 Bug Fighting 2 2 Bug Fire 2 3 Bug Flying 14 4 Bug Ghost 1 ... ... ... ... 131 Water Ice 3 132 Water Poison 3 133 Water Psychic 5 134 Water Rock 4 135 Water Steel 1 136 rows \u00d7 3 columns Calcular hp promedio y hp total agrupados si el pokemon es legendario o no m\u00e9todo 01: ocupando el comando agg # metodo 01: ocupando el comando agg grupo = pokemon_data . groupby ([ 'legendary' ]) df_leng = grupo . agg ({ 'hp' :[ np . mean , sum ]}) . reset_index () df_leng .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } legendary hp mean sum 0 False 67.182313 49379 1 True 92.738462 6028 m\u00e9todo 02: ocupando el comando apply # metodo 02: ocupando el comando apply def my_custom_function ( x ): \"\"\" Funcion que calcula el hp promedio y total \"\"\" names = { 'mean' : x [ 'hp' ] . mean (), 'sum' : x [ 'hp' ] . sum ()} return pd . Series ( names , index = [ 'mean' , 'sum' ]) grupo = pokemon_data . groupby ([ 'legendary' ]) df_leng = grupo . apply ( my_custom_function ) . reset_index () df_leng .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } legendary mean sum 0 False 67.182313 49379.0 1 True 92.738462 6028.0 Normalizar las estad\u00edsticas agrupados por generaci\u00f3n cols_statistics = [ 'generation' , 'hp' , 'attack' , 'defense' , 'sp__atk' , 'sp__def' , 'speed' ] grupo = pokemon_data [ cols_statistics ] . groupby ( 'generation' ) sc = lambda x : ( x - x . mean ()) / x . std () grupo . transform ( sc ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } hp attack defense sp__atk sp__def speed 0 -0.739479 -0.898969 -0.763283 -0.198010 -0.160373 -0.929521 1 -0.206695 -0.476132 -0.274479 0.237542 0.427740 -0.424060 2 0.503685 0.174386 0.423812 0.818277 1.211892 0.249889 3 0.503685 0.759852 1.820395 1.457086 1.996043 0.249889 4 -0.952593 -0.801391 -0.972770 -0.343193 -0.748487 -0.255573 ... ... ... ... ... ... ... 795 -0.873754 0.829182 2.337149 0.808641 2.496477 -0.639851 796 -0.873754 2.885421 1.062058 2.695980 1.166968 1.695510 797 0.561116 1.171889 -0.531806 2.381423 1.831723 0.138603 798 0.561116 2.885421 -0.531806 3.010536 1.831723 0.527830 799 0.561116 1.171889 1.380831 1.752310 0.502214 0.138603 800 rows \u00d7 6 columns Identificar generaciones que tienen menos de 100 pokemones grupo = pokemon_data [[ 'name' , 'generation' ]] . groupby ( 'generation' ) grupo . filter ( lambda x : len ( x [ 'name' ]) < 150 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name generation 166 Chikorita 2 167 Bayleef 2 168 Meganium 2 169 Cyndaquil 2 170 Quilava 2 ... ... ... 795 Diancie 6 796 Mega Diancie 6 797 Hoopa Confined 6 798 Hoopa Unbound 6 799 Volcanion 6 309 rows \u00d7 2 columns Referencia Groupby","title":"Groupby"},{"location":"lectures/data_manipulation/data_manipulation/groupby/#groupby","text":"Groupby es un concepto bastante simple. Podemos crear una agrupaci\u00f3n de categor\u00edas y aplicar una funci\u00f3n a las categor\u00edas. El proceso de groupby se puede resumiren los siguientes pasos: Divisi\u00f3n : es un proceso en el que dividimos los datos en grupos aplicando algunas condiciones en los conjuntos de datos. Aplicaci\u00f3n : es un proceso en el que aplicamos una funci\u00f3n a cada grupo de forma independiente Combinaci\u00f3n : es un proceso en el que combinamos diferentes conjuntos de datos despu\u00e9s de aplicar groupby y resultados en una estructura de datos Despu\u00e9s de dividir los datos en un grupo, aplicamos una funci\u00f3n a cada grupo para realizar algunas operaciones que son: Agregaci\u00f3n : es un proceso en el que calculamos una estad\u00edstica resumida (o estad\u00edstica) sobre cada grupo. Por ejemplo, Calcular sumas de grupo o medios Transformaci\u00f3n : es un proceso en el que realizamos algunos c\u00e1lculos espec\u00edficos del grupo y devolvemos un \u00edndice similar. Por ejemplo, llenar NA dentro de grupos con un valor derivado de cada grupo Filtraci\u00f3n : es un proceso en el cual descartamos algunos grupos, de acuerdo con un c\u00e1lculo grupal que eval\u00faa Verdadero o Falso. Por ejemplo, Filtrar datos en funci\u00f3n de la suma o media grupal","title":"Groupby"},{"location":"lectures/data_manipulation/data_manipulation/groupby/#aplicacion","text":"Para comprender mejor el concepto de agrupaci\u00f3n de tablas, se realiza un ejercicio simple sobre el conjunto de datos pokemon.csv # libreria import pandas as pd import numpy as np import os # cargar datos pokemon_data = pd . read_csv ( os . path . join ( \"data\" , \"pokemon.csv\" ), sep = \",\" ) pokemon_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # Name Type 1 Type 2 HP Attack Defense Sp. Atk Sp. Def Speed Generation Legendary 0 1 Bulbasaur Grass Poison 45 49 49 65 65 45 1 False 1 2 Ivysaur Grass Poison 60 62 63 80 80 60 1 False 2 3 Venusaur Grass Poison 80 82 83 100 100 80 1 False 3 4 Mega Venusaur Grass Poison 80 100 123 122 120 80 1 False 4 5 Charmander Fire NaN 39 52 43 60 50 65 1 False # renombrar columnas pokemon_data . columns = pokemon_data . columns . str . lower () . str . replace ( '.' , '_' ) . str . replace ( ' ' , '_' ) #change into upper case pokemon_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # name type_1 type_2 hp attack defense sp__atk sp__def speed generation legendary 0 1 Bulbasaur Grass Poison 45 49 49 65 65 45 1 False 1 2 Ivysaur Grass Poison 60 62 63 80 80 60 1 False 2 3 Venusaur Grass Poison 80 82 83 100 100 80 1 False 3 4 Mega Venusaur Grass Poison 80 100 123 122 120 80 1 False 4 5 Charmander Fire NaN 39 52 43 60 50 65 1 False","title":"Aplicaci\u00f3n"},{"location":"lectures/data_manipulation/data_manipulation/groupby/#numero-de-pokemones-por-generacion","text":"grupo = pokemon_data . groupby ( 'generation' ) grupo [[ 'name' ]] . count () . reset_index () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } generation name 0 1 165 1 2 106 2 3 160 3 4 121 4 5 165 5 6 82","title":"N\u00famero de pokemones por generaci\u00f3n"},{"location":"lectures/data_manipulation/data_manipulation/groupby/#numero-de-pokemones-por-par-tipo-i-y-tipo-ii","text":"grupo = pokemon_data . groupby ([ 'type_1' , 'type_2' ]) grupo [ 'name' ] . count () . reset_index () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type_1 type_2 name 0 Bug Electric 2 1 Bug Fighting 2 2 Bug Fire 2 3 Bug Flying 14 4 Bug Ghost 1 ... ... ... ... 131 Water Ice 3 132 Water Poison 3 133 Water Psychic 5 134 Water Rock 4 135 Water Steel 1 136 rows \u00d7 3 columns","title":"N\u00famero de pokemones  por par Tipo I y Tipo II"},{"location":"lectures/data_manipulation/data_manipulation/groupby/#calcular-hp-promedio-y-hp-total-agrupados-si-el-pokemon-es-legendario-o-no","text":"","title":"Calcular hp promedio y hp total agrupados si el pokemon es legendario o no"},{"location":"lectures/data_manipulation/data_manipulation/groupby/#metodo-01-ocupando-el-comando-agg","text":"# metodo 01: ocupando el comando agg grupo = pokemon_data . groupby ([ 'legendary' ]) df_leng = grupo . agg ({ 'hp' :[ np . mean , sum ]}) . reset_index () df_leng .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } legendary hp mean sum 0 False 67.182313 49379 1 True 92.738462 6028","title":"m\u00e9todo 01: ocupando el comando agg"},{"location":"lectures/data_manipulation/data_manipulation/groupby/#metodo-02-ocupando-el-comando-apply","text":"# metodo 02: ocupando el comando apply def my_custom_function ( x ): \"\"\" Funcion que calcula el hp promedio y total \"\"\" names = { 'mean' : x [ 'hp' ] . mean (), 'sum' : x [ 'hp' ] . sum ()} return pd . Series ( names , index = [ 'mean' , 'sum' ]) grupo = pokemon_data . groupby ([ 'legendary' ]) df_leng = grupo . apply ( my_custom_function ) . reset_index () df_leng .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } legendary mean sum 0 False 67.182313 49379.0 1 True 92.738462 6028.0","title":"m\u00e9todo 02: ocupando el comando apply"},{"location":"lectures/data_manipulation/data_manipulation/groupby/#normalizar-las-estadisticas-agrupados-por-generacion","text":"cols_statistics = [ 'generation' , 'hp' , 'attack' , 'defense' , 'sp__atk' , 'sp__def' , 'speed' ] grupo = pokemon_data [ cols_statistics ] . groupby ( 'generation' ) sc = lambda x : ( x - x . mean ()) / x . std () grupo . transform ( sc ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } hp attack defense sp__atk sp__def speed 0 -0.739479 -0.898969 -0.763283 -0.198010 -0.160373 -0.929521 1 -0.206695 -0.476132 -0.274479 0.237542 0.427740 -0.424060 2 0.503685 0.174386 0.423812 0.818277 1.211892 0.249889 3 0.503685 0.759852 1.820395 1.457086 1.996043 0.249889 4 -0.952593 -0.801391 -0.972770 -0.343193 -0.748487 -0.255573 ... ... ... ... ... ... ... 795 -0.873754 0.829182 2.337149 0.808641 2.496477 -0.639851 796 -0.873754 2.885421 1.062058 2.695980 1.166968 1.695510 797 0.561116 1.171889 -0.531806 2.381423 1.831723 0.138603 798 0.561116 2.885421 -0.531806 3.010536 1.831723 0.527830 799 0.561116 1.171889 1.380831 1.752310 0.502214 0.138603 800 rows \u00d7 6 columns","title":"Normalizar las estad\u00edsticas agrupados por generaci\u00f3n"},{"location":"lectures/data_manipulation/data_manipulation/groupby/#identificar-generaciones-que-tienen-menos-de-100-pokemones","text":"grupo = pokemon_data [[ 'name' , 'generation' ]] . groupby ( 'generation' ) grupo . filter ( lambda x : len ( x [ 'name' ]) < 150 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name generation 166 Chikorita 2 167 Bayleef 2 168 Meganium 2 169 Cyndaquil 2 170 Quilava 2 ... ... ... 795 Diancie 6 796 Mega Diancie 6 797 Hoopa Confined 6 798 Hoopa Unbound 6 799 Volcanion 6 309 rows \u00d7 2 columns","title":"Identificar generaciones  que tienen menos de 100 pokemones"},{"location":"lectures/data_manipulation/data_manipulation/groupby/#referencia","text":"Groupby","title":"Referencia"},{"location":"lectures/data_manipulation/data_manipulation/merge_concat/","text":"Merge & Concat En muchas ocasiones nos podemos encontrar con que los conjuntos de datos no se encuentran agregados en una \u00fanica tabla. Cuando esto sucede, existen dos formas para unir la informaci\u00f3n de distintas tablas: merge y concat . Concat La funci\u00f3n concat() realiza todo el trabajo pesado de realizar operaciones de concatenaci\u00f3n a lo largo de un eje mientras realiza la l\u00f3gica de conjunto opcional (uni\u00f3n o intersecci\u00f3n) de los \u00edndices (si los hay) en los otros ejes. Tenga en cuenta que digo \"si hay alguno\" porque solo hay un \u00fanico eje posible de concatenaci\u00f3n para Series. Concatenar varias tablas con las mismas columnas import os import numpy as pd import pandas as pd df1 = pd . DataFrame ({ 'A' : [ 'A0' , 'A1' , 'A2' , 'A3' ], 'B' : [ 'B0' , 'B1' , 'B2' , 'B3' ], 'C' : [ 'C0' , 'C1' , 'C2' , 'C3' ], 'D' : [ 'D0' , 'D1' , 'D2' , 'D3' ]}, index = [ 0 , 1 , 2 , 3 ]) df2 = pd . DataFrame ({ 'A' : [ 'A4' , 'A5' , 'A6' , 'A7' ], 'B' : [ 'B4' , 'B5' , 'B6' , 'B7' ], 'C' : [ 'C4' , 'C5' , 'C6' , 'C7' ], 'D' : [ 'D4' , 'D5' , 'D6' , 'D7' ]}, index = [ 4 , 5 , 6 , 7 ]) df3 = pd . DataFrame ({ 'A' : [ 'A8' , 'A9' , 'A10' , 'A11' ], 'B' : [ 'B8' , 'B9' , 'B10' , 'B11' ], 'C' : [ 'C8' , 'C9' , 'C10' , 'C11' ], 'D' : [ 'D8' , 'D9' , 'D10' , 'D11' ]}, index = [ 8 , 9 , 10 , 11 ]) frames = [ df1 , df2 , df3 ] result = pd . concat ( frames ) Concatenar varias tablas con distintas columnas (por filas) df4 = pd . DataFrame ({ 'B1' : [ 'hola' , 'B3' , 'B6' , 'B7' ], 'D' : [ 'D2' , 'D3' , 'D6' , 'D7' ], 'F' : [ 'F2' , 'F3' , 'F6' , 'F7' ]}, index = [ 2 , 3 , 6 , 7 ]) result = pd . concat ([ df1 , df4 ], axis = 0 , sort = False ) Concatenar varias tablas con distintas columnas (por columnas) result = pd . concat ([ df1 , df4 ], axis = 1 , sort = False ) Merge La funci\u00f3n merge() se usa para combinar dos (o m\u00e1s) tablas sobre valores de columnas comunes (keys). left = pd . DataFrame ({ 'key' : [ 'K0' , 'K1' , 'K2' , 'K3' ], 'A' : [ 'A0' , 'A1' , 'A2' , 'A3' ], 'B' : [ 'B0' , 'B1' , 'B2' , 'B3' ]}) right = pd . DataFrame ({ 'key' : [ 'K0' , 'K1' , 'K2' , 'K3' ], 'C' : [ 'C0' , 'C1' , 'C2' , 'C3' ], 'D' : [ 'D0' , 'D1' , 'D2' , 'D3' ]}) result = pd . merge ( left , right , on = 'key' ) En este ejemplo, se especifica en la opci\u00f3n on las columnas (keys) donde se realizar\u00e1 el cruce de informaci\u00f3n de ambas tablas. Tipos de merge La opci\u00f3n how especificica el tipo de cruce que se realizar\u00e1. left : usa las llaves solo de la tabla izquierda right : usa las llaves solo de la tabla derecha outer : usa las llaves de la uni\u00f3n de ambas tablas. inner : usa las llaves de la intersecci\u00f3n de ambas tablas. left = pd . DataFrame ({ 'key1' : [ 'K0' , 'K0' , 'K1' , 'K2' ], 'key2' : [ 'K0' , 'K1' , 'K0' , 'K1' ], 'A' : [ 'A0' , 'A1' , 'A2' , 'A3' ], 'B' : [ 'B0' , 'B1' , 'B2' , 'B3' ]}) right = pd . DataFrame ({ 'key1' : [ 'K0' , 'K1' , 'K1' , 'K2' ], 'key2' : [ 'K0' , 'K0' , 'K0' , 'K0' ], 'C' : [ 'C0' , 'C1' , 'C2' , 'C3' ], 'D' : [ 'D0' , 'D1' , 'D2' , 'D3' ]}) Merge left merge_left = pd . merge ( left , right , how = 'left' , on = [ 'key1' , 'key2' ]) Merge right merge_rigth = pd . merge ( left , right , how = 'right' , on = [ 'key1' , 'key2' ]) Merge outer merge_outer = pd . merge ( left , right , how = 'outer' , on = [ 'key1' , 'key2' ]) Merge inner merge_inner = pd . merge ( left , right , how = 'inner' , on = [ 'key1' , 'key2' ]) Problemas de llaves duplicadas Cuando se quiere realizar el cruce de dos tablas, pero an ambas tablas existe una columna (key) con el mismo nombre, para diferenciar la informaci\u00f3n entre la columna de una tabla y otra, pandas devulve el nombre de la columna con un gui\u00f3n bajo x (key_x) y otra con un gui\u00f3n bajo y (key_y) left = pd . DataFrame ({ 'A' : [ 1 , 2 ], 'B' : [ 2 , 2 ]}) right = pd . DataFrame ({ 'A' : [ 4 , 5 , 6 ], 'B' : [ 2 , 2 , 2 ]}) result = pd . merge ( left , right , on = 'B' , how = 'outer' ) Referencia Merge, join, and concatenate","title":"Merge & Concat"},{"location":"lectures/data_manipulation/data_manipulation/merge_concat/#merge-concat","text":"En muchas ocasiones nos podemos encontrar con que los conjuntos de datos no se encuentran agregados en una \u00fanica tabla. Cuando esto sucede, existen dos formas para unir la informaci\u00f3n de distintas tablas: merge y concat .","title":"Merge &amp; Concat"},{"location":"lectures/data_manipulation/data_manipulation/merge_concat/#concat","text":"La funci\u00f3n concat() realiza todo el trabajo pesado de realizar operaciones de concatenaci\u00f3n a lo largo de un eje mientras realiza la l\u00f3gica de conjunto opcional (uni\u00f3n o intersecci\u00f3n) de los \u00edndices (si los hay) en los otros ejes. Tenga en cuenta que digo \"si hay alguno\" porque solo hay un \u00fanico eje posible de concatenaci\u00f3n para Series.","title":"Concat"},{"location":"lectures/data_manipulation/data_manipulation/merge_concat/#concatenar-varias-tablas-con-las-mismas-columnas","text":"import os import numpy as pd import pandas as pd df1 = pd . DataFrame ({ 'A' : [ 'A0' , 'A1' , 'A2' , 'A3' ], 'B' : [ 'B0' , 'B1' , 'B2' , 'B3' ], 'C' : [ 'C0' , 'C1' , 'C2' , 'C3' ], 'D' : [ 'D0' , 'D1' , 'D2' , 'D3' ]}, index = [ 0 , 1 , 2 , 3 ]) df2 = pd . DataFrame ({ 'A' : [ 'A4' , 'A5' , 'A6' , 'A7' ], 'B' : [ 'B4' , 'B5' , 'B6' , 'B7' ], 'C' : [ 'C4' , 'C5' , 'C6' , 'C7' ], 'D' : [ 'D4' , 'D5' , 'D6' , 'D7' ]}, index = [ 4 , 5 , 6 , 7 ]) df3 = pd . DataFrame ({ 'A' : [ 'A8' , 'A9' , 'A10' , 'A11' ], 'B' : [ 'B8' , 'B9' , 'B10' , 'B11' ], 'C' : [ 'C8' , 'C9' , 'C10' , 'C11' ], 'D' : [ 'D8' , 'D9' , 'D10' , 'D11' ]}, index = [ 8 , 9 , 10 , 11 ]) frames = [ df1 , df2 , df3 ] result = pd . concat ( frames )","title":"Concatenar varias tablas con las mismas columnas"},{"location":"lectures/data_manipulation/data_manipulation/merge_concat/#concatenar-varias-tablas-con-distintas-columnas-por-filas","text":"df4 = pd . DataFrame ({ 'B1' : [ 'hola' , 'B3' , 'B6' , 'B7' ], 'D' : [ 'D2' , 'D3' , 'D6' , 'D7' ], 'F' : [ 'F2' , 'F3' , 'F6' , 'F7' ]}, index = [ 2 , 3 , 6 , 7 ]) result = pd . concat ([ df1 , df4 ], axis = 0 , sort = False )","title":"Concatenar varias tablas con distintas columnas (por filas)"},{"location":"lectures/data_manipulation/data_manipulation/merge_concat/#concatenar-varias-tablas-con-distintas-columnas-por-columnas","text":"result = pd . concat ([ df1 , df4 ], axis = 1 , sort = False )","title":"Concatenar varias tablas con distintas columnas (por columnas)"},{"location":"lectures/data_manipulation/data_manipulation/merge_concat/#merge","text":"La funci\u00f3n merge() se usa para combinar dos (o m\u00e1s) tablas sobre valores de columnas comunes (keys). left = pd . DataFrame ({ 'key' : [ 'K0' , 'K1' , 'K2' , 'K3' ], 'A' : [ 'A0' , 'A1' , 'A2' , 'A3' ], 'B' : [ 'B0' , 'B1' , 'B2' , 'B3' ]}) right = pd . DataFrame ({ 'key' : [ 'K0' , 'K1' , 'K2' , 'K3' ], 'C' : [ 'C0' , 'C1' , 'C2' , 'C3' ], 'D' : [ 'D0' , 'D1' , 'D2' , 'D3' ]}) result = pd . merge ( left , right , on = 'key' ) En este ejemplo, se especifica en la opci\u00f3n on las columnas (keys) donde se realizar\u00e1 el cruce de informaci\u00f3n de ambas tablas.","title":"Merge"},{"location":"lectures/data_manipulation/data_manipulation/merge_concat/#tipos-de-merge","text":"La opci\u00f3n how especificica el tipo de cruce que se realizar\u00e1. left : usa las llaves solo de la tabla izquierda right : usa las llaves solo de la tabla derecha outer : usa las llaves de la uni\u00f3n de ambas tablas. inner : usa las llaves de la intersecci\u00f3n de ambas tablas. left = pd . DataFrame ({ 'key1' : [ 'K0' , 'K0' , 'K1' , 'K2' ], 'key2' : [ 'K0' , 'K1' , 'K0' , 'K1' ], 'A' : [ 'A0' , 'A1' , 'A2' , 'A3' ], 'B' : [ 'B0' , 'B1' , 'B2' , 'B3' ]}) right = pd . DataFrame ({ 'key1' : [ 'K0' , 'K1' , 'K1' , 'K2' ], 'key2' : [ 'K0' , 'K0' , 'K0' , 'K0' ], 'C' : [ 'C0' , 'C1' , 'C2' , 'C3' ], 'D' : [ 'D0' , 'D1' , 'D2' , 'D3' ]})","title":"Tipos de merge"},{"location":"lectures/data_manipulation/data_manipulation/merge_concat/#merge-left","text":"merge_left = pd . merge ( left , right , how = 'left' , on = [ 'key1' , 'key2' ])","title":"Merge left"},{"location":"lectures/data_manipulation/data_manipulation/merge_concat/#merge-right","text":"merge_rigth = pd . merge ( left , right , how = 'right' , on = [ 'key1' , 'key2' ])","title":"Merge right"},{"location":"lectures/data_manipulation/data_manipulation/merge_concat/#merge-outer","text":"merge_outer = pd . merge ( left , right , how = 'outer' , on = [ 'key1' , 'key2' ])","title":"Merge outer"},{"location":"lectures/data_manipulation/data_manipulation/merge_concat/#merge-inner","text":"merge_inner = pd . merge ( left , right , how = 'inner' , on = [ 'key1' , 'key2' ])","title":"Merge inner"},{"location":"lectures/data_manipulation/data_manipulation/merge_concat/#problemas-de-llaves-duplicadas","text":"Cuando se quiere realizar el cruce de dos tablas, pero an ambas tablas existe una columna (key) con el mismo nombre, para diferenciar la informaci\u00f3n entre la columna de una tabla y otra, pandas devulve el nombre de la columna con un gui\u00f3n bajo x (key_x) y otra con un gui\u00f3n bajo y (key_y) left = pd . DataFrame ({ 'A' : [ 1 , 2 ], 'B' : [ 2 , 2 ]}) right = pd . DataFrame ({ 'A' : [ 4 , 5 , 6 ], 'B' : [ 2 , 2 , 2 ]}) result = pd . merge ( left , right , on = 'B' , how = 'outer' )","title":"Problemas de llaves duplicadas"},{"location":"lectures/data_manipulation/data_manipulation/merge_concat/#referencia","text":"Merge, join, and concatenate","title":"Referencia"},{"location":"lectures/data_manipulation/data_manipulation/pandas/","text":"Pandas Pandas es un paquete de Python que proporciona estructuras de datos r\u00e1pidas, flexibles y expresivas dise\u00f1adas para que trabajar con datos \"relacionales\" o \"etiquetados\" sea f\u00e1cil e intuitivo. Su objetivo es ser el bloque de construcci\u00f3n fundamental de alto nivel para hacer an\u00e1lisis de datos pr\u00e1cticos del mundo real en Python. Adem\u00e1s, tiene el objetivo m\u00e1s amplio de convertirse en la herramienta de an\u00e1lisis/manipulaci\u00f3n de datos de c\u00f3digo abierto m\u00e1s potente y flexible disponible en cualquier idioma. Ya est\u00e1 en camino hacia este objetivo. Series y DataFrames Las series son arreglos unidimensionales con etiquetas. Se puede pensar como una generalizaci\u00f3n de los diccionarios de Python. Los dataframe son arreglos bidimensionales y una extensi\u00f3n natural de las series. Se puede pensar como la generalizaci\u00f3n de un numpy.array. Pandas Series Trabajando con pandas.Series # importar libreria: pandas, os import pandas as pd import numpy as np import os # crear serie my_serie = pd . Series ( range ( 3 , 33 , 3 ), index = list ( 'abcdefghij' ) ) # imprimir serie print ( \"serie:\" ) print ( my_serie ) serie: a 3 b 6 c 9 d 12 e 15 f 18 g 21 h 24 i 27 j 30 dtype: int64 # tipo print ( \"type:\" ) print ( type ( my_serie ) ) type: <class 'pandas.core.series.Series'> # valores print ( \"values:\" ) print ( my_serie . values ) values: [ 3 6 9 12 15 18 21 24 27 30] # indice print ( \"index:\" ) print ( my_serie . index ) index: Index(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'], dtype='object') # acceder al valor de la serie: directo print ( \"direct:\" ) print ( my_serie [ 'b' ]) direct: 6 # acceder al valor de la serie: loc print ( \"loc:\" ) print ( my_serie . loc [ 'b' ]) loc: 6 # acceder al valor de la serie: iloc print ( \"iloc:\" ) print ( my_serie . iloc [ 1 ]) iloc: 6 # editar valores print ( \"edit:\" ) print ( \" \\n old 'd':\" , my_serie . loc [ 'd' ] ) my_serie . loc [ 'd' ] = 1000 print ( \"new 'd':\" , my_serie . loc [ 'd' ] ) edit: old 'd': 12 new 'd': 1000 # ordenar valores print ( \"order:\" ) my_serie . sort_values ( ascending = True ) order: a 3 b 6 c 9 e 15 f 18 g 21 h 24 i 27 j 30 d 1000 dtype: int64 Operaciones matem\u00e1ticas Al igual que numpy, las series de pandas pueden realizar operaciones matem\u00e1ticas similares (mientr\u00e1s los arreglos a operar sean del tipo num\u00e9rico). Por otro lado existen otras funciones de utilidad. B\u00e1sicas s1 = pd . Series ([ 1 , 2 , 3 , 4 , 5 ]) s2 = pd . Series ([ 1 , 1 , 2 , 2 , 2 ]) # suma print ( f \"suma: \\n { s1 + s2 } \\n \" ) # multiplicacion print ( f \"multiplicacion: \\n { s1 * s2 } \" ) suma: 0 2 1 3 2 5 3 6 4 7 dtype: int64 multiplicacion: 0 1 1 2 2 6 3 8 4 10 dtype: int64 Estad\u00edsticas # crear serie s1 = pd . Series ([ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 , 4 , 5 , 5 , 5 , 5 ]) print ( f \"max: { s1 . max () } \" ) # maximo print ( f \"min: { s1 . min () } \" ) # minimo print ( f \"mean: { s1 . mean () } \" ) # promedio print ( f \"median: { s1 . median () } \" ) # mediana max: 5 min: 1 mean: 3.0 median: 3.0 # funcion describe s1 . describe () count 14.000000 mean 3.000000 std 1.568929 min 1.000000 25% 2.000000 50% 3.000000 75% 4.750000 max 5.000000 dtype: float64 Conteos # funcion count_values s2 = pd . Series ([ \"a\" , \"a\" , \"b\" , \"c\" , \"c\" , \"c\" , \"c\" , \"d\" ,]) print ( f \"Conteo: \\n { s2 . value_counts () } \" ) Conteo: c 4 a 2 d 1 b 1 dtype: int64 Masking Existen m\u00f3dulos para acceder a valores que queremos que cumplan una determinada regla. Por ejemplo, acceder al valor m\u00e1ximo de una serie. En este caso a esta regla la denominaremos mask . # 1. definir valor maximo n_max = s1 . max () # 2.- definir \"mask\" que busca el valor objetivo mask = ( s1 == n_max ) # 3.- aplicar mask sobre la serie s1 [ mask ] 10 5 11 5 12 5 13 5 dtype: int64 Valores Nulos o datos perdidos En algunas ocaciones, los arreglos no tienen informaci\u00f3n en una determinada posici\u00f3n, lo cual puede ser perjudicial si no se tiene control sobre estos valores. Encontrar valores nulos # crear serie s_null = pd . Series ([ 1 , 2 , np . nan , 4 , 5 , 6 , 7 , np . nan , 9 ]) s_null 0 1.0 1 2.0 2 NaN 3 4.0 4 5.0 5 6.0 6 7.0 7 NaN 8 9.0 dtype: float64 # mask valores nulos print ( \"is null?: \\n \" ) print ( s_null . isnull () ) is null?: 0 False 1 False 2 True 3 False 4 False 5 False 6 False 7 True 8 False dtype: bool # filtrar valores nulos print ( \"null serie: \\n \" ) print ( s_null [ s_null . isnull ()] ) null serie: 2 NaN 7 NaN dtype: float64 Encontrar valores no nulos # imprimir serie print ( \"serie:\" ) print ( s_null ) serie: 0 1.0 1 2.0 2 NaN 3 4.0 4 5.0 5 6.0 6 7.0 7 NaN 8 9.0 dtype: float64 # mask valores no nulos print ( \" \\n is not null?:\" ) print ( s_null . notnull () ) is not null?: 0 True 1 True 2 False 3 True 4 True 5 True 6 True 7 False 8 True dtype: bool # filtrar valores no nulos print ( \" \\n serie with not null values\" ) print ( s_null [ s_null . notnull ()] ) serie with not null values 0 1.0 1 2.0 3 4.0 4 5.0 5 6.0 6 7.0 8 9.0 dtype: float64 La pregunta que nos queda hacer es: \u00bf Qu\u00e9 se debe hacer con los valores nulos ?, la respuesta es depende . Si tenemos muchos datos, lo m\u00e1s probable es que se puedan eliminar estos datos sin culpa. Si se tienen poco datos, lo m\u00e1s probable es que se necesite inputar un valor por defecto a los valores nulos ( ejemplo : el promedio). Manejo de Fechas Pandas tambi\u00e9n trae m\u00f3dulos para trabajar el formato de fechas. Crear Serie de fechas # crear serie de fechas date_rng = pd . date_range ( start = '1/1/2019' , end = '1/03/2019' , freq = '4H' ) # imprimir serie print ( \"serie:\" ) print ( date_rng ) serie: DatetimeIndex(['2019-01-01 00:00:00', '2019-01-01 04:00:00', '2019-01-01 08:00:00', '2019-01-01 12:00:00', '2019-01-01 16:00:00', '2019-01-01 20:00:00', '2019-01-02 00:00:00', '2019-01-02 04:00:00', '2019-01-02 08:00:00', '2019-01-02 12:00:00', '2019-01-02 16:00:00', '2019-01-02 20:00:00', '2019-01-03 00:00:00'], dtype='datetime64[ns]', freq='4H') # tipo print ( \"type: \\n \" ) print ( type ( date_rng ) ) type: <class 'pandas.core.indexes.datetimes.DatetimeIndex'> Atributos de las fechas # obtener fechas print ( \"date: \\n \" ) print ( date_rng . date ) # obtener tiempo print ( \" \\n hour: \\n \" ) print ( date_rng . time ) date: [datetime.date(2019, 1, 1) datetime.date(2019, 1, 1) datetime.date(2019, 1, 1) datetime.date(2019, 1, 1) datetime.date(2019, 1, 1) datetime.date(2019, 1, 1) datetime.date(2019, 1, 2) datetime.date(2019, 1, 2) datetime.date(2019, 1, 2) datetime.date(2019, 1, 2) datetime.date(2019, 1, 2) datetime.date(2019, 1, 2) datetime.date(2019, 1, 3)] hour: [datetime.time(0, 0) datetime.time(4, 0) datetime.time(8, 0) datetime.time(12, 0) datetime.time(16, 0) datetime.time(20, 0) datetime.time(0, 0) datetime.time(4, 0) datetime.time(8, 0) datetime.time(12, 0) datetime.time(16, 0) datetime.time(20, 0) datetime.time(0, 0)] Fechas y string # elementos de datetime a string string_date_rng = [ str ( x ) for x in date_rng ] print ( \"datetime to string: \\n \" ) print ( np . array ( string_date_rng ) ) datetime to string: ['2019-01-01 00:00:00' '2019-01-01 04:00:00' '2019-01-01 08:00:00' '2019-01-01 12:00:00' '2019-01-01 16:00:00' '2019-01-01 20:00:00' '2019-01-02 00:00:00' '2019-01-02 04:00:00' '2019-01-02 08:00:00' '2019-01-02 12:00:00' '2019-01-02 16:00:00' '2019-01-02 20:00:00' '2019-01-03 00:00:00'] # elementos de string a datetime timestamp_date_rng = pd . to_datetime ( string_date_rng , infer_datetime_format = True ) print ( \"string to datetime: \\n \" ) print ( timestamp_date_rng ) string to datetime: DatetimeIndex(['2019-01-01 00:00:00', '2019-01-01 04:00:00', '2019-01-01 08:00:00', '2019-01-01 12:00:00', '2019-01-01 16:00:00', '2019-01-01 20:00:00', '2019-01-02 00:00:00', '2019-01-02 04:00:00', '2019-01-02 08:00:00', '2019-01-02 12:00:00', '2019-01-02 16:00:00', '2019-01-02 20:00:00', '2019-01-03 00:00:00'], dtype='datetime64[ns]', freq=None) Pandas Dataframes Trabajando con pandas.DataFrames Como se mencina anteriormente, los dataframes son arreglos de series, los cuales pueden ser de distintos tipos (num\u00e9ricos, string, etc.). En esta parte mostraremos un ejemplo aplicado de las distintas funcionalidades de los dataframes. Creaci\u00f3n de dataframes La creaci\u00f3n se puede hacer de variadas formas con listas, dictionarios , numpy array , entre otros. # empty dataframe df_empty = pd . DataFrame () df_empty .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # dataframe with list df_list = pd . DataFrame ( [ [ \"nombre_01\" , \"apellido_01\" , 60 ], [ \"nombre_02\" , \"apellido_02\" , 14 ] ], columns = [ \"nombre\" , \"apellido\" , \"edad\" ] ) df_list .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } nombre apellido edad 0 nombre_01 apellido_01 60 1 nombre_02 apellido_02 14 # dataframe with dct df_dct = pd . DataFrame ( { \"nombre\" : [ \"nombre_01\" , \"nombre_02\" ,], \"apellido\" : [ \"apellido_01\" , \"apellido_02\" ], \"edad\" : np . array ([ 60 , 14 ]), } ) Lectura de datos con dataframes En general, cuando se trabajan con datos, estos se almacenan en alg\u00fan lugar y en alg\u00fan tipo de formato, por ejemplo: * .txt * .csv * .xlsx * .db * etc. Para cada formato, existe un m\u00f3dulo para realizar la lectura de datos. En este caso, se analiza el conjunto de datos 'player_data.csv', el cual muestra informacion b\u00e1sica de algunos jugadores de la NBA. # load data player_data = pd . read_csv ( os . path . join ( 'data' , 'player_data.csv' ), # path sep = \",\" # separation ) player_data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name year_start year_end position height weight birth_date college 0 Alaa Abdelnaby 1991 1995 F-C 6-10 240.0 June 24, 1968 Duke University 1 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 2 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 3 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 4 Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University ... ... ... ... ... ... ... ... ... 4545 Ante Zizic 2018 2018 F-C 6-11 250.0 January 4, 1997 NaN 4546 Jim Zoet 1983 1983 C 7-1 240.0 December 20, 1953 Kent State University 4547 Bill Zopf 1971 1971 G 6-1 170.0 June 7, 1948 Duquesne University 4548 Ivica Zubac 2017 2018 C 7-1 265.0 March 18, 1997 NaN 4549 Matt Zunic 1949 1949 G-F 6-3 195.0 December 19, 1919 George Washington University 4550 rows \u00d7 8 columns M\u00f3dulos b\u00e1sicos Existen m\u00f3dulos para comprender r\u00e1pidamente la naturaleza del dataframe. # primeras silas print ( \"first 5 rows:\" ) player_data . head ( 5 ) first 5 rows: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name year_start year_end position height weight birth_date college 0 Alaa Abdelnaby 1991 1995 F-C 6-10 240.0 June 24, 1968 Duke University 1 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 2 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 3 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 4 Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University # ultimas filas print ( \" \\n last 5 rows:\" ) player_data . tail ( 5 ) last 5 rows: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name year_start year_end position height weight birth_date college 4545 Ante Zizic 2018 2018 F-C 6-11 250.0 January 4, 1997 NaN 4546 Jim Zoet 1983 1983 C 7-1 240.0 December 20, 1953 Kent State University 4547 Bill Zopf 1971 1971 G 6-1 170.0 June 7, 1948 Duquesne University 4548 Ivica Zubac 2017 2018 C 7-1 265.0 March 18, 1997 NaN 4549 Matt Zunic 1949 1949 G-F 6-3 195.0 December 19, 1919 George Washington University # tipo print ( \" \\n type of dataframe:\" ) type ( player_data ) type of dataframe: pandas.core.frame.DataFrame # tipo por columnas print ( \" \\n type of columns:\" ) player_data . dtypes type of columns: name object year_start int64 year_end int64 position object height object weight float64 birth_date object college object dtype: object # dimension print ( \" \\n shape:\" ) player_data . shape shape: (4550, 8) # nombre de las columnas print ( \" \\n cols:\" ) player_data . columns cols: Index(['name', 'year_start', 'year_end', 'position', 'height', 'weight', 'birth_date', 'college'], dtype='object') # indice print ( \" \\n index:\" ) player_data . index index: RangeIndex(start=0, stop=4550, step=1) # acceder a la columna posicion print ( \" \\n column 'position': \" ) player_data [ 'position' ] . head () column 'position': 0 F-C 1 C-F 2 C 3 G 4 F Name: position, dtype: object # cambiar nombre de una o varias columnas player_data = player_data . rename ( columns = { \"birth_date\" : \"Birth\" , \"college\" : \"College\" }) player_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name year_start year_end position height weight Birth College 0 Alaa Abdelnaby 1991 1995 F-C 6-10 240.0 June 24, 1968 Duke University 1 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 2 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 3 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 4 Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University # fijar columna especifica como indice player_data = player_data . set_index ([ \"name\" ]) player_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year_start year_end position height weight Birth College name Alaa Abdelnaby 1991 1995 F-C 6-10 240.0 June 24, 1968 Duke University Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University # ordenar dataframe por columna especifica player_data = player_data . sort_values ( \"weight\" ) player_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year_start year_end position height weight Birth College name Penny Early 1969 1969 G 5-3 114.0 May 30, 1943 NaN Spud Webb 1986 1998 G 5-6 133.0 July 13, 1963 North Carolina State University Earl Boykins 1999 2012 G 5-5 135.0 June 2, 1976 Eastern Michigan University Muggsy Bogues 1988 2001 G 5-3 136.0 January 9, 1965 Wake Forest University Chet Aubuchon 1947 1947 G 5-10 137.0 May 18, 1916 Michigan State University # resumen de la informaci\u00f3n player_data . describe ( include = 'all' ) # player_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year_start year_end position height weight Birth College count 4550.000000 4550.000000 4549 4549 4544.000000 4519 4248 unique NaN NaN 7 28 NaN 4161 473 top NaN NaN G 6-7 NaN June 26, 1984 University of Kentucky freq NaN NaN 1574 473 NaN 3 99 mean 1985.076264 1989.272527 NaN NaN 208.908011 NaN NaN std 20.974188 21.874761 NaN NaN 26.268662 NaN NaN min 1947.000000 1947.000000 NaN NaN 114.000000 NaN NaN 25% 1969.000000 1973.000000 NaN NaN 190.000000 NaN NaN 50% 1986.000000 1992.000000 NaN NaN 210.000000 NaN NaN 75% 2003.000000 2009.000000 NaN NaN 225.000000 NaN NaN max 2018.000000 2018.000000 NaN NaN 360.000000 NaN NaN Operando sobre Dataframes Cuando se trabaja con un conjunto de datos, se crea una din\u00e1mica de preguntas y respuestas, en donde a medida que necesito informaci\u00f3n, se va accediendo al dataframe. En algunas ocaciones es directo, basta un simple m\u00f3dulo, aunque en otras ser\u00e1 necesaria realizar operaciones un poco m\u00e1s complejas. Por ejemplo, del conjunto de datos en estudio, se esta interesado en responder las siguientes preguntas: a) Determine si el dataframe tiene valores nulos # ocupar comando .notnull().all(axis=1) player_data . notnull () . all ( axis = 1 ) . head ( 10 ) name Penny Early False Spud Webb True Earl Boykins True Muggsy Bogues True Chet Aubuchon True Greg Grant True Angelo Musi True Ernie Calverley True Tyler Ulis True Lionel Malamed True dtype: bool b) Elimine los valores nulos del dataframe # ocupar masking mask = lambda df : df . notnull () . all ( axis = 1 ) player_data = player_data [ mask ] player_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year_start year_end position height weight Birth College name Spud Webb 1986 1998 G 5-6 133.0 July 13, 1963 North Carolina State University Earl Boykins 1999 2012 G 5-5 135.0 June 2, 1976 Eastern Michigan University Muggsy Bogues 1988 2001 G 5-3 136.0 January 9, 1965 Wake Forest University Chet Aubuchon 1947 1947 G 5-10 137.0 May 18, 1916 Michigan State University Greg Grant 1990 1996 G 5-7 140.0 August 29, 1966 Trenton State University c) Determinar el tiempo (en a\u00f1os) de cada jugador en su posici\u00f3n player_data [ 'duration' ] = player_data [ 'year_end' ] - player_data [ 'year_start' ] player_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year_start year_end position height weight Birth College duration name Spud Webb 1986 1998 G 5-6 133.0 July 13, 1963 North Carolina State University 12 Earl Boykins 1999 2012 G 5-5 135.0 June 2, 1976 Eastern Michigan University 13 Muggsy Bogues 1988 2001 G 5-3 136.0 January 9, 1965 Wake Forest University 13 Chet Aubuchon 1947 1947 G 5-10 137.0 May 18, 1916 Michigan State University 0 Greg Grant 1990 1996 G 5-7 140.0 August 29, 1966 Trenton State University 6 d) Castear la fecha de str a objeto datetime player_data [ 'birth_date_dt' ] = pd . to_datetime ( player_data [ 'Birth' ], format = \"%B %d , %Y\" ) player_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year_start year_end position height weight Birth College duration birth_date_dt name Spud Webb 1986 1998 G 5-6 133.0 July 13, 1963 North Carolina State University 12 1963-07-13 Earl Boykins 1999 2012 G 5-5 135.0 June 2, 1976 Eastern Michigan University 13 1976-06-02 Muggsy Bogues 1988 2001 G 5-3 136.0 January 9, 1965 Wake Forest University 13 1965-01-09 Chet Aubuchon 1947 1947 G 5-10 137.0 May 18, 1916 Michigan State University 0 1916-05-18 Greg Grant 1990 1996 G 5-7 140.0 August 29, 1966 Trenton State University 6 1966-08-29 e) Determinar todas las posiciones positions = player_data [ 'position' ] . unique () positions array(['G', 'G-F', 'F-G', 'F', 'F-C', 'C-F', 'C'], dtype=object) f) Iterar sobre cada posici\u00f3n y encontrar el mayor valor asociado a la columna weight # Iterar sobre cada posici\u00f3n y encontrar el mayor valor. nba_position_duration = dict () # iterar for position in positions : # filtrar df_aux = player_data . loc [ lambda x : x [ 'position' ] == position ] # encontrar maximo de la columna objetivo max_duration = df_aux [ 'weight' ] . max () # guardar en pd.Series nba_position_duration [ position ] = max_duration # retornar serie nba_position_duration {'G': 235.0, 'G-F': 240.0, 'F-G': 245.0, 'F': 284.0, 'F-C': 290.0, 'C-F': 280.0, 'C': 360.0} Referencia Python Pandas Tutorial: A Complete Introduction for Beginners General functions","title":"Pandas"},{"location":"lectures/data_manipulation/data_manipulation/pandas/#pandas","text":"Pandas es un paquete de Python que proporciona estructuras de datos r\u00e1pidas, flexibles y expresivas dise\u00f1adas para que trabajar con datos \"relacionales\" o \"etiquetados\" sea f\u00e1cil e intuitivo. Su objetivo es ser el bloque de construcci\u00f3n fundamental de alto nivel para hacer an\u00e1lisis de datos pr\u00e1cticos del mundo real en Python. Adem\u00e1s, tiene el objetivo m\u00e1s amplio de convertirse en la herramienta de an\u00e1lisis/manipulaci\u00f3n de datos de c\u00f3digo abierto m\u00e1s potente y flexible disponible en cualquier idioma. Ya est\u00e1 en camino hacia este objetivo. Series y DataFrames Las series son arreglos unidimensionales con etiquetas. Se puede pensar como una generalizaci\u00f3n de los diccionarios de Python. Los dataframe son arreglos bidimensionales y una extensi\u00f3n natural de las series. Se puede pensar como la generalizaci\u00f3n de un numpy.array.","title":"Pandas"},{"location":"lectures/data_manipulation/data_manipulation/pandas/#pandas-series","text":"","title":"Pandas Series"},{"location":"lectures/data_manipulation/data_manipulation/pandas/#trabajando-con-pandasseries","text":"# importar libreria: pandas, os import pandas as pd import numpy as np import os # crear serie my_serie = pd . Series ( range ( 3 , 33 , 3 ), index = list ( 'abcdefghij' ) ) # imprimir serie print ( \"serie:\" ) print ( my_serie ) serie: a 3 b 6 c 9 d 12 e 15 f 18 g 21 h 24 i 27 j 30 dtype: int64 # tipo print ( \"type:\" ) print ( type ( my_serie ) ) type: <class 'pandas.core.series.Series'> # valores print ( \"values:\" ) print ( my_serie . values ) values: [ 3 6 9 12 15 18 21 24 27 30] # indice print ( \"index:\" ) print ( my_serie . index ) index: Index(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'], dtype='object') # acceder al valor de la serie: directo print ( \"direct:\" ) print ( my_serie [ 'b' ]) direct: 6 # acceder al valor de la serie: loc print ( \"loc:\" ) print ( my_serie . loc [ 'b' ]) loc: 6 # acceder al valor de la serie: iloc print ( \"iloc:\" ) print ( my_serie . iloc [ 1 ]) iloc: 6 # editar valores print ( \"edit:\" ) print ( \" \\n old 'd':\" , my_serie . loc [ 'd' ] ) my_serie . loc [ 'd' ] = 1000 print ( \"new 'd':\" , my_serie . loc [ 'd' ] ) edit: old 'd': 12 new 'd': 1000 # ordenar valores print ( \"order:\" ) my_serie . sort_values ( ascending = True ) order: a 3 b 6 c 9 e 15 f 18 g 21 h 24 i 27 j 30 d 1000 dtype: int64","title":"Trabajando con  pandas.Series"},{"location":"lectures/data_manipulation/data_manipulation/pandas/#operaciones-matematicas","text":"Al igual que numpy, las series de pandas pueden realizar operaciones matem\u00e1ticas similares (mientr\u00e1s los arreglos a operar sean del tipo num\u00e9rico). Por otro lado existen otras funciones de utilidad. B\u00e1sicas s1 = pd . Series ([ 1 , 2 , 3 , 4 , 5 ]) s2 = pd . Series ([ 1 , 1 , 2 , 2 , 2 ]) # suma print ( f \"suma: \\n { s1 + s2 } \\n \" ) # multiplicacion print ( f \"multiplicacion: \\n { s1 * s2 } \" ) suma: 0 2 1 3 2 5 3 6 4 7 dtype: int64 multiplicacion: 0 1 1 2 2 6 3 8 4 10 dtype: int64 Estad\u00edsticas # crear serie s1 = pd . Series ([ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 , 4 , 5 , 5 , 5 , 5 ]) print ( f \"max: { s1 . max () } \" ) # maximo print ( f \"min: { s1 . min () } \" ) # minimo print ( f \"mean: { s1 . mean () } \" ) # promedio print ( f \"median: { s1 . median () } \" ) # mediana max: 5 min: 1 mean: 3.0 median: 3.0 # funcion describe s1 . describe () count 14.000000 mean 3.000000 std 1.568929 min 1.000000 25% 2.000000 50% 3.000000 75% 4.750000 max 5.000000 dtype: float64 Conteos # funcion count_values s2 = pd . Series ([ \"a\" , \"a\" , \"b\" , \"c\" , \"c\" , \"c\" , \"c\" , \"d\" ,]) print ( f \"Conteo: \\n { s2 . value_counts () } \" ) Conteo: c 4 a 2 d 1 b 1 dtype: int64","title":"Operaciones matem\u00e1ticas"},{"location":"lectures/data_manipulation/data_manipulation/pandas/#masking","text":"Existen m\u00f3dulos para acceder a valores que queremos que cumplan una determinada regla. Por ejemplo, acceder al valor m\u00e1ximo de una serie. En este caso a esta regla la denominaremos mask . # 1. definir valor maximo n_max = s1 . max () # 2.- definir \"mask\" que busca el valor objetivo mask = ( s1 == n_max ) # 3.- aplicar mask sobre la serie s1 [ mask ] 10 5 11 5 12 5 13 5 dtype: int64","title":"Masking"},{"location":"lectures/data_manipulation/data_manipulation/pandas/#valores-nulos-o-datos-perdidos","text":"En algunas ocaciones, los arreglos no tienen informaci\u00f3n en una determinada posici\u00f3n, lo cual puede ser perjudicial si no se tiene control sobre estos valores. Encontrar valores nulos # crear serie s_null = pd . Series ([ 1 , 2 , np . nan , 4 , 5 , 6 , 7 , np . nan , 9 ]) s_null 0 1.0 1 2.0 2 NaN 3 4.0 4 5.0 5 6.0 6 7.0 7 NaN 8 9.0 dtype: float64 # mask valores nulos print ( \"is null?: \\n \" ) print ( s_null . isnull () ) is null?: 0 False 1 False 2 True 3 False 4 False 5 False 6 False 7 True 8 False dtype: bool # filtrar valores nulos print ( \"null serie: \\n \" ) print ( s_null [ s_null . isnull ()] ) null serie: 2 NaN 7 NaN dtype: float64 Encontrar valores no nulos # imprimir serie print ( \"serie:\" ) print ( s_null ) serie: 0 1.0 1 2.0 2 NaN 3 4.0 4 5.0 5 6.0 6 7.0 7 NaN 8 9.0 dtype: float64 # mask valores no nulos print ( \" \\n is not null?:\" ) print ( s_null . notnull () ) is not null?: 0 True 1 True 2 False 3 True 4 True 5 True 6 True 7 False 8 True dtype: bool # filtrar valores no nulos print ( \" \\n serie with not null values\" ) print ( s_null [ s_null . notnull ()] ) serie with not null values 0 1.0 1 2.0 3 4.0 4 5.0 5 6.0 6 7.0 8 9.0 dtype: float64 La pregunta que nos queda hacer es: \u00bf Qu\u00e9 se debe hacer con los valores nulos ?, la respuesta es depende . Si tenemos muchos datos, lo m\u00e1s probable es que se puedan eliminar estos datos sin culpa. Si se tienen poco datos, lo m\u00e1s probable es que se necesite inputar un valor por defecto a los valores nulos ( ejemplo : el promedio).","title":"Valores Nulos o datos perdidos"},{"location":"lectures/data_manipulation/data_manipulation/pandas/#manejo-de-fechas","text":"Pandas tambi\u00e9n trae m\u00f3dulos para trabajar el formato de fechas. Crear Serie de fechas # crear serie de fechas date_rng = pd . date_range ( start = '1/1/2019' , end = '1/03/2019' , freq = '4H' ) # imprimir serie print ( \"serie:\" ) print ( date_rng ) serie: DatetimeIndex(['2019-01-01 00:00:00', '2019-01-01 04:00:00', '2019-01-01 08:00:00', '2019-01-01 12:00:00', '2019-01-01 16:00:00', '2019-01-01 20:00:00', '2019-01-02 00:00:00', '2019-01-02 04:00:00', '2019-01-02 08:00:00', '2019-01-02 12:00:00', '2019-01-02 16:00:00', '2019-01-02 20:00:00', '2019-01-03 00:00:00'], dtype='datetime64[ns]', freq='4H') # tipo print ( \"type: \\n \" ) print ( type ( date_rng ) ) type: <class 'pandas.core.indexes.datetimes.DatetimeIndex'> Atributos de las fechas # obtener fechas print ( \"date: \\n \" ) print ( date_rng . date ) # obtener tiempo print ( \" \\n hour: \\n \" ) print ( date_rng . time ) date: [datetime.date(2019, 1, 1) datetime.date(2019, 1, 1) datetime.date(2019, 1, 1) datetime.date(2019, 1, 1) datetime.date(2019, 1, 1) datetime.date(2019, 1, 1) datetime.date(2019, 1, 2) datetime.date(2019, 1, 2) datetime.date(2019, 1, 2) datetime.date(2019, 1, 2) datetime.date(2019, 1, 2) datetime.date(2019, 1, 2) datetime.date(2019, 1, 3)] hour: [datetime.time(0, 0) datetime.time(4, 0) datetime.time(8, 0) datetime.time(12, 0) datetime.time(16, 0) datetime.time(20, 0) datetime.time(0, 0) datetime.time(4, 0) datetime.time(8, 0) datetime.time(12, 0) datetime.time(16, 0) datetime.time(20, 0) datetime.time(0, 0)] Fechas y string # elementos de datetime a string string_date_rng = [ str ( x ) for x in date_rng ] print ( \"datetime to string: \\n \" ) print ( np . array ( string_date_rng ) ) datetime to string: ['2019-01-01 00:00:00' '2019-01-01 04:00:00' '2019-01-01 08:00:00' '2019-01-01 12:00:00' '2019-01-01 16:00:00' '2019-01-01 20:00:00' '2019-01-02 00:00:00' '2019-01-02 04:00:00' '2019-01-02 08:00:00' '2019-01-02 12:00:00' '2019-01-02 16:00:00' '2019-01-02 20:00:00' '2019-01-03 00:00:00'] # elementos de string a datetime timestamp_date_rng = pd . to_datetime ( string_date_rng , infer_datetime_format = True ) print ( \"string to datetime: \\n \" ) print ( timestamp_date_rng ) string to datetime: DatetimeIndex(['2019-01-01 00:00:00', '2019-01-01 04:00:00', '2019-01-01 08:00:00', '2019-01-01 12:00:00', '2019-01-01 16:00:00', '2019-01-01 20:00:00', '2019-01-02 00:00:00', '2019-01-02 04:00:00', '2019-01-02 08:00:00', '2019-01-02 12:00:00', '2019-01-02 16:00:00', '2019-01-02 20:00:00', '2019-01-03 00:00:00'], dtype='datetime64[ns]', freq=None)","title":"Manejo de Fechas"},{"location":"lectures/data_manipulation/data_manipulation/pandas/#pandas-dataframes","text":"","title":"Pandas Dataframes"},{"location":"lectures/data_manipulation/data_manipulation/pandas/#trabajando-con-pandasdataframes","text":"Como se mencina anteriormente, los dataframes son arreglos de series, los cuales pueden ser de distintos tipos (num\u00e9ricos, string, etc.). En esta parte mostraremos un ejemplo aplicado de las distintas funcionalidades de los dataframes.","title":"Trabajando  con pandas.DataFrames"},{"location":"lectures/data_manipulation/data_manipulation/pandas/#creacion-de-dataframes","text":"La creaci\u00f3n se puede hacer de variadas formas con listas, dictionarios , numpy array , entre otros. # empty dataframe df_empty = pd . DataFrame () df_empty .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # dataframe with list df_list = pd . DataFrame ( [ [ \"nombre_01\" , \"apellido_01\" , 60 ], [ \"nombre_02\" , \"apellido_02\" , 14 ] ], columns = [ \"nombre\" , \"apellido\" , \"edad\" ] ) df_list .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } nombre apellido edad 0 nombre_01 apellido_01 60 1 nombre_02 apellido_02 14 # dataframe with dct df_dct = pd . DataFrame ( { \"nombre\" : [ \"nombre_01\" , \"nombre_02\" ,], \"apellido\" : [ \"apellido_01\" , \"apellido_02\" ], \"edad\" : np . array ([ 60 , 14 ]), } )","title":"Creaci\u00f3n de dataframes"},{"location":"lectures/data_manipulation/data_manipulation/pandas/#lectura-de-datos-con-dataframes","text":"En general, cuando se trabajan con datos, estos se almacenan en alg\u00fan lugar y en alg\u00fan tipo de formato, por ejemplo: * .txt * .csv * .xlsx * .db * etc. Para cada formato, existe un m\u00f3dulo para realizar la lectura de datos. En este caso, se analiza el conjunto de datos 'player_data.csv', el cual muestra informacion b\u00e1sica de algunos jugadores de la NBA. # load data player_data = pd . read_csv ( os . path . join ( 'data' , 'player_data.csv' ), # path sep = \",\" # separation ) player_data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name year_start year_end position height weight birth_date college 0 Alaa Abdelnaby 1991 1995 F-C 6-10 240.0 June 24, 1968 Duke University 1 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 2 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 3 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 4 Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University ... ... ... ... ... ... ... ... ... 4545 Ante Zizic 2018 2018 F-C 6-11 250.0 January 4, 1997 NaN 4546 Jim Zoet 1983 1983 C 7-1 240.0 December 20, 1953 Kent State University 4547 Bill Zopf 1971 1971 G 6-1 170.0 June 7, 1948 Duquesne University 4548 Ivica Zubac 2017 2018 C 7-1 265.0 March 18, 1997 NaN 4549 Matt Zunic 1949 1949 G-F 6-3 195.0 December 19, 1919 George Washington University 4550 rows \u00d7 8 columns","title":"Lectura de datos con dataframes"},{"location":"lectures/data_manipulation/data_manipulation/pandas/#modulos-basicos","text":"Existen m\u00f3dulos para comprender r\u00e1pidamente la naturaleza del dataframe. # primeras silas print ( \"first 5 rows:\" ) player_data . head ( 5 ) first 5 rows: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name year_start year_end position height weight birth_date college 0 Alaa Abdelnaby 1991 1995 F-C 6-10 240.0 June 24, 1968 Duke University 1 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 2 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 3 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 4 Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University # ultimas filas print ( \" \\n last 5 rows:\" ) player_data . tail ( 5 ) last 5 rows: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name year_start year_end position height weight birth_date college 4545 Ante Zizic 2018 2018 F-C 6-11 250.0 January 4, 1997 NaN 4546 Jim Zoet 1983 1983 C 7-1 240.0 December 20, 1953 Kent State University 4547 Bill Zopf 1971 1971 G 6-1 170.0 June 7, 1948 Duquesne University 4548 Ivica Zubac 2017 2018 C 7-1 265.0 March 18, 1997 NaN 4549 Matt Zunic 1949 1949 G-F 6-3 195.0 December 19, 1919 George Washington University # tipo print ( \" \\n type of dataframe:\" ) type ( player_data ) type of dataframe: pandas.core.frame.DataFrame # tipo por columnas print ( \" \\n type of columns:\" ) player_data . dtypes type of columns: name object year_start int64 year_end int64 position object height object weight float64 birth_date object college object dtype: object # dimension print ( \" \\n shape:\" ) player_data . shape shape: (4550, 8) # nombre de las columnas print ( \" \\n cols:\" ) player_data . columns cols: Index(['name', 'year_start', 'year_end', 'position', 'height', 'weight', 'birth_date', 'college'], dtype='object') # indice print ( \" \\n index:\" ) player_data . index index: RangeIndex(start=0, stop=4550, step=1) # acceder a la columna posicion print ( \" \\n column 'position': \" ) player_data [ 'position' ] . head () column 'position': 0 F-C 1 C-F 2 C 3 G 4 F Name: position, dtype: object # cambiar nombre de una o varias columnas player_data = player_data . rename ( columns = { \"birth_date\" : \"Birth\" , \"college\" : \"College\" }) player_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name year_start year_end position height weight Birth College 0 Alaa Abdelnaby 1991 1995 F-C 6-10 240.0 June 24, 1968 Duke University 1 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 2 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 3 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 4 Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University # fijar columna especifica como indice player_data = player_data . set_index ([ \"name\" ]) player_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year_start year_end position height weight Birth College name Alaa Abdelnaby 1991 1995 F-C 6-10 240.0 June 24, 1968 Duke University Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University # ordenar dataframe por columna especifica player_data = player_data . sort_values ( \"weight\" ) player_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year_start year_end position height weight Birth College name Penny Early 1969 1969 G 5-3 114.0 May 30, 1943 NaN Spud Webb 1986 1998 G 5-6 133.0 July 13, 1963 North Carolina State University Earl Boykins 1999 2012 G 5-5 135.0 June 2, 1976 Eastern Michigan University Muggsy Bogues 1988 2001 G 5-3 136.0 January 9, 1965 Wake Forest University Chet Aubuchon 1947 1947 G 5-10 137.0 May 18, 1916 Michigan State University # resumen de la informaci\u00f3n player_data . describe ( include = 'all' ) # player_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year_start year_end position height weight Birth College count 4550.000000 4550.000000 4549 4549 4544.000000 4519 4248 unique NaN NaN 7 28 NaN 4161 473 top NaN NaN G 6-7 NaN June 26, 1984 University of Kentucky freq NaN NaN 1574 473 NaN 3 99 mean 1985.076264 1989.272527 NaN NaN 208.908011 NaN NaN std 20.974188 21.874761 NaN NaN 26.268662 NaN NaN min 1947.000000 1947.000000 NaN NaN 114.000000 NaN NaN 25% 1969.000000 1973.000000 NaN NaN 190.000000 NaN NaN 50% 1986.000000 1992.000000 NaN NaN 210.000000 NaN NaN 75% 2003.000000 2009.000000 NaN NaN 225.000000 NaN NaN max 2018.000000 2018.000000 NaN NaN 360.000000 NaN NaN","title":"M\u00f3dulos b\u00e1sicos"},{"location":"lectures/data_manipulation/data_manipulation/pandas/#operando-sobre-dataframes","text":"Cuando se trabaja con un conjunto de datos, se crea una din\u00e1mica de preguntas y respuestas, en donde a medida que necesito informaci\u00f3n, se va accediendo al dataframe. En algunas ocaciones es directo, basta un simple m\u00f3dulo, aunque en otras ser\u00e1 necesaria realizar operaciones un poco m\u00e1s complejas. Por ejemplo, del conjunto de datos en estudio, se esta interesado en responder las siguientes preguntas: a) Determine si el dataframe tiene valores nulos # ocupar comando .notnull().all(axis=1) player_data . notnull () . all ( axis = 1 ) . head ( 10 ) name Penny Early False Spud Webb True Earl Boykins True Muggsy Bogues True Chet Aubuchon True Greg Grant True Angelo Musi True Ernie Calverley True Tyler Ulis True Lionel Malamed True dtype: bool b) Elimine los valores nulos del dataframe # ocupar masking mask = lambda df : df . notnull () . all ( axis = 1 ) player_data = player_data [ mask ] player_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year_start year_end position height weight Birth College name Spud Webb 1986 1998 G 5-6 133.0 July 13, 1963 North Carolina State University Earl Boykins 1999 2012 G 5-5 135.0 June 2, 1976 Eastern Michigan University Muggsy Bogues 1988 2001 G 5-3 136.0 January 9, 1965 Wake Forest University Chet Aubuchon 1947 1947 G 5-10 137.0 May 18, 1916 Michigan State University Greg Grant 1990 1996 G 5-7 140.0 August 29, 1966 Trenton State University c) Determinar el tiempo (en a\u00f1os) de cada jugador en su posici\u00f3n player_data [ 'duration' ] = player_data [ 'year_end' ] - player_data [ 'year_start' ] player_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year_start year_end position height weight Birth College duration name Spud Webb 1986 1998 G 5-6 133.0 July 13, 1963 North Carolina State University 12 Earl Boykins 1999 2012 G 5-5 135.0 June 2, 1976 Eastern Michigan University 13 Muggsy Bogues 1988 2001 G 5-3 136.0 January 9, 1965 Wake Forest University 13 Chet Aubuchon 1947 1947 G 5-10 137.0 May 18, 1916 Michigan State University 0 Greg Grant 1990 1996 G 5-7 140.0 August 29, 1966 Trenton State University 6 d) Castear la fecha de str a objeto datetime player_data [ 'birth_date_dt' ] = pd . to_datetime ( player_data [ 'Birth' ], format = \"%B %d , %Y\" ) player_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year_start year_end position height weight Birth College duration birth_date_dt name Spud Webb 1986 1998 G 5-6 133.0 July 13, 1963 North Carolina State University 12 1963-07-13 Earl Boykins 1999 2012 G 5-5 135.0 June 2, 1976 Eastern Michigan University 13 1976-06-02 Muggsy Bogues 1988 2001 G 5-3 136.0 January 9, 1965 Wake Forest University 13 1965-01-09 Chet Aubuchon 1947 1947 G 5-10 137.0 May 18, 1916 Michigan State University 0 1916-05-18 Greg Grant 1990 1996 G 5-7 140.0 August 29, 1966 Trenton State University 6 1966-08-29 e) Determinar todas las posiciones positions = player_data [ 'position' ] . unique () positions array(['G', 'G-F', 'F-G', 'F', 'F-C', 'C-F', 'C'], dtype=object) f) Iterar sobre cada posici\u00f3n y encontrar el mayor valor asociado a la columna weight # Iterar sobre cada posici\u00f3n y encontrar el mayor valor. nba_position_duration = dict () # iterar for position in positions : # filtrar df_aux = player_data . loc [ lambda x : x [ 'position' ] == position ] # encontrar maximo de la columna objetivo max_duration = df_aux [ 'weight' ] . max () # guardar en pd.Series nba_position_duration [ position ] = max_duration # retornar serie nba_position_duration {'G': 235.0, 'G-F': 240.0, 'F-G': 245.0, 'F': 284.0, 'F-C': 290.0, 'C-F': 280.0, 'C': 360.0}","title":"Operando sobre Dataframes"},{"location":"lectures/data_manipulation/data_manipulation/pandas/#referencia","text":"Python Pandas Tutorial: A Complete Introduction for Beginners General functions","title":"Referencia"},{"location":"lectures/data_manipulation/data_manipulation/pivot/","text":"Pivot Formato Wide y Formato Long Dentro del mundo de los dataframe (o datos tabulares) existen dos formas de presentar la naturaleza de los datos: formato wide y formato long. Por ejemplo, el conjunto de datos Zoo Data Set presenta las caracter\u00edsticas de diversos animales, de los cuales presentamos las primeras 5 columnas. animal_name hair feathers eggs milk antelope 1 0 0 1 bear 1 0 0 1 buffalo 1 0 0 1 catfish 0 0 1 0 La tabla as\u00ed presentada se encuentra en wide format , es decir, donde los valores se extienden a trav\u00e9s de las columnas. Ser\u00eda posible representar el mismo contenido anterior en long format , es decir, donde los mismos valores se indicaran a trav\u00e9s de las filas: animal_name characteristic value antelope hair 1 antelope feathers 0 antelope eggs 0 antelope milk 1 ... ... ... catfish hair 0 catfish feathers 0 catfish eggs 1 catfish milk 0 En python existen maneras de pasar del formato wide al formato long y viceversa. Pivotear y despivotear tablas Pivot El pivoteo de una tabla corresponde al paso de una tabla desde el formato long al formato wide . T\u00edpicamente esto se realiza para poder comparar los valores que se obtienen para alg\u00fan registro en particular, o para utilizar algunas herramientas de visualizaci\u00f3n b\u00e1sica que requieren dicho formato. Para ejemplificar estos resultados, ocupemos el conjunto de datos terremotos.csv , con contiene los registros de terremotos de distintos paises desde el a\u00f1o 2000 al 2011. import pandas as pd import numpy as np import os # formato long df = pd . read_csv ( os . path . join ( \"data\" , \"terremotos.csv\" ), sep = \",\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A\u00f1o Pais Magnitud 0 2011 Turkey 7.1 1 2011 India 6.9 2 2011 Japan 7.1 3 2011 Burma 6.8 4 2011 Japan 9.0 Por ejemplo, se quiere saber el terremoto de mayor magnitud a nivel de pa\u00eds a\u00f1o. Tenemos dos formas de mostrar la informaci\u00f3n. # formato long df . groupby ([ 'Pais' , 'A\u00f1o' ]) . max () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Magnitud Pais A\u00f1o Afghanistan 2000 6.3 2001 5.0 2002 5.8 2003 5.8 2004 6.5 ... ... ... Turkmenistan 2000 7.0 United States 2001 6.8 2003 6.6 Venezuela 2006 5.5 Vietnam 2005 5.3 154 rows \u00d7 1 columns # formato wide df . pivot_table ( index = \"Pais\" , columns = \"A\u00f1o\" , values = \"Magnitud\" , fill_value = '' , aggfunc = pd . np . max ) /home/falfaro/.cache/pypoetry/virtualenvs/pymessi-xyyw3p3f-py3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A\u00f1o 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 Pais Afghanistan 6.3 5 5.8 5.8 6.5 Afghanistan 7.3 6.5 Algeria 5.7 5.7 5.2 5.5 Algeria 6.8 Argentina 7.2 6.1 ... ... ... ... ... ... ... ... ... ... ... ... ... Turkey 6.3 6.1 7.1 Turkmenistan 7 United States 6.8 6.6 Venezuela 5.5 Vietnam 5.3 72 rows \u00d7 12 columns Despivotear un tabla El despivotear una tabla corresponde al paso de una tabla desde el formato wide al formato long . Se reconocen dos situaciones: El valor indicado para la columna es \u00fanico, y s\u00f3lo se requiere definir correctamente las columnas. El valor indicado por la columna no es \u00fanico o requiere un procesamiento adicional, y se requiere una iteraci\u00f3n m\u00e1s profunda. Para ejemplificar esto, crearemos un conjunto de datos con los horarios de los ramos que se tiene que dictar en un determinado d\u00eda, hora y lugar. a) El valor indicado para la columna es \u00fanico columns = [ \"sala\" , \"dia\" , \"08:00\" , \"09:00\" , \"10:00\" ] data = [ [ \"C201\" , \"Lu\" , \"mat1\" , \"mat1\" , \"\" ], [ \"C201\" , \"Ma\" , \"\" , \"\" , \"\" ], [ \"C202\" , \"Lu\" , \"\" , \"\" , \"\" ], [ \"C202\" , \"Ma\" , \"mat1\" , \"mat1\" , \"\" ], [ \"C203\" , \"Lu\" , \"fis1\" , \"fis1\" , \"fis1\" ], [ \"C203\" , \"Ma\" , \"fis1\" , \"fis1\" , \"fis1\" ], ] df = pd . DataFrame ( data = data , columns = columns ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sala dia 08:00 09:00 10:00 0 C201 Lu mat1 mat1 1 C201 Ma 2 C202 Lu 3 C202 Ma mat1 mat1 4 C203 Lu fis1 fis1 fis1 5 C203 Ma fis1 fis1 fis1 # Despivotear incorrectamente la tabla df . melt ( id_vars = [ \"sala\" ], var_name = \"hora\" , value_name = \"curso\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sala hora curso 0 C201 dia Lu 1 C201 dia Ma 2 C202 dia Lu 3 C202 dia Ma 4 C203 dia Lu 5 C203 dia Ma 6 C201 08:00 mat1 7 C201 08:00 8 C202 08:00 9 C202 08:00 mat1 10 C203 08:00 fis1 11 C203 08:00 fis1 12 C201 09:00 mat1 13 C201 09:00 14 C202 09:00 15 C202 09:00 mat1 16 C203 09:00 fis1 17 C203 09:00 fis1 18 C201 10:00 19 C201 10:00 20 C202 10:00 21 C202 10:00 22 C203 10:00 fis1 23 C203 10:00 fis1 # Despivotear correctamente la tabla df_melt = df . melt ( id_vars = [ \"sala\" , \"dia\" ], var_name = \"hora\" , value_name = \"curso\" ) df_melt [ df_melt . curso != \"\" ] . sort_values ([ \"sala\" , \"dia\" , \"hora\" ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sala dia hora curso 0 C201 Lu 08:00 mat1 6 C201 Lu 09:00 mat1 3 C202 Ma 08:00 mat1 9 C202 Ma 09:00 mat1 4 C203 Lu 08:00 fis1 10 C203 Lu 09:00 fis1 16 C203 Lu 10:00 fis1 5 C203 Ma 08:00 fis1 11 C203 Ma 09:00 fis1 17 C203 Ma 10:00 fis1 b) Relaciones no \u00fanicas columns = [ \"sala\" , \"curso\" , \"Lu\" , \"Ma\" , \"hora\" ] data = [ [ \"C201\" , \"mat1\" , \"X\" , \"\" , \"8:00-10:00\" ], [ \"C202\" , \"mat1\" , \"\" , \"X\" , \"8:00-10:00\" ], [ \"C203\" , \"fis1\" , \"X\" , \"X\" , \"8:00-11:00\" ], ] df = pd . DataFrame ( data = data , columns = columns ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sala curso Lu Ma hora 0 C201 mat1 X 8:00-10:00 1 C202 mat1 X 8:00-10:00 2 C203 fis1 X X 8:00-11:00 M\u00e9todos m\u00e9todo 01: Despivotear manualmente y generar un nuevo dataframe Ventajas : Si se puede es una soluci\u00f3n directa y r\u00e1pida. Desventaja : requiere programaci\u00f3n expl\u00edcita de la tarea, no es reutilizable. # Obtener el d\u00eda lunes df_Lu = df . loc [ df . Lu == \"X\" , [ \"sala\" , \"curso\" , \"hora\" ]] df_Lu [ \"dia\" ] = \"Lu\" df_Lu .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sala curso hora dia 0 C201 mat1 8:00-10:00 Lu 2 C203 fis1 8:00-11:00 Lu # Obtener el d\u00eda martes df_Ma = df . loc [ df . Ma == \"X\" , [ \"sala\" , \"curso\" , \"hora\" ]] df_Ma [ \"dia\" ] = \"Ma\" df_Ma .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sala curso hora dia 1 C202 mat1 8:00-10:00 Ma 2 C203 fis1 8:00-11:00 Ma # Juntar pd . concat ([ df_Lu , df_Ma ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sala curso hora dia 0 C201 mat1 8:00-10:00 Lu 2 C203 fis1 8:00-11:00 Lu 1 C202 mat1 8:00-10:00 Ma 2 C203 fis1 8:00-11:00 Ma m\u00e9todo 02: Iterar sobre las filas y generar contenido para un nuevo dataframe * Ventajas : En general, f\u00e1cil de codificar. * Desventaja : puede ser lento, es ineficiente. my_columns = [ \"sala\" , \"curso\" , \"dia\" , \"hora\" ] my_data = [] for i , df_row in df . iterrows (): # Procesar cada fila if df_row . Lu == \"X\" : my_row = [ df_row . sala , df_row . curso , \"Lu\" , df_row . hora ] my_data . append ( my_row ) if df_row . Ma == \"X\" : my_row = [ df_row . sala , df_row . curso , \"Ma\" , df_row . hora ] my_data . append ( my_row ) new_df = pd . DataFrame ( data = my_data , columns = my_columns ) new_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sala curso dia hora 0 C201 mat1 Lu 8:00-10:00 1 C202 mat1 Ma 8:00-10:00 2 C203 fis1 Lu 8:00-11:00 3 C203 fis1 Ma 8:00-11:00 Referencia Reshaping and pivot tables","title":"Pivot Tables"},{"location":"lectures/data_manipulation/data_manipulation/pivot/#pivot","text":"","title":"Pivot"},{"location":"lectures/data_manipulation/data_manipulation/pivot/#formato-wide-y-formato-long","text":"Dentro del mundo de los dataframe (o datos tabulares) existen dos formas de presentar la naturaleza de los datos: formato wide y formato long. Por ejemplo, el conjunto de datos Zoo Data Set presenta las caracter\u00edsticas de diversos animales, de los cuales presentamos las primeras 5 columnas. animal_name hair feathers eggs milk antelope 1 0 0 1 bear 1 0 0 1 buffalo 1 0 0 1 catfish 0 0 1 0 La tabla as\u00ed presentada se encuentra en wide format , es decir, donde los valores se extienden a trav\u00e9s de las columnas. Ser\u00eda posible representar el mismo contenido anterior en long format , es decir, donde los mismos valores se indicaran a trav\u00e9s de las filas: animal_name characteristic value antelope hair 1 antelope feathers 0 antelope eggs 0 antelope milk 1 ... ... ... catfish hair 0 catfish feathers 0 catfish eggs 1 catfish milk 0 En python existen maneras de pasar del formato wide al formato long y viceversa.","title":"Formato Wide y Formato Long"},{"location":"lectures/data_manipulation/data_manipulation/pivot/#pivotear-y-despivotear-tablas","text":"","title":"Pivotear y despivotear tablas"},{"location":"lectures/data_manipulation/data_manipulation/pivot/#pivot_1","text":"El pivoteo de una tabla corresponde al paso de una tabla desde el formato long al formato wide . T\u00edpicamente esto se realiza para poder comparar los valores que se obtienen para alg\u00fan registro en particular, o para utilizar algunas herramientas de visualizaci\u00f3n b\u00e1sica que requieren dicho formato. Para ejemplificar estos resultados, ocupemos el conjunto de datos terremotos.csv , con contiene los registros de terremotos de distintos paises desde el a\u00f1o 2000 al 2011. import pandas as pd import numpy as np import os # formato long df = pd . read_csv ( os . path . join ( \"data\" , \"terremotos.csv\" ), sep = \",\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A\u00f1o Pais Magnitud 0 2011 Turkey 7.1 1 2011 India 6.9 2 2011 Japan 7.1 3 2011 Burma 6.8 4 2011 Japan 9.0 Por ejemplo, se quiere saber el terremoto de mayor magnitud a nivel de pa\u00eds a\u00f1o. Tenemos dos formas de mostrar la informaci\u00f3n. # formato long df . groupby ([ 'Pais' , 'A\u00f1o' ]) . max () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Magnitud Pais A\u00f1o Afghanistan 2000 6.3 2001 5.0 2002 5.8 2003 5.8 2004 6.5 ... ... ... Turkmenistan 2000 7.0 United States 2001 6.8 2003 6.6 Venezuela 2006 5.5 Vietnam 2005 5.3 154 rows \u00d7 1 columns # formato wide df . pivot_table ( index = \"Pais\" , columns = \"A\u00f1o\" , values = \"Magnitud\" , fill_value = '' , aggfunc = pd . np . max ) /home/falfaro/.cache/pypoetry/virtualenvs/pymessi-xyyw3p3f-py3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A\u00f1o 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 Pais Afghanistan 6.3 5 5.8 5.8 6.5 Afghanistan 7.3 6.5 Algeria 5.7 5.7 5.2 5.5 Algeria 6.8 Argentina 7.2 6.1 ... ... ... ... ... ... ... ... ... ... ... ... ... Turkey 6.3 6.1 7.1 Turkmenistan 7 United States 6.8 6.6 Venezuela 5.5 Vietnam 5.3 72 rows \u00d7 12 columns","title":"Pivot"},{"location":"lectures/data_manipulation/data_manipulation/pivot/#despivotear-un-tabla","text":"El despivotear una tabla corresponde al paso de una tabla desde el formato wide al formato long . Se reconocen dos situaciones: El valor indicado para la columna es \u00fanico, y s\u00f3lo se requiere definir correctamente las columnas. El valor indicado por la columna no es \u00fanico o requiere un procesamiento adicional, y se requiere una iteraci\u00f3n m\u00e1s profunda. Para ejemplificar esto, crearemos un conjunto de datos con los horarios de los ramos que se tiene que dictar en un determinado d\u00eda, hora y lugar. a) El valor indicado para la columna es \u00fanico columns = [ \"sala\" , \"dia\" , \"08:00\" , \"09:00\" , \"10:00\" ] data = [ [ \"C201\" , \"Lu\" , \"mat1\" , \"mat1\" , \"\" ], [ \"C201\" , \"Ma\" , \"\" , \"\" , \"\" ], [ \"C202\" , \"Lu\" , \"\" , \"\" , \"\" ], [ \"C202\" , \"Ma\" , \"mat1\" , \"mat1\" , \"\" ], [ \"C203\" , \"Lu\" , \"fis1\" , \"fis1\" , \"fis1\" ], [ \"C203\" , \"Ma\" , \"fis1\" , \"fis1\" , \"fis1\" ], ] df = pd . DataFrame ( data = data , columns = columns ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sala dia 08:00 09:00 10:00 0 C201 Lu mat1 mat1 1 C201 Ma 2 C202 Lu 3 C202 Ma mat1 mat1 4 C203 Lu fis1 fis1 fis1 5 C203 Ma fis1 fis1 fis1 # Despivotear incorrectamente la tabla df . melt ( id_vars = [ \"sala\" ], var_name = \"hora\" , value_name = \"curso\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sala hora curso 0 C201 dia Lu 1 C201 dia Ma 2 C202 dia Lu 3 C202 dia Ma 4 C203 dia Lu 5 C203 dia Ma 6 C201 08:00 mat1 7 C201 08:00 8 C202 08:00 9 C202 08:00 mat1 10 C203 08:00 fis1 11 C203 08:00 fis1 12 C201 09:00 mat1 13 C201 09:00 14 C202 09:00 15 C202 09:00 mat1 16 C203 09:00 fis1 17 C203 09:00 fis1 18 C201 10:00 19 C201 10:00 20 C202 10:00 21 C202 10:00 22 C203 10:00 fis1 23 C203 10:00 fis1 # Despivotear correctamente la tabla df_melt = df . melt ( id_vars = [ \"sala\" , \"dia\" ], var_name = \"hora\" , value_name = \"curso\" ) df_melt [ df_melt . curso != \"\" ] . sort_values ([ \"sala\" , \"dia\" , \"hora\" ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sala dia hora curso 0 C201 Lu 08:00 mat1 6 C201 Lu 09:00 mat1 3 C202 Ma 08:00 mat1 9 C202 Ma 09:00 mat1 4 C203 Lu 08:00 fis1 10 C203 Lu 09:00 fis1 16 C203 Lu 10:00 fis1 5 C203 Ma 08:00 fis1 11 C203 Ma 09:00 fis1 17 C203 Ma 10:00 fis1 b) Relaciones no \u00fanicas columns = [ \"sala\" , \"curso\" , \"Lu\" , \"Ma\" , \"hora\" ] data = [ [ \"C201\" , \"mat1\" , \"X\" , \"\" , \"8:00-10:00\" ], [ \"C202\" , \"mat1\" , \"\" , \"X\" , \"8:00-10:00\" ], [ \"C203\" , \"fis1\" , \"X\" , \"X\" , \"8:00-11:00\" ], ] df = pd . DataFrame ( data = data , columns = columns ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sala curso Lu Ma hora 0 C201 mat1 X 8:00-10:00 1 C202 mat1 X 8:00-10:00 2 C203 fis1 X X 8:00-11:00","title":"Despivotear un tabla"},{"location":"lectures/data_manipulation/data_manipulation/pivot/#metodos","text":"m\u00e9todo 01: Despivotear manualmente y generar un nuevo dataframe Ventajas : Si se puede es una soluci\u00f3n directa y r\u00e1pida. Desventaja : requiere programaci\u00f3n expl\u00edcita de la tarea, no es reutilizable. # Obtener el d\u00eda lunes df_Lu = df . loc [ df . Lu == \"X\" , [ \"sala\" , \"curso\" , \"hora\" ]] df_Lu [ \"dia\" ] = \"Lu\" df_Lu .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sala curso hora dia 0 C201 mat1 8:00-10:00 Lu 2 C203 fis1 8:00-11:00 Lu # Obtener el d\u00eda martes df_Ma = df . loc [ df . Ma == \"X\" , [ \"sala\" , \"curso\" , \"hora\" ]] df_Ma [ \"dia\" ] = \"Ma\" df_Ma .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sala curso hora dia 1 C202 mat1 8:00-10:00 Ma 2 C203 fis1 8:00-11:00 Ma # Juntar pd . concat ([ df_Lu , df_Ma ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sala curso hora dia 0 C201 mat1 8:00-10:00 Lu 2 C203 fis1 8:00-11:00 Lu 1 C202 mat1 8:00-10:00 Ma 2 C203 fis1 8:00-11:00 Ma m\u00e9todo 02: Iterar sobre las filas y generar contenido para un nuevo dataframe * Ventajas : En general, f\u00e1cil de codificar. * Desventaja : puede ser lento, es ineficiente. my_columns = [ \"sala\" , \"curso\" , \"dia\" , \"hora\" ] my_data = [] for i , df_row in df . iterrows (): # Procesar cada fila if df_row . Lu == \"X\" : my_row = [ df_row . sala , df_row . curso , \"Lu\" , df_row . hora ] my_data . append ( my_row ) if df_row . Ma == \"X\" : my_row = [ df_row . sala , df_row . curso , \"Ma\" , df_row . hora ] my_data . append ( my_row ) new_df = pd . DataFrame ( data = my_data , columns = my_columns ) new_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sala curso dia hora 0 C201 mat1 Lu 8:00-10:00 1 C202 mat1 Ma 8:00-10:00 2 C203 fis1 Lu 8:00-11:00 3 C203 fis1 Ma 8:00-11:00","title":"M\u00e9todos"},{"location":"lectures/data_manipulation/data_manipulation/pivot/#referencia","text":"Reshaping and pivot tables","title":"Referencia"},{"location":"lectures/data_manipulation/eda/eda/","text":"Caso Aplicado EL dataset terremotos.csv contiene la informaci\u00f3n de los terremotos de los pa\u00edses durante el a\u00f1o 2000 al 2011. Debido a que la informaci\u00f3n de este dataset es relativamente f\u00e1cil de trabajar, hemos creado un dataset denominado terremotos_contaminados.csv que posee informaci\u00f3n contaminada en cada una de sus columnas. De esta forma se podr\u00e1 ilustrar los distintos inconvenientes que se puede tener en el an\u00e1lisis exploratorio de datos. Bases del experimento Lo primero es identificar las variables que influyen en el estudio y la naturaleza de esta. Pa\u00eds : Descripci\u00f3n: Pa\u00eds del evento s\u00edsmico. Tipo de dato: string Limitantes: No pueden haber nombre de ciudades, comunas o pueblos. A\u00f1o : Descripci\u00f3n: A\u00f1o del evento s\u00edsmico. Tipo de dato: integer . Limitantes: el a\u00f1o debe estar entre al a\u00f1o 2000 y 2011. Magnitud : Descripci\u00f3n: Magnitud del evento s\u00edsmico. Tipo de dato: float . Limitantes: la magnitud puede estar entre 5 y 9.6. \u00bf Por qu\u00e9 no puede ser un terremoto con una intensidad mayor a 9.6? . Esto se debe a que el terremoto con mayor magnitud registrado por la humanidad es de 9.6, ocurrido en Chile (Valdivia) durante el a\u00f1o 1960. Por lo tanto, entre mayor conocimiento se tenga del fen\u00f3meno en estudio, m\u00e1s restrictivo se vulve el an\u00e1lisis exploratorio y m\u00e1s sentido tienen los resultados obtenidos. Conjunto de datos El conjunto de datos consta de cuatro columnas: * Pa\u00eds * A\u00f1o * Magnitud * Informaci\u00f3n Checklist del experimento Dado que conocemos el fen\u00f3meno en estudio, vayamos realizando un checklist de los procesos para hacer un correcto EDA. 1. \u00bf Qu\u00e9 pregunta (s) est\u00e1s tratando de resolver (o probar que est\u00e1s equivocado)? El objetivo es encontrar el terremoto de mayor magnitud por pa\u00eds en los distintos a\u00f1os. 2. \u00bf Qu\u00e9 tipo de datos tienes ? Los tipos de variables que tiene el conjunto de datos son: Categ\u00f3ricas: Pa\u00eds, Informaci\u00f3n. Num\u00e9ricas: A\u00f1o, Magnitud. # cargar librerias import os import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns pd . set_option ( 'display.max_columns' , 500 ) # Ver m\u00e1s columnas de los dataframes # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab % matplotlib inline # cargar datos terremotos_data = pd . read_csv ( os . path . join ( \"data\" , \"terremotos_contaminados.csv\" ), sep = \",\" ) terremotos_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A\u00f1o Pais Magnitud Informacion 0 2000 Turkey 6 info no valiosa 1 2000 Turkmenistan 7 info no valiosa 2 2000 Azerbaijan 6.5 info no valiosa 3 2000 Azerbaijan 6.8 info no valiosa 4 2000 Papua New Guinea 8 info no valiosa # normalizar columnas (minusculas y sin espacios) terremotos_data . columns = terremotos_data . columns . str . lower () . str . strip () terremotos_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a\u00f1o pais magnitud informacion 0 2000 Turkey 6 info no valiosa 1 2000 Turkmenistan 7 info no valiosa 2 2000 Azerbaijan 6.5 info no valiosa 3 2000 Azerbaijan 6.8 info no valiosa 4 2000 Papua New Guinea 8 info no valiosa # resumen de la informacion def resumen_por_columna ( df , cols ): pd_series = df [ cols ] # elementos distintos l_unique = pd_series . unique () # elementos vacios l_vacios = pd_series [ pd_series . isna ()] df_info = pd . DataFrame ({ 'columna' : [ cols ], 'unicos' : [ len ( l_unique )], 'vacios' : [ len ( l_vacios )] }) return df_info frames = [] for col in terremotos_data . columns : aux_df = resumen_por_columna ( terremotos_data , col ) frames . append ( aux_df ) df_info = pd . concat ( frames ) . reset_index ( drop = True ) df_info [ '% vacios' ] = df_info [ 'vacios' ] / len ( terremotos_data ) df_info .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } columna unicos vacios % vacios 0 a\u00f1o 17 2 0.008772 1 pais 75 2 0.008772 2 magnitud 46 3 0.013158 3 informacion 4 8 0.035088 plt . style . use ( 'default' ) plt . figure ( figsize = ( 7 , 5 )) plotting = sns . barplot ( x = \"columna\" , y = \"unicos\" , data = df_info . sort_values ( 'unicos' ), palette = \"Greys_d\" , linewidth = 3 ) for container in plotting . containers : plotting . bar_label ( container , fontsize = 12 ) plt . title ( \"Valores Distintos\" ) plt . yticks ( fontsize = 14 ) plt . xticks ( fontsize = 14 ) plt . show () \u00bfQu\u00e9 falta en los datos y c\u00f3mo los maneja? En este caso, se tiene la informaci\u00f3n suficiente para poder realizar el experimento, solo falta ver que los datos de la muestra no esten lo suficientemente contaminados. \u00bfQu\u00e9 hacer con los datos faltantes, outliers o informaci\u00f3n mal inputada? Este caso es m\u00e1s interesante, y se necesita ir detallando columna por columna los distintos an\u00e1lisis. Columna: A\u00f1o Los a\u00f1os distintos en la muestra son: terremotos_data [ 'a\u00f1o' ] . unique () array(['2000', '2001', 'dos mil uno', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '1997', '1990', '1999', nan], dtype=object) # bar plot: a\u00f1o plt . style . use ( 'default' ) plt . figure ( figsize = ( 7 , 4 )) plotting = sns . countplot ( y = \"a\u00f1o\" , data = terremotos_data , order = terremotos_data [ 'a\u00f1o' ] . value_counts () . index , palette = \"Greys_d\" , linewidth = 3 ) for container in plotting . containers : plotting . bar_label ( container , fontsize = 12 ) plt . show () Se presentan las siguientes anomalidades: A\u00f1os sin importancia : Se ha establecido que los a\u00f1os de estudios son desde el a\u00f1o 2000 al 2011. Nombres mal escritos : en este caso sabemos que 'dos mil uno' corresponde a '2001'. Datos vac\u00edo Ahora la pregunta es, \u00bf qu\u00e9 debemos hacer primero?. Lo primero es corregir la informaci\u00f3n, dar un formato est\u00e1ndar a los datos y luego filtrar. a) Correcci\u00f3n terremotos_data . loc [ terremotos_data [ 'a\u00f1o' ] == 'dos mil uno' , 'a\u00f1o' ] = '2001' terremotos_data . loc [ terremotos_data [ 'a\u00f1o' ] . isnull (), 'a\u00f1o' ] = '0' terremotos_data [ 'a\u00f1o' ] . unique () array(['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '1997', '1990', '1999', '0'], dtype=object) b) Formato El formato de los a\u00f1os es integer , por lo tanto se le debe dar ese formato. terremotos_data [ 'a\u00f1o' ] = terremotos_data [ 'a\u00f1o' ] . astype ( int ) c) Filtro Se ha establecido que los a\u00f1os de estudios son desde el a\u00f1o 2000 al 2011, por lo tanto los a\u00f1os en estudios deberian ser: anios_estudio = [ x for x in range ( 2000 , 2011 + 1 )] print ( anios_estudio ) [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011] Por lo tanto ya tenemo nuestro primer filtro: mask_anio = terremotos_data [ 'a\u00f1o' ] . isin ( anios_estudio ) Columna: Pa\u00eds Los paises distintos en la muestra son: set ( terremotos_data [ 'pais' ] . unique ()) {'Afghanistan', 'Afghanistan ', 'Algeria', 'Algeria ', 'Argentina', 'Azerbaijan', 'Azerbaijan ', 'Bangladesh', 'Burma ', 'Chile', 'Chile ', 'China', 'China ', 'Colombia', 'Costa Rica', 'Costa Rica ', 'Democratic Republic of the Congo', 'Democratic Republic of the Congo ', 'Dominican Republic', 'Ecuador', 'El Salvador ', 'Greece', 'Greece ', 'Guadeloupe', 'Guatemala', 'Haiti ', 'India', 'India ', 'Indonesia', 'Indonesia ', 'Iran', 'Iran ', 'Iran, 2005 Qeshm earthquake', 'Italy', 'Italy ', 'Japan', 'Japan ', 'Kazakhstan', 'Kyrgyzstan ', 'Martinique', 'Mexico ', 'Morocco', 'Morocco ', 'Mozambique', 'New Zealand', 'New Zealand ', 'Nicaragua', 'Pakistan', 'Pakistan ', 'Panama', 'Papua New Guinea', 'Peru', 'Peru ', 'Philippines', 'Russian Federation', 'Rwanda', 'Samoa ', 'Serbia', 'Slovenia', 'Solomon Islands ', 'Taiwan', 'Taiwan ', 'Tajikistan', 'Tajikistan ', 'Tanzania', 'Tanzania ', 'Turkey', 'Turkey ', 'Turkmenistan', 'United States ', 'Venezuela', 'Vietnam', 'arica', nan, 'shile'} # bar plot: pais plt . style . use ( 'default' ) plt . figure ( figsize = ( 14 , 14 )) plotting = sns . countplot ( y = \"pais\" , data = terremotos_data , order = terremotos_data . pais . value_counts () . index , palette = \"Greys_d\" , linewidth = 3 ) for container in plotting . containers : plotting . bar_label ( container , fontsize = 12 ) plt . show () Se presentan las siguientes anomalidades: Formato de los nombres : no se le ha aplicado strip() y lower() , por lo cual tenemos casos como: 'Turkey' y 'Turkey ' como elementos diferentes. Nombres mal escritos : en este caso sabemos que 'shile' corresponde a 'Chile' e 'Iran, 2005 Qeshm earthquake' corrsponde a 'Iran'. Datos vac\u00edo Se solucionar\u00e1 cada uno de estos inconvenientes: Correcci\u00f3n de los nombres terremotos_data . loc [ terremotos_data [ 'pais' ] == 'arica' , 'pais' ] = 'chile' terremotos_data . loc [ terremotos_data [ 'pais' ] == 'shile' , 'pais' ] = 'chile' terremotos_data . loc [ terremotos_data [ 'pais' ] == 'Iran, 2005 Qeshm earthquake' , 'pais' ] = 'iran' terremotos_data . loc [ terremotos_data [ 'pais' ] . isnull (), 'pais' ] = 'sin_nombre' Formato # correccion formato de nombre terremotos_data [ 'pais' ] = terremotos_data [ 'pais' ] . str . lower () . str . strip () Filtro mask_pais = terremotos_data [ 'pais' ] != 'sin_nombre' Columna: Magnitud Las magnitudes distintas en la muestra son: terremotos_data [ 'magnitud' ] . unique () array(['6', '7', '6.5', '6.8', '8', '5.7', '6.4', '5.5', '6.3', '5.4', '6.1', '6.7', '7.9', '7.2', '7.5', '5.3', '5.9', '9.7', '5.8', '4.7', '7.6', '8.4', '5', '5.6', '6.6', '6.2', '7.1', '7.3', '5.1', '5.2', '8.3', '6.9', '9.1', '4.9', '7.8', '8.6', '7.7', '7.4', '8.5', '8.1', '8.8', '9', '-10', nan, '2002-Tanzania-5.8', '2003-japan-8.5'], dtype=object) # bar plot: magnitud plt . style . use ( 'default' ) plt . figure ( figsize = ( 10 , 7 )) plotting = sns . countplot ( y = \"magnitud\" , data = terremotos_data , order = terremotos_data [ 'magnitud' ] . value_counts () . index , palette = \"Greys_d\" , linewidth = 3 ) for container in plotting . containers : plotting . bar_label ( container , fontsize = 10 ) plt . show () Se presentan las siguientes anomalidades: Magnitudes sin importancia : Se ha establecido que las magnitudes de un terremoto son de 5 a 9.6. Datos con informaci\u00f3n comprimida : Debido a una inputaci\u00f3n incorrecta de los datos o a una mala lectura, la informaci\u00f3n se concentra en una celda. Datos vac\u00edo Correcci\u00f3n de las magnitudes # caso: tradicional terremotos_data . loc [ terremotos_data [ 'magnitud' ] == '2002-Tanzania-5.8' , 'magnitud' ] = '0' terremotos_data . loc [ terremotos_data [ 'magnitud' ] == '2003-japan-8.5' , 'magnitud' ] = '0' terremotos_data . loc [ terremotos_data [ 'magnitud' ] . isnull (), 'magnitud' ] = '0' # caso: informacion comprimida terremotos_data . loc [ - 1 ] = [ 2002 , 'tanzania' , '5.8' , '-' ] terremotos_data . loc [ - 2 ] = [ 2003 , 'japan' , '8.5' , '-' ] terremotos_data = terremotos_data . reset_index ( drop = True ) terremotos_data . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a\u00f1o pais magnitud informacion 225 0 sin_nombre 0 NaN 226 0 sin_nombre 0 NaN 227 2005 chile 8 valiosa 228 2002 tanzania 5.8 - 229 2003 japan 8.5 - Correcci\u00f3n formato de las magnitudes terremotos_data [ 'magnitud' ] = terremotos_data [ 'magnitud' ] . astype ( float ) terremotos_data [ 'magnitud' ] . unique () array([ 6. , 7. , 6.5, 6.8, 8. , 5.7, 6.4, 5.5, 6.3, 5.4, 6.1, 6.7, 7.9, 7.2, 7.5, 5.3, 5.9, 9.7, 5.8, 4.7, 7.6, 8.4, 5. , 5.6, 6.6, 6.2, 7.1, 7.3, 5.1, 5.2, 8.3, 6.9, 9.1, 4.9, 7.8, 8.6, 7.7, 7.4, 8.5, 8.1, 8.8, 9. , -10. , 0. ]) c) Filtro de las magnitudes mask_mag_inf = terremotos_data [ 'magnitud' ] >= 5 mask_mag_sup = terremotos_data [ 'magnitud' ] <= 9.6 mask_mag = mask_mag_inf & mask_mag_sup Columna: Informaci\u00f3n La cantidad de elementos distintos para la columna informaci\u00f3n son: terremotos_data [ 'informacion' ] . unique () array(['info no valiosa', 'info valiosa', nan, 'valiosa', '-'], dtype=object) Se observa que esta columna no aporta informaci\u00f3n valiosa al estudio, quedando excluida para cualquier an\u00e1lisis importante. terremotos_data . drop ( 'informacion' , axis = 1 , inplace = True ) terremotos_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a\u00f1o pais magnitud 0 2000 turkey 6.0 1 2000 turkmenistan 7.0 2 2000 azerbaijan 6.5 3 2000 azerbaijan 6.8 4 2000 papua new guinea 8.0 5. \u00bfSe puede sacar m\u00e1s provecho a los datos ? Una vez realizado toda la limpieza de datos, debemos filtrar la informaci\u00f3n que se considere importante. A\u00f1os : A\u00f1os desde el 2000 al 2011. Pa\u00eds : Paises con nombre distinto de sin_nombre Magnitud : Magnitud entre 5 y 9.6. # aplicar filtros terremotos_data_filtrado = terremotos_data [ mask_anio & mask_pais & mask_mag ] terremotos_data_filtrado . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a\u00f1o pais magnitud 0 2000 turkey 6.0 1 2000 turkmenistan 7.0 2 2000 azerbaijan 6.5 3 2000 azerbaijan 6.8 4 2000 papua new guinea 8.0 Veamos cu\u00e1nta informaci\u00f3n se perdio: print ( 'Cantidad de filas dataset sin filtro:' , len ( terremotos_data )) print ( 'Cantidad de filas dataset con filtro:' , len ( terremotos_data_filtrado )) Cantidad de filas dataset sin filtro: 230 Cantidad de filas dataset con filtro: 212 Ahora veamos el el resumen de la informaci\u00f3n para el nuevo dataset: frames = [] for col in terremotos_data_filtrado . columns : aux_df = resumen_por_columna ( terremotos_data_filtrado , col ) frames . append ( aux_df ) df_info = pd . concat ( frames ) . reset_index ( drop = True ) df_info .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } columna unicos vacios 0 a\u00f1o 12 0 1 pais 50 0 2 magnitud 39 0 Finalmente, podemos responder la pregunta del inicio: Soluci\u00f3n # formato wide terremotos_data_filtrado . pivot_table ( index = \"pais\" , columns = \"a\u00f1o\" , values = \"magnitud\" , fill_value = '' , aggfunc = pd . np . max ) <ipython-input-31-8c1a5edb19d8>:6: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead aggfunc=pd.np.max) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a\u00f1o 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 pais afghanistan 6.3 5.0 7.3 5.8 6.5 6.5 algeria 5.7 6.8 5.2 5.5 argentina 7.2 6.1 azerbaijan 6.8 bangladesh 5.6 burma 6.8 chile 6.3 8.0 7.7 8.8 china 5.9 5.6 5.5 6.3 5.3 5.2 5.0 6.1 7.9 5.7 6.9 5.4 colombia 6.5 5.9 costa rica 6.4 6.1 democratic republic of the congo 6.2 5.9 dominican republic 6.4 ecuador 5.5 el salvador 7.6 greece 6.2 6.4 guadeloupe 6.3 guatemala 6.4 haiti 7.0 india 7.6 6.5 5.1 5.3 5.1 6.9 indonesia 7.9 7.5 6.9 9.1 8.6 7.7 8.5 7.3 7.6 iran 5.3 6.5 6.6 6.3 6.4 6.1 italy 5.9 6.2 japan 6.1 6.8 8.3 6.6 6.6 6.7 6.9 6.4 9.0 kazakhstan 6.0 kyrgyzstan 6.9 martinique 7.4 mexico 7.5 morocco 6.3 mozambique 7.0 new zealand 5.4 6.6 6.3 nicaragua 5.4 pakistan 6.3 5.4 7.6 5.2 6.4 panama 6.5 papua new guinea 8.0 7.6 6.1 peru 8.4 7.5 8.0 philippines 7.5 6.5 7.1 5.3 russian federation 7.3 6.2 rwanda 5.3 samoa 8.1 serbia 5.7 slovenia 5.2 solomon islands 8.1 taiwan 6.4 7.1 5.2 7.0 tajikistan 5.2 5.6 5.2 tanzania 6.4 5.5 6.8 turkey 6.0 6.5 6.3 5.6 5.9 6.1 7.1 turkmenistan 7.0 united states 6.8 6.6 venezuela 5.5 vietnam 5.3 \u00bf Podemos sacar m\u00e1s informaci\u00f3n ?. Por supuesto que se puede, no obstante, siempre se debe ser preciso con la informaci\u00f3n que se Conclusi\u00f3n del caso El an\u00e1lisis exploratorio de datos (EDA) es una metodolog\u00eda que sirve para asegurarse de la calidad de los datos. A medida que se tiene m\u00e1s expertice en el tema, mejor es el an\u00e1lisis de datos y por tanto, mejor son los resultados obtenidos. No existe un procedimiento est\u00e1ndar para realizar el EDA, pero siempre se debe tener una claridad mental con: Problema que se quiere resolverlo C\u00f3mo resolver el problema Posibles problemas de la muestra (datos perdidos, ouliers, etc.) Referencia Detailed exploratory data analysis with python Exploratory Data Analysis (EDA) and Data Visualization with Python","title":"Caso Aplicado"},{"location":"lectures/data_manipulation/eda/eda/#caso-aplicado","text":"EL dataset terremotos.csv contiene la informaci\u00f3n de los terremotos de los pa\u00edses durante el a\u00f1o 2000 al 2011. Debido a que la informaci\u00f3n de este dataset es relativamente f\u00e1cil de trabajar, hemos creado un dataset denominado terremotos_contaminados.csv que posee informaci\u00f3n contaminada en cada una de sus columnas. De esta forma se podr\u00e1 ilustrar los distintos inconvenientes que se puede tener en el an\u00e1lisis exploratorio de datos.","title":"Caso Aplicado"},{"location":"lectures/data_manipulation/eda/eda/#bases-del-experimento","text":"Lo primero es identificar las variables que influyen en el estudio y la naturaleza de esta. Pa\u00eds : Descripci\u00f3n: Pa\u00eds del evento s\u00edsmico. Tipo de dato: string Limitantes: No pueden haber nombre de ciudades, comunas o pueblos. A\u00f1o : Descripci\u00f3n: A\u00f1o del evento s\u00edsmico. Tipo de dato: integer . Limitantes: el a\u00f1o debe estar entre al a\u00f1o 2000 y 2011. Magnitud : Descripci\u00f3n: Magnitud del evento s\u00edsmico. Tipo de dato: float . Limitantes: la magnitud puede estar entre 5 y 9.6. \u00bf Por qu\u00e9 no puede ser un terremoto con una intensidad mayor a 9.6? . Esto se debe a que el terremoto con mayor magnitud registrado por la humanidad es de 9.6, ocurrido en Chile (Valdivia) durante el a\u00f1o 1960. Por lo tanto, entre mayor conocimiento se tenga del fen\u00f3meno en estudio, m\u00e1s restrictivo se vulve el an\u00e1lisis exploratorio y m\u00e1s sentido tienen los resultados obtenidos.","title":"Bases del experimento"},{"location":"lectures/data_manipulation/eda/eda/#conjunto-de-datos","text":"El conjunto de datos consta de cuatro columnas: * Pa\u00eds * A\u00f1o * Magnitud * Informaci\u00f3n","title":"Conjunto de datos"},{"location":"lectures/data_manipulation/eda/eda/#checklist-del-experimento","text":"Dado que conocemos el fen\u00f3meno en estudio, vayamos realizando un checklist de los procesos para hacer un correcto EDA. 1. \u00bf Qu\u00e9 pregunta (s) est\u00e1s tratando de resolver (o probar que est\u00e1s equivocado)? El objetivo es encontrar el terremoto de mayor magnitud por pa\u00eds en los distintos a\u00f1os. 2. \u00bf Qu\u00e9 tipo de datos tienes ? Los tipos de variables que tiene el conjunto de datos son: Categ\u00f3ricas: Pa\u00eds, Informaci\u00f3n. Num\u00e9ricas: A\u00f1o, Magnitud. # cargar librerias import os import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns pd . set_option ( 'display.max_columns' , 500 ) # Ver m\u00e1s columnas de los dataframes # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab % matplotlib inline # cargar datos terremotos_data = pd . read_csv ( os . path . join ( \"data\" , \"terremotos_contaminados.csv\" ), sep = \",\" ) terremotos_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A\u00f1o Pais Magnitud Informacion 0 2000 Turkey 6 info no valiosa 1 2000 Turkmenistan 7 info no valiosa 2 2000 Azerbaijan 6.5 info no valiosa 3 2000 Azerbaijan 6.8 info no valiosa 4 2000 Papua New Guinea 8 info no valiosa # normalizar columnas (minusculas y sin espacios) terremotos_data . columns = terremotos_data . columns . str . lower () . str . strip () terremotos_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a\u00f1o pais magnitud informacion 0 2000 Turkey 6 info no valiosa 1 2000 Turkmenistan 7 info no valiosa 2 2000 Azerbaijan 6.5 info no valiosa 3 2000 Azerbaijan 6.8 info no valiosa 4 2000 Papua New Guinea 8 info no valiosa # resumen de la informacion def resumen_por_columna ( df , cols ): pd_series = df [ cols ] # elementos distintos l_unique = pd_series . unique () # elementos vacios l_vacios = pd_series [ pd_series . isna ()] df_info = pd . DataFrame ({ 'columna' : [ cols ], 'unicos' : [ len ( l_unique )], 'vacios' : [ len ( l_vacios )] }) return df_info frames = [] for col in terremotos_data . columns : aux_df = resumen_por_columna ( terremotos_data , col ) frames . append ( aux_df ) df_info = pd . concat ( frames ) . reset_index ( drop = True ) df_info [ '% vacios' ] = df_info [ 'vacios' ] / len ( terremotos_data ) df_info .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } columna unicos vacios % vacios 0 a\u00f1o 17 2 0.008772 1 pais 75 2 0.008772 2 magnitud 46 3 0.013158 3 informacion 4 8 0.035088 plt . style . use ( 'default' ) plt . figure ( figsize = ( 7 , 5 )) plotting = sns . barplot ( x = \"columna\" , y = \"unicos\" , data = df_info . sort_values ( 'unicos' ), palette = \"Greys_d\" , linewidth = 3 ) for container in plotting . containers : plotting . bar_label ( container , fontsize = 12 ) plt . title ( \"Valores Distintos\" ) plt . yticks ( fontsize = 14 ) plt . xticks ( fontsize = 14 ) plt . show () \u00bfQu\u00e9 falta en los datos y c\u00f3mo los maneja? En este caso, se tiene la informaci\u00f3n suficiente para poder realizar el experimento, solo falta ver que los datos de la muestra no esten lo suficientemente contaminados. \u00bfQu\u00e9 hacer con los datos faltantes, outliers o informaci\u00f3n mal inputada? Este caso es m\u00e1s interesante, y se necesita ir detallando columna por columna los distintos an\u00e1lisis.","title":"Checklist del experimento"},{"location":"lectures/data_manipulation/eda/eda/#columna-ano","text":"Los a\u00f1os distintos en la muestra son: terremotos_data [ 'a\u00f1o' ] . unique () array(['2000', '2001', 'dos mil uno', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '1997', '1990', '1999', nan], dtype=object) # bar plot: a\u00f1o plt . style . use ( 'default' ) plt . figure ( figsize = ( 7 , 4 )) plotting = sns . countplot ( y = \"a\u00f1o\" , data = terremotos_data , order = terremotos_data [ 'a\u00f1o' ] . value_counts () . index , palette = \"Greys_d\" , linewidth = 3 ) for container in plotting . containers : plotting . bar_label ( container , fontsize = 12 ) plt . show () Se presentan las siguientes anomalidades: A\u00f1os sin importancia : Se ha establecido que los a\u00f1os de estudios son desde el a\u00f1o 2000 al 2011. Nombres mal escritos : en este caso sabemos que 'dos mil uno' corresponde a '2001'. Datos vac\u00edo Ahora la pregunta es, \u00bf qu\u00e9 debemos hacer primero?. Lo primero es corregir la informaci\u00f3n, dar un formato est\u00e1ndar a los datos y luego filtrar. a) Correcci\u00f3n terremotos_data . loc [ terremotos_data [ 'a\u00f1o' ] == 'dos mil uno' , 'a\u00f1o' ] = '2001' terremotos_data . loc [ terremotos_data [ 'a\u00f1o' ] . isnull (), 'a\u00f1o' ] = '0' terremotos_data [ 'a\u00f1o' ] . unique () array(['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '1997', '1990', '1999', '0'], dtype=object) b) Formato El formato de los a\u00f1os es integer , por lo tanto se le debe dar ese formato. terremotos_data [ 'a\u00f1o' ] = terremotos_data [ 'a\u00f1o' ] . astype ( int ) c) Filtro Se ha establecido que los a\u00f1os de estudios son desde el a\u00f1o 2000 al 2011, por lo tanto los a\u00f1os en estudios deberian ser: anios_estudio = [ x for x in range ( 2000 , 2011 + 1 )] print ( anios_estudio ) [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011] Por lo tanto ya tenemo nuestro primer filtro: mask_anio = terremotos_data [ 'a\u00f1o' ] . isin ( anios_estudio )","title":"Columna: A\u00f1o"},{"location":"lectures/data_manipulation/eda/eda/#columna-pais","text":"Los paises distintos en la muestra son: set ( terremotos_data [ 'pais' ] . unique ()) {'Afghanistan', 'Afghanistan ', 'Algeria', 'Algeria ', 'Argentina', 'Azerbaijan', 'Azerbaijan ', 'Bangladesh', 'Burma ', 'Chile', 'Chile ', 'China', 'China ', 'Colombia', 'Costa Rica', 'Costa Rica ', 'Democratic Republic of the Congo', 'Democratic Republic of the Congo ', 'Dominican Republic', 'Ecuador', 'El Salvador ', 'Greece', 'Greece ', 'Guadeloupe', 'Guatemala', 'Haiti ', 'India', 'India ', 'Indonesia', 'Indonesia ', 'Iran', 'Iran ', 'Iran, 2005 Qeshm earthquake', 'Italy', 'Italy ', 'Japan', 'Japan ', 'Kazakhstan', 'Kyrgyzstan ', 'Martinique', 'Mexico ', 'Morocco', 'Morocco ', 'Mozambique', 'New Zealand', 'New Zealand ', 'Nicaragua', 'Pakistan', 'Pakistan ', 'Panama', 'Papua New Guinea', 'Peru', 'Peru ', 'Philippines', 'Russian Federation', 'Rwanda', 'Samoa ', 'Serbia', 'Slovenia', 'Solomon Islands ', 'Taiwan', 'Taiwan ', 'Tajikistan', 'Tajikistan ', 'Tanzania', 'Tanzania ', 'Turkey', 'Turkey ', 'Turkmenistan', 'United States ', 'Venezuela', 'Vietnam', 'arica', nan, 'shile'} # bar plot: pais plt . style . use ( 'default' ) plt . figure ( figsize = ( 14 , 14 )) plotting = sns . countplot ( y = \"pais\" , data = terremotos_data , order = terremotos_data . pais . value_counts () . index , palette = \"Greys_d\" , linewidth = 3 ) for container in plotting . containers : plotting . bar_label ( container , fontsize = 12 ) plt . show () Se presentan las siguientes anomalidades: Formato de los nombres : no se le ha aplicado strip() y lower() , por lo cual tenemos casos como: 'Turkey' y 'Turkey ' como elementos diferentes. Nombres mal escritos : en este caso sabemos que 'shile' corresponde a 'Chile' e 'Iran, 2005 Qeshm earthquake' corrsponde a 'Iran'. Datos vac\u00edo Se solucionar\u00e1 cada uno de estos inconvenientes: Correcci\u00f3n de los nombres terremotos_data . loc [ terremotos_data [ 'pais' ] == 'arica' , 'pais' ] = 'chile' terremotos_data . loc [ terremotos_data [ 'pais' ] == 'shile' , 'pais' ] = 'chile' terremotos_data . loc [ terremotos_data [ 'pais' ] == 'Iran, 2005 Qeshm earthquake' , 'pais' ] = 'iran' terremotos_data . loc [ terremotos_data [ 'pais' ] . isnull (), 'pais' ] = 'sin_nombre' Formato # correccion formato de nombre terremotos_data [ 'pais' ] = terremotos_data [ 'pais' ] . str . lower () . str . strip () Filtro mask_pais = terremotos_data [ 'pais' ] != 'sin_nombre'","title":"Columna: Pa\u00eds"},{"location":"lectures/data_manipulation/eda/eda/#columna-magnitud","text":"Las magnitudes distintas en la muestra son: terremotos_data [ 'magnitud' ] . unique () array(['6', '7', '6.5', '6.8', '8', '5.7', '6.4', '5.5', '6.3', '5.4', '6.1', '6.7', '7.9', '7.2', '7.5', '5.3', '5.9', '9.7', '5.8', '4.7', '7.6', '8.4', '5', '5.6', '6.6', '6.2', '7.1', '7.3', '5.1', '5.2', '8.3', '6.9', '9.1', '4.9', '7.8', '8.6', '7.7', '7.4', '8.5', '8.1', '8.8', '9', '-10', nan, '2002-Tanzania-5.8', '2003-japan-8.5'], dtype=object) # bar plot: magnitud plt . style . use ( 'default' ) plt . figure ( figsize = ( 10 , 7 )) plotting = sns . countplot ( y = \"magnitud\" , data = terremotos_data , order = terremotos_data [ 'magnitud' ] . value_counts () . index , palette = \"Greys_d\" , linewidth = 3 ) for container in plotting . containers : plotting . bar_label ( container , fontsize = 10 ) plt . show () Se presentan las siguientes anomalidades: Magnitudes sin importancia : Se ha establecido que las magnitudes de un terremoto son de 5 a 9.6. Datos con informaci\u00f3n comprimida : Debido a una inputaci\u00f3n incorrecta de los datos o a una mala lectura, la informaci\u00f3n se concentra en una celda. Datos vac\u00edo Correcci\u00f3n de las magnitudes # caso: tradicional terremotos_data . loc [ terremotos_data [ 'magnitud' ] == '2002-Tanzania-5.8' , 'magnitud' ] = '0' terremotos_data . loc [ terremotos_data [ 'magnitud' ] == '2003-japan-8.5' , 'magnitud' ] = '0' terremotos_data . loc [ terremotos_data [ 'magnitud' ] . isnull (), 'magnitud' ] = '0' # caso: informacion comprimida terremotos_data . loc [ - 1 ] = [ 2002 , 'tanzania' , '5.8' , '-' ] terremotos_data . loc [ - 2 ] = [ 2003 , 'japan' , '8.5' , '-' ] terremotos_data = terremotos_data . reset_index ( drop = True ) terremotos_data . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a\u00f1o pais magnitud informacion 225 0 sin_nombre 0 NaN 226 0 sin_nombre 0 NaN 227 2005 chile 8 valiosa 228 2002 tanzania 5.8 - 229 2003 japan 8.5 - Correcci\u00f3n formato de las magnitudes terremotos_data [ 'magnitud' ] = terremotos_data [ 'magnitud' ] . astype ( float ) terremotos_data [ 'magnitud' ] . unique () array([ 6. , 7. , 6.5, 6.8, 8. , 5.7, 6.4, 5.5, 6.3, 5.4, 6.1, 6.7, 7.9, 7.2, 7.5, 5.3, 5.9, 9.7, 5.8, 4.7, 7.6, 8.4, 5. , 5.6, 6.6, 6.2, 7.1, 7.3, 5.1, 5.2, 8.3, 6.9, 9.1, 4.9, 7.8, 8.6, 7.7, 7.4, 8.5, 8.1, 8.8, 9. , -10. , 0. ]) c) Filtro de las magnitudes mask_mag_inf = terremotos_data [ 'magnitud' ] >= 5 mask_mag_sup = terremotos_data [ 'magnitud' ] <= 9.6 mask_mag = mask_mag_inf & mask_mag_sup","title":"Columna: Magnitud"},{"location":"lectures/data_manipulation/eda/eda/#columna-informacion","text":"La cantidad de elementos distintos para la columna informaci\u00f3n son: terremotos_data [ 'informacion' ] . unique () array(['info no valiosa', 'info valiosa', nan, 'valiosa', '-'], dtype=object) Se observa que esta columna no aporta informaci\u00f3n valiosa al estudio, quedando excluida para cualquier an\u00e1lisis importante. terremotos_data . drop ( 'informacion' , axis = 1 , inplace = True ) terremotos_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a\u00f1o pais magnitud 0 2000 turkey 6.0 1 2000 turkmenistan 7.0 2 2000 azerbaijan 6.5 3 2000 azerbaijan 6.8 4 2000 papua new guinea 8.0 5. \u00bfSe puede sacar m\u00e1s provecho a los datos ? Una vez realizado toda la limpieza de datos, debemos filtrar la informaci\u00f3n que se considere importante. A\u00f1os : A\u00f1os desde el 2000 al 2011. Pa\u00eds : Paises con nombre distinto de sin_nombre Magnitud : Magnitud entre 5 y 9.6. # aplicar filtros terremotos_data_filtrado = terremotos_data [ mask_anio & mask_pais & mask_mag ] terremotos_data_filtrado . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a\u00f1o pais magnitud 0 2000 turkey 6.0 1 2000 turkmenistan 7.0 2 2000 azerbaijan 6.5 3 2000 azerbaijan 6.8 4 2000 papua new guinea 8.0 Veamos cu\u00e1nta informaci\u00f3n se perdio: print ( 'Cantidad de filas dataset sin filtro:' , len ( terremotos_data )) print ( 'Cantidad de filas dataset con filtro:' , len ( terremotos_data_filtrado )) Cantidad de filas dataset sin filtro: 230 Cantidad de filas dataset con filtro: 212 Ahora veamos el el resumen de la informaci\u00f3n para el nuevo dataset: frames = [] for col in terremotos_data_filtrado . columns : aux_df = resumen_por_columna ( terremotos_data_filtrado , col ) frames . append ( aux_df ) df_info = pd . concat ( frames ) . reset_index ( drop = True ) df_info .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } columna unicos vacios 0 a\u00f1o 12 0 1 pais 50 0 2 magnitud 39 0 Finalmente, podemos responder la pregunta del inicio:","title":"Columna: Informaci\u00f3n"},{"location":"lectures/data_manipulation/eda/eda/#solucion","text":"# formato wide terremotos_data_filtrado . pivot_table ( index = \"pais\" , columns = \"a\u00f1o\" , values = \"magnitud\" , fill_value = '' , aggfunc = pd . np . max ) <ipython-input-31-8c1a5edb19d8>:6: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead aggfunc=pd.np.max) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a\u00f1o 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 pais afghanistan 6.3 5.0 7.3 5.8 6.5 6.5 algeria 5.7 6.8 5.2 5.5 argentina 7.2 6.1 azerbaijan 6.8 bangladesh 5.6 burma 6.8 chile 6.3 8.0 7.7 8.8 china 5.9 5.6 5.5 6.3 5.3 5.2 5.0 6.1 7.9 5.7 6.9 5.4 colombia 6.5 5.9 costa rica 6.4 6.1 democratic republic of the congo 6.2 5.9 dominican republic 6.4 ecuador 5.5 el salvador 7.6 greece 6.2 6.4 guadeloupe 6.3 guatemala 6.4 haiti 7.0 india 7.6 6.5 5.1 5.3 5.1 6.9 indonesia 7.9 7.5 6.9 9.1 8.6 7.7 8.5 7.3 7.6 iran 5.3 6.5 6.6 6.3 6.4 6.1 italy 5.9 6.2 japan 6.1 6.8 8.3 6.6 6.6 6.7 6.9 6.4 9.0 kazakhstan 6.0 kyrgyzstan 6.9 martinique 7.4 mexico 7.5 morocco 6.3 mozambique 7.0 new zealand 5.4 6.6 6.3 nicaragua 5.4 pakistan 6.3 5.4 7.6 5.2 6.4 panama 6.5 papua new guinea 8.0 7.6 6.1 peru 8.4 7.5 8.0 philippines 7.5 6.5 7.1 5.3 russian federation 7.3 6.2 rwanda 5.3 samoa 8.1 serbia 5.7 slovenia 5.2 solomon islands 8.1 taiwan 6.4 7.1 5.2 7.0 tajikistan 5.2 5.6 5.2 tanzania 6.4 5.5 6.8 turkey 6.0 6.5 6.3 5.6 5.9 6.1 7.1 turkmenistan 7.0 united states 6.8 6.6 venezuela 5.5 vietnam 5.3 \u00bf Podemos sacar m\u00e1s informaci\u00f3n ?. Por supuesto que se puede, no obstante, siempre se debe ser preciso con la informaci\u00f3n que se Conclusi\u00f3n del caso El an\u00e1lisis exploratorio de datos (EDA) es una metodolog\u00eda que sirve para asegurarse de la calidad de los datos. A medida que se tiene m\u00e1s expertice en el tema, mejor es el an\u00e1lisis de datos y por tanto, mejor son los resultados obtenidos. No existe un procedimiento est\u00e1ndar para realizar el EDA, pero siempre se debe tener una claridad mental con: Problema que se quiere resolverlo C\u00f3mo resolver el problema Posibles problemas de la muestra (datos perdidos, ouliers, etc.)","title":"Soluci\u00f3n"},{"location":"lectures/data_manipulation/eda/eda/#referencia","text":"Detailed exploratory data analysis with python Exploratory Data Analysis (EDA) and Data Visualization with Python","title":"Referencia"},{"location":"lectures/data_manipulation/scientific_computing/numpy/","text":"Numpy NumPy es el paquete fundamental para la computaci\u00f3n cient\u00edfica con Python. Contiene entre otras cosas: un poderoso objeto de matriz N-dimensional funciones sofisticadas (de transmisi\u00f3n) herramientas para integrar c\u00f3digo C / C ++ y Fortran \u00c1lgebra lineal \u00fatil, transformada de Fourier y capacidades de n\u00fameros aleatorios Adem\u00e1s de sus obvios usos cient\u00edficos, NumPy tambi\u00e9n se puede utilizar como un eficiente contenedor multidimensional de datos gen\u00e9ricos. Se pueden definir tipos de datos arbitrarios. Esto permite que NumPy se integre sin problemas y r\u00e1pidamente con una amplia variedad de bases de datos. NumPy tiene licencia bajo la licencia BSD, lo que permite su reutilizaci\u00f3n con pocas restricciones. \u00bf Por qu\u00e9 usar Numpy ? Las razones por las que deber\u00eda usar NumPy en lugar de cualquier otro objeto _iterable en Python son: NumPy proporciona una estructura de ndarray para almacenar datos num\u00e9ricos de manera contigua. Tambi\u00e9n implementa operaciones matem\u00e1ticas r\u00e1pidas en ndarrays, que explotan esta contig\u00fcidad. Brevedad de la sintaxis para las operaciones de matriz. Un lenguaje como C o Java requerir\u00eda que escribamos un bucle para una operaci\u00f3n matricial tan simple como C = A + B. Operaciones b\u00e1sicas de NumPy Las razones por las que deber\u00eda usar NumPy en lugar de cualquier otro objeto _iterable en Python son: NumPy proporciona una estructura de ndarray para almacenar datos num\u00e9ricos de manera contigua. Tambi\u00e9n implementa operaciones matem\u00e1ticas r\u00e1pidas en ndarrays, que explotan esta contig\u00fcidad. Brevedad de la sintaxis para las operaciones de matriz. Un lenguaje como C o Java requerir\u00eda que escribamos un bucle para una operaci\u00f3n matricial tan simple como C = A + B. # importar libreria: numpy import numpy as np import time import sys # Arreglo de ceros: np.zeros(shape) print ( \"Zeros:\" ) print ( np . zeros (( 3 , 3 )) ) Zeros: [[0. 0. 0.] [0. 0. 0.] [0. 0. 0.]] # Arreglos de uno: np.ones(shape) print ( \" \\n Ones:\" ) print ( np . ones (( 3 , 3 )) ) Ones: [[1. 1. 1.] [1. 1. 1.] [1. 1. 1.]] # Arreglo vacio: np.empty(shape) print ( \" \\n Empty:\" ) print ( np . empty ([ 2 , 2 ]) ) Empty: [[5.e-324 4.e-323] [4.e-323 4.e-323]] # Rango de valores: np.range(start, stop, step) print ( \" \\n Range:\" ) np . arange ( 0. , 10. , 1. ) Range: array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]) # Grilla de valores: np.linspace(start, end, n_values) print ( \" \\n Regular grid:\" ) print ( np . linspace ( 0. , 1. , 9 ) ) Regular grid: [0. 0.125 0.25 0.375 0.5 0.625 0.75 0.875 1. ] # fijar semilla np . random . seed ( 42 ) # Sequencia aleatoria: np.random print ( \" \\n Random sequences:\" ) print ( np . random . uniform ( 10 , size = 6 ) ) Random sequences: [6.62913893 1.44357124 3.41205452 4.61207364 8.59583224 8.59604932] # Construccion de arreglos: np.array( python_iterable ) print ( \" \\n Array constructor\" ) print ( np . array ([ 2 , 3 , 5 , 10 , - 1 ]) ) Array constructor [ 2 3 5 10 -1] Manipulaci\u00f3n de datos En esta secci\u00f3n se presentan operaciones b\u00e1sicas de los arreglos de numpy. # matrix matriz_34 = np . array ([ [ 1 , 2 , 3 , 4 ], [ 5 , 6 , 7 , 8 ], [ 9 , 10 , 11 , 12 ] ]) matriz_34 array([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12]]) ### tipo type ( matriz_34 ) numpy.ndarray ### dimensiones size = matriz_34 . shape print ( f \"rows: { size [ 0 ] } \" ) print ( f \"columns: { size [ 1 ] } \" ) rows: 3 columns: 4 ### Escoger filas y columnas print ( f \"first row: \\n { matriz_34 [ 0 ] } \\n \" ) print ( f \"first col: \\n { matriz_34 [:, 0 ] } \\n \" ) print ( f \"first and second rows: \\n { matriz_34 [: 2 ] } \\n \" ) print ( f \"first and second cols: \\n { matriz_34 [:,: 2 ] } \\n \" ) first row: [1 2 3 4] first col: [1 5 9] first and second rows: [[1 2 3 4] [5 6 7 8]] first and second cols: [[ 1 2] [ 5 6] [ 9 10]] A\u00f1adir filas y columnas Para a\u00f1adir una columna, se debe asegurar que se este agregando un arreglo de tama\u00f1o: \\((1,n)\\) A1 = np . array ([[ 0 , 0 , 0 , 0 ]]) # [[]] A1 . shape (1, 4) # add row add_row = np . r_ [ matriz_34 , A1 ] add_row array([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12], [ 0, 0, 0, 0]]) Para a\u00f1adir una columna, se debe asegurar que se este agregando un arreglo de tama\u00f1o \\((n,)\\) A2 = np . array ([ 0 , 0 , 0 ]) # [] A2 . shape (3,) # add column add_column = np . c_ [ matriz_34 , A2 ] add_column array([[ 1, 2, 3, 4, 0], [ 5, 6, 7, 8, 0], [ 9, 10, 11, 12, 0]]) Observaci\u00f3n : * Se debe tener cuidado al momento de operar vectores con dimensi\u00f3n \\((1,n)\\) o \\((n,)\\) . * Para a\u00f1adir filas o columnas a una matriz o juntar arreglos de numpy, se puede utilizar la funci\u00f3n np.concatenate() . Operaciones matem\u00e1ticas b\u00e1sicas de matrices La mayor\u00eda de las operaciones realizadas en NumPy se manejan por elementos, es decir, calcular C = A + B se traducir\u00e1 en $ C [i, j] = A [i, j] + B [i, j] $. (La excepci\u00f3n es la transmisi\u00f3n y se explicar\u00e1 pronto). A continuaci\u00f3n hay una lista con las operaciones matem\u00e1ticas m\u00e1s comunes. # fijar semilla np . random . seed ( 42 ) # crear dos arreglos A = np . array ([ [ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ] ]) B = np . array ([[ 9 , 8 , 7 ],[ 6 , 5 , 4 ],[ 3 , 2 , 1 ]]) print ( f \"Matrix A: \\n { A } \\n \" ) print ( f \"Matrix B: \\n { B } \" ) Matrix A: [[1 2 3] [4 5 6] [7 8 9]] Matrix B: [[9 8 7] [6 5 4] [3 2 1]] # suma print ( \"Sum:\" ) print ( A + B ) Sum: [[10 10 10] [10 10 10] [10 10 10]] # resta print ( \" \\n Subtraction\" ) print ( A - B ) Subtraction [[-8 -6 -4] [-2 0 2] [ 4 6 8]] # producto uno a uno print ( \" \\n Product\" ) print ( A * B ) Product [[ 9 16 21] [24 25 24] [21 16 9]] # producto matricial print ( \" \\n Matricial Product\" ) print ( np . dot ( A , B ) ) Matricial Product [[ 30 24 18] [ 84 69 54] [138 114 90]] # potencia print ( \" \\n Power\" ) print ( A ** 2 ) Power [[ 1 4 9] [16 25 36] [49 64 81]] # algunas funciones comunes print ( \" \\n np.exp()\" ) print ( np . exp ( A ) ) print ( \" \\n np.sin()\" ) print ( np . sin ( A ) ) print ( \" \\n np.cos()\" ) print ( np . cos ( A )) print ( \" \\n np.tan()\" ) print ( np . tan ( A ) ) np.exp() [[2.71828183e+00 7.38905610e+00 2.00855369e+01] [5.45981500e+01 1.48413159e+02 4.03428793e+02] [1.09663316e+03 2.98095799e+03 8.10308393e+03]] np.sin() [[ 0.84147098 0.90929743 0.14112001] [-0.7568025 -0.95892427 -0.2794155 ] [ 0.6569866 0.98935825 0.41211849]] np.cos() [[ 0.54030231 -0.41614684 -0.9899925 ] [-0.65364362 0.28366219 0.96017029] [ 0.75390225 -0.14550003 -0.91113026]] np.tan() [[ 1.55740772 -2.18503986 -0.14254654] [ 1.15782128 -3.38051501 -0.29100619] [ 0.87144798 -6.79971146 -0.45231566]] \u00c1lgebra Lineal En esta secci\u00f3n se presentan algunas propiedades comunmente ocupadas en \u00e1lgebra lineal de matrices. # crear un arreglo A = np . array ([[ 1 , 2 ], [ 3 , 4 ]]) # transpuesta print ( \"Transpose: \" ) print ( A . T ) Transpose: [[1 3] [2 4]] # determinante print ( \"determinant\" ) print ( round ( np . linalg . det ( A ) , 2 )) determinant -2.0 # Inversa print ( \"Inverse\" ) print ( np . linalg . inv ( A ) ) Inverse [[-2. 1. ] [ 1.5 -0.5]] # traza print ( \"Trace\" ) print ( np . trace ( A )) Trace 5 # sistemas lineales: Ax = b b = np . array ([[ 5. ], [ 7. ]]) print ( \"linear system: Ax=b\" ) print ( \" \\n x = \" ) print ( np . linalg . solve ( A , b ) ) linear system: Ax=b x = [[-3.] [ 4.]] # Valores y vectores propios eigenvalues , eigenvectors = np . linalg . eig ( A ) print ( \"eigenvalues\" ) print ( eigenvalues ) print ( \" \\n eigenvectors\" ) print ( eigenvectors ) eigenvalues [-0.37228132 5.37228132] eigenvectors [[-0.82456484 -0.41597356] [ 0.56576746 -0.90937671]] # descomposicion QR Q , R = np . linalg . qr ( A ) print ( \"QR decomposition:\" ) print ( \" \\n Q\" ) print ( Q ) print ( \" \\n R\" ) print ( R ) QR decomposition: Q [[-0.31622777 -0.9486833 ] [-0.9486833 0.31622777]] R [[-3.16227766 -4.42718872] [ 0. -0.63245553]] Broadcasting Unas de las ventajas de numpy es que podemos hacer broadcasting , es to significa que numpy permite realizar operaciones binarias con arreglos de distintos tama\u00f1os. # example 01 a = np . arange ( 3 ) + 5 print ( f \"np.arange(3)+ 5: \\n { a } \" ) np.arange(3)+ 5: [5 6 7] # example 02 b = np . ones (( 3 , 3 )) + np . arange ( 3 ) print ( f \"np.ones((3,3))+np.arange(3): \\n { b } \" ) np.ones((3,3))+np.arange(3): [[1. 2. 3.] [1. 2. 3.] [1. 2. 3.]] # example 03 c = np . arange ( 3 ) . reshape (( 3 , 1 )) + np . arange ( 3 ) print ( f \"np.arange(3).reshape((3,1)) + np.arange(3): \\n { c } \" ) np.arange(3).reshape((3,1)) + np.arange(3): [[0 1 2] [1 2 3] [2 3 4]] Python Lists vs. Numpy Arrays La librer\u00eda principal de Python son las listas. Una lista es el equivalente a Python de una matriz, pero es redimensionable y puede contener elementos de diferentes tipos. Una pregunta com\u00fan para principiantes es cu\u00e1l es la verdadera diferencia aqu\u00ed. La respuesta es el rendimiento . Las estructuras de datos de Numpy funcionan mejor en: Tama\u00f1o : las estructuras de datos de Numpy ocupan menos espacio Rendimiento : necesitan velocidad y son m\u00e1s r\u00e1pidos que las listas Funcionalidad : SciPy y NumPy tienen funciones optimizadas, como las operaciones de \u00e1lgebra lineal integradas. Memoria Los principales beneficios del uso de matrices NumPy deber\u00edan ser un menor consumo de memoria y un mejor comportamiento en tiempo de ejecuci\u00f3n. Para las listas de Python: podemos concluir de esto que para cada elemento nuevo, necesitamos otros ocho bytes para la referencia al nuevo objeto. El nuevo objeto entero en s\u00ed consume 28 bytes. El tama\u00f1o de una lista lst sin el tama\u00f1o de los elementos se puede calcular con: 64 + 8 * len (lst) + + len (lst) * 28 NumPy ocupa menos espacio. Esto significa que una matriz entera arbitraria de longitud n en necesidades numpy se calcula por: 96 + n * 8 bytes Para convensernos de esto, ejecutemos un ejemplo: # example # class: array class Array : \"\"\" Clase array que da como rsultado el tiempo y espacio en memoria de los objetos list y numpy array \"\"\" def __init__ ( self , size_of_vec ): self . size_of_vec = size_of_vec def pure_python_version ( self ): \"\"\" Tiempo y espacio en memoria para objeto list \"\"\" t1 = time . time () X = range ( self . size_of_vec ) Y = range ( self . size_of_vec ) Z = [ X [ i ] + Y [ i ] for i in range ( len ( X )) ] return ( time . time () - t1 , sys . getsizeof ( Z ) ) def numpy_version ( self ): \"\"\" Tiempo y espacio en memoria para objeto numpy array \"\"\" t1 = time . time () X = np . arange ( self . size_of_vec ) Y = np . arange ( self . size_of_vec ) Z = X + Y return ( time . time () - t1 , sys . getsizeof ( Z ) ) # parameters size_of_vec = 1000000 class_array = Array ( size_of_vec ) t1 , size1 = class_array . pure_python_version () t2 , size2 = class_array . numpy_version () # results print ( f \"python list -- time: { round ( t1 , 8 ) } seg, size: { size1 } bytes\" ) print ( f \"numpy array -- time: { round ( t2 , 8 ) } seg, size: { size2 } bytes\" ) python list -- time: 0.49169469 seg, size: 8697464 bytes numpy array -- time: 0.00699592 seg, size: 4000096 bytes Referencia Quickstart tutorial-numpy Mathematical functions","title":"Numpy"},{"location":"lectures/data_manipulation/scientific_computing/numpy/#numpy","text":"NumPy es el paquete fundamental para la computaci\u00f3n cient\u00edfica con Python. Contiene entre otras cosas: un poderoso objeto de matriz N-dimensional funciones sofisticadas (de transmisi\u00f3n) herramientas para integrar c\u00f3digo C / C ++ y Fortran \u00c1lgebra lineal \u00fatil, transformada de Fourier y capacidades de n\u00fameros aleatorios Adem\u00e1s de sus obvios usos cient\u00edficos, NumPy tambi\u00e9n se puede utilizar como un eficiente contenedor multidimensional de datos gen\u00e9ricos. Se pueden definir tipos de datos arbitrarios. Esto permite que NumPy se integre sin problemas y r\u00e1pidamente con una amplia variedad de bases de datos. NumPy tiene licencia bajo la licencia BSD, lo que permite su reutilizaci\u00f3n con pocas restricciones.","title":"Numpy"},{"location":"lectures/data_manipulation/scientific_computing/numpy/#por-que-usar-numpy","text":"Las razones por las que deber\u00eda usar NumPy en lugar de cualquier otro objeto _iterable en Python son: NumPy proporciona una estructura de ndarray para almacenar datos num\u00e9ricos de manera contigua. Tambi\u00e9n implementa operaciones matem\u00e1ticas r\u00e1pidas en ndarrays, que explotan esta contig\u00fcidad. Brevedad de la sintaxis para las operaciones de matriz. Un lenguaje como C o Java requerir\u00eda que escribamos un bucle para una operaci\u00f3n matricial tan simple como C = A + B.","title":"\u00bf Por qu\u00e9 usar Numpy ?"},{"location":"lectures/data_manipulation/scientific_computing/numpy/#operaciones-basicas-de-numpy","text":"Las razones por las que deber\u00eda usar NumPy en lugar de cualquier otro objeto _iterable en Python son: NumPy proporciona una estructura de ndarray para almacenar datos num\u00e9ricos de manera contigua. Tambi\u00e9n implementa operaciones matem\u00e1ticas r\u00e1pidas en ndarrays, que explotan esta contig\u00fcidad. Brevedad de la sintaxis para las operaciones de matriz. Un lenguaje como C o Java requerir\u00eda que escribamos un bucle para una operaci\u00f3n matricial tan simple como C = A + B. # importar libreria: numpy import numpy as np import time import sys # Arreglo de ceros: np.zeros(shape) print ( \"Zeros:\" ) print ( np . zeros (( 3 , 3 )) ) Zeros: [[0. 0. 0.] [0. 0. 0.] [0. 0. 0.]] # Arreglos de uno: np.ones(shape) print ( \" \\n Ones:\" ) print ( np . ones (( 3 , 3 )) ) Ones: [[1. 1. 1.] [1. 1. 1.] [1. 1. 1.]] # Arreglo vacio: np.empty(shape) print ( \" \\n Empty:\" ) print ( np . empty ([ 2 , 2 ]) ) Empty: [[5.e-324 4.e-323] [4.e-323 4.e-323]] # Rango de valores: np.range(start, stop, step) print ( \" \\n Range:\" ) np . arange ( 0. , 10. , 1. ) Range: array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]) # Grilla de valores: np.linspace(start, end, n_values) print ( \" \\n Regular grid:\" ) print ( np . linspace ( 0. , 1. , 9 ) ) Regular grid: [0. 0.125 0.25 0.375 0.5 0.625 0.75 0.875 1. ] # fijar semilla np . random . seed ( 42 ) # Sequencia aleatoria: np.random print ( \" \\n Random sequences:\" ) print ( np . random . uniform ( 10 , size = 6 ) ) Random sequences: [6.62913893 1.44357124 3.41205452 4.61207364 8.59583224 8.59604932] # Construccion de arreglos: np.array( python_iterable ) print ( \" \\n Array constructor\" ) print ( np . array ([ 2 , 3 , 5 , 10 , - 1 ]) ) Array constructor [ 2 3 5 10 -1]","title":"Operaciones b\u00e1sicas de NumPy"},{"location":"lectures/data_manipulation/scientific_computing/numpy/#manipulacion-de-datos","text":"En esta secci\u00f3n se presentan operaciones b\u00e1sicas de los arreglos de numpy. # matrix matriz_34 = np . array ([ [ 1 , 2 , 3 , 4 ], [ 5 , 6 , 7 , 8 ], [ 9 , 10 , 11 , 12 ] ]) matriz_34 array([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12]]) ### tipo type ( matriz_34 ) numpy.ndarray ### dimensiones size = matriz_34 . shape print ( f \"rows: { size [ 0 ] } \" ) print ( f \"columns: { size [ 1 ] } \" ) rows: 3 columns: 4 ### Escoger filas y columnas print ( f \"first row: \\n { matriz_34 [ 0 ] } \\n \" ) print ( f \"first col: \\n { matriz_34 [:, 0 ] } \\n \" ) print ( f \"first and second rows: \\n { matriz_34 [: 2 ] } \\n \" ) print ( f \"first and second cols: \\n { matriz_34 [:,: 2 ] } \\n \" ) first row: [1 2 3 4] first col: [1 5 9] first and second rows: [[1 2 3 4] [5 6 7 8]] first and second cols: [[ 1 2] [ 5 6] [ 9 10]] A\u00f1adir filas y columnas Para a\u00f1adir una columna, se debe asegurar que se este agregando un arreglo de tama\u00f1o: \\((1,n)\\) A1 = np . array ([[ 0 , 0 , 0 , 0 ]]) # [[]] A1 . shape (1, 4) # add row add_row = np . r_ [ matriz_34 , A1 ] add_row array([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12], [ 0, 0, 0, 0]]) Para a\u00f1adir una columna, se debe asegurar que se este agregando un arreglo de tama\u00f1o \\((n,)\\) A2 = np . array ([ 0 , 0 , 0 ]) # [] A2 . shape (3,) # add column add_column = np . c_ [ matriz_34 , A2 ] add_column array([[ 1, 2, 3, 4, 0], [ 5, 6, 7, 8, 0], [ 9, 10, 11, 12, 0]]) Observaci\u00f3n : * Se debe tener cuidado al momento de operar vectores con dimensi\u00f3n \\((1,n)\\) o \\((n,)\\) . * Para a\u00f1adir filas o columnas a una matriz o juntar arreglos de numpy, se puede utilizar la funci\u00f3n np.concatenate() .","title":"Manipulaci\u00f3n de datos"},{"location":"lectures/data_manipulation/scientific_computing/numpy/#operaciones-matematicas-basicas-de-matrices","text":"La mayor\u00eda de las operaciones realizadas en NumPy se manejan por elementos, es decir, calcular C = A + B se traducir\u00e1 en $ C [i, j] = A [i, j] + B [i, j] $. (La excepci\u00f3n es la transmisi\u00f3n y se explicar\u00e1 pronto). A continuaci\u00f3n hay una lista con las operaciones matem\u00e1ticas m\u00e1s comunes. # fijar semilla np . random . seed ( 42 ) # crear dos arreglos A = np . array ([ [ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ] ]) B = np . array ([[ 9 , 8 , 7 ],[ 6 , 5 , 4 ],[ 3 , 2 , 1 ]]) print ( f \"Matrix A: \\n { A } \\n \" ) print ( f \"Matrix B: \\n { B } \" ) Matrix A: [[1 2 3] [4 5 6] [7 8 9]] Matrix B: [[9 8 7] [6 5 4] [3 2 1]] # suma print ( \"Sum:\" ) print ( A + B ) Sum: [[10 10 10] [10 10 10] [10 10 10]] # resta print ( \" \\n Subtraction\" ) print ( A - B ) Subtraction [[-8 -6 -4] [-2 0 2] [ 4 6 8]] # producto uno a uno print ( \" \\n Product\" ) print ( A * B ) Product [[ 9 16 21] [24 25 24] [21 16 9]] # producto matricial print ( \" \\n Matricial Product\" ) print ( np . dot ( A , B ) ) Matricial Product [[ 30 24 18] [ 84 69 54] [138 114 90]] # potencia print ( \" \\n Power\" ) print ( A ** 2 ) Power [[ 1 4 9] [16 25 36] [49 64 81]] # algunas funciones comunes print ( \" \\n np.exp()\" ) print ( np . exp ( A ) ) print ( \" \\n np.sin()\" ) print ( np . sin ( A ) ) print ( \" \\n np.cos()\" ) print ( np . cos ( A )) print ( \" \\n np.tan()\" ) print ( np . tan ( A ) ) np.exp() [[2.71828183e+00 7.38905610e+00 2.00855369e+01] [5.45981500e+01 1.48413159e+02 4.03428793e+02] [1.09663316e+03 2.98095799e+03 8.10308393e+03]] np.sin() [[ 0.84147098 0.90929743 0.14112001] [-0.7568025 -0.95892427 -0.2794155 ] [ 0.6569866 0.98935825 0.41211849]] np.cos() [[ 0.54030231 -0.41614684 -0.9899925 ] [-0.65364362 0.28366219 0.96017029] [ 0.75390225 -0.14550003 -0.91113026]] np.tan() [[ 1.55740772 -2.18503986 -0.14254654] [ 1.15782128 -3.38051501 -0.29100619] [ 0.87144798 -6.79971146 -0.45231566]]","title":"Operaciones matem\u00e1ticas b\u00e1sicas de matrices"},{"location":"lectures/data_manipulation/scientific_computing/numpy/#algebra-lineal","text":"En esta secci\u00f3n se presentan algunas propiedades comunmente ocupadas en \u00e1lgebra lineal de matrices. # crear un arreglo A = np . array ([[ 1 , 2 ], [ 3 , 4 ]]) # transpuesta print ( \"Transpose: \" ) print ( A . T ) Transpose: [[1 3] [2 4]] # determinante print ( \"determinant\" ) print ( round ( np . linalg . det ( A ) , 2 )) determinant -2.0 # Inversa print ( \"Inverse\" ) print ( np . linalg . inv ( A ) ) Inverse [[-2. 1. ] [ 1.5 -0.5]] # traza print ( \"Trace\" ) print ( np . trace ( A )) Trace 5 # sistemas lineales: Ax = b b = np . array ([[ 5. ], [ 7. ]]) print ( \"linear system: Ax=b\" ) print ( \" \\n x = \" ) print ( np . linalg . solve ( A , b ) ) linear system: Ax=b x = [[-3.] [ 4.]] # Valores y vectores propios eigenvalues , eigenvectors = np . linalg . eig ( A ) print ( \"eigenvalues\" ) print ( eigenvalues ) print ( \" \\n eigenvectors\" ) print ( eigenvectors ) eigenvalues [-0.37228132 5.37228132] eigenvectors [[-0.82456484 -0.41597356] [ 0.56576746 -0.90937671]] # descomposicion QR Q , R = np . linalg . qr ( A ) print ( \"QR decomposition:\" ) print ( \" \\n Q\" ) print ( Q ) print ( \" \\n R\" ) print ( R ) QR decomposition: Q [[-0.31622777 -0.9486833 ] [-0.9486833 0.31622777]] R [[-3.16227766 -4.42718872] [ 0. -0.63245553]]","title":"\u00c1lgebra Lineal"},{"location":"lectures/data_manipulation/scientific_computing/numpy/#broadcasting","text":"Unas de las ventajas de numpy es que podemos hacer broadcasting , es to significa que numpy permite realizar operaciones binarias con arreglos de distintos tama\u00f1os. # example 01 a = np . arange ( 3 ) + 5 print ( f \"np.arange(3)+ 5: \\n { a } \" ) np.arange(3)+ 5: [5 6 7] # example 02 b = np . ones (( 3 , 3 )) + np . arange ( 3 ) print ( f \"np.ones((3,3))+np.arange(3): \\n { b } \" ) np.ones((3,3))+np.arange(3): [[1. 2. 3.] [1. 2. 3.] [1. 2. 3.]] # example 03 c = np . arange ( 3 ) . reshape (( 3 , 1 )) + np . arange ( 3 ) print ( f \"np.arange(3).reshape((3,1)) + np.arange(3): \\n { c } \" ) np.arange(3).reshape((3,1)) + np.arange(3): [[0 1 2] [1 2 3] [2 3 4]]","title":"Broadcasting"},{"location":"lectures/data_manipulation/scientific_computing/numpy/#python-lists-vs-numpy-arrays","text":"La librer\u00eda principal de Python son las listas. Una lista es el equivalente a Python de una matriz, pero es redimensionable y puede contener elementos de diferentes tipos. Una pregunta com\u00fan para principiantes es cu\u00e1l es la verdadera diferencia aqu\u00ed. La respuesta es el rendimiento . Las estructuras de datos de Numpy funcionan mejor en: Tama\u00f1o : las estructuras de datos de Numpy ocupan menos espacio Rendimiento : necesitan velocidad y son m\u00e1s r\u00e1pidos que las listas Funcionalidad : SciPy y NumPy tienen funciones optimizadas, como las operaciones de \u00e1lgebra lineal integradas.","title":"Python Lists vs. Numpy Arrays"},{"location":"lectures/data_manipulation/scientific_computing/numpy/#memoria","text":"Los principales beneficios del uso de matrices NumPy deber\u00edan ser un menor consumo de memoria y un mejor comportamiento en tiempo de ejecuci\u00f3n. Para las listas de Python: podemos concluir de esto que para cada elemento nuevo, necesitamos otros ocho bytes para la referencia al nuevo objeto. El nuevo objeto entero en s\u00ed consume 28 bytes. El tama\u00f1o de una lista lst sin el tama\u00f1o de los elementos se puede calcular con: 64 + 8 * len (lst) + + len (lst) * 28 NumPy ocupa menos espacio. Esto significa que una matriz entera arbitraria de longitud n en necesidades numpy se calcula por: 96 + n * 8 bytes Para convensernos de esto, ejecutemos un ejemplo: # example # class: array class Array : \"\"\" Clase array que da como rsultado el tiempo y espacio en memoria de los objetos list y numpy array \"\"\" def __init__ ( self , size_of_vec ): self . size_of_vec = size_of_vec def pure_python_version ( self ): \"\"\" Tiempo y espacio en memoria para objeto list \"\"\" t1 = time . time () X = range ( self . size_of_vec ) Y = range ( self . size_of_vec ) Z = [ X [ i ] + Y [ i ] for i in range ( len ( X )) ] return ( time . time () - t1 , sys . getsizeof ( Z ) ) def numpy_version ( self ): \"\"\" Tiempo y espacio en memoria para objeto numpy array \"\"\" t1 = time . time () X = np . arange ( self . size_of_vec ) Y = np . arange ( self . size_of_vec ) Z = X + Y return ( time . time () - t1 , sys . getsizeof ( Z ) ) # parameters size_of_vec = 1000000 class_array = Array ( size_of_vec ) t1 , size1 = class_array . pure_python_version () t2 , size2 = class_array . numpy_version () # results print ( f \"python list -- time: { round ( t1 , 8 ) } seg, size: { size1 } bytes\" ) print ( f \"numpy array -- time: { round ( t2 , 8 ) } seg, size: { size2 } bytes\" ) python list -- time: 0.49169469 seg, size: 8697464 bytes numpy array -- time: 0.00699592 seg, size: 4000096 bytes","title":"Memoria"},{"location":"lectures/data_manipulation/scientific_computing/numpy/#referencia","text":"Quickstart tutorial-numpy Mathematical functions","title":"Referencia"},{"location":"lectures/data_manipulation/scientific_computing/scipy/","text":"SciPy Introducci\u00f3n SciPy se basa en el marco NumPy de bajo nivel para matrices multidimensionales y proporciona una gran cantidad de algoritmos cient\u00edficos de alto nivel. Algunos de los temas que cubre SciPy son: Special functions ( scipy.special ) Integration ( scipy.integrate ) Optimization ( scipy.optimize ) Interpolation ( scipy.interpolate ) Fourier Transforms ( scipy.fftpack ) Signal Processing ( scipy.signal ) Sparse Eigenvalue Problems ( scipy.sparse ) Statistics ( scipy.stats ) Cada uno de estos subm\u00f3dulos proporciona una serie de funciones y clases que se pueden utilizar para resolver problemas en sus respectivos temas. En esta lecci\u00f3n veremos c\u00f3mo usar algunos de estos subpaquetes. Para acceder al paquete SciPy en un programa Python, comenzamos importando todo desde el m\u00f3dulo scipy . from scipy import * Si solo necesitamos usar parte del marco SciPy, podemos incluir selectivamente solo aquellos m\u00f3dulos que nos interesan. Por ejemplo, para incluir el paquete de \u00e1lgebra lineal bajo el nombre la , podemos hacer: import scipy.linalg as la Special functions Un gran n\u00famero de funciones matem\u00e1ticas especiales son importantes para muchos problemas de f\u00edsica computacional. SciPy proporciona implementaciones de un conjunto muy extenso de funciones especiales. Para obtener m\u00e1s detalles, consulte la lista de funciones en el documento de referencia en http://docs.scipy.org/doc/scipy/reference/special.html#module-scipy.special. Para demostrar el uso t\u00edpico de funciones especiales, veremos con m\u00e1s detalle las funciones de Bessel: # # The scipy.special module includes a large number of Bessel-functions # Here we will use the functions jn and yn, which are the Bessel functions # of the first and second kind and real-valued order. We also include the # function jn_zeros and yn_zeros that gives the zeroes of the functions jn # and yn. # from scipy.special import jn , yn , jn_zeros , yn_zeros import numpy as np import matplotlib.pyplot as plt n = 0 # order x = 0.0 # Bessel function of first kind print ( f \"J_ { n } ( { x } ) = { jn ( n , x ) } \" ) J_0(0.0) = 1.0 x = 1.0 # Bessel function of second kind print ( f \"Y_ { n } ( { x } ) = { yn ( n , x ) } \" ) Y_0(1.0) = 0.08825696421567697 x = np . linspace ( 0 , 10 , 100 ) fig , ax = plt . subplots () for n in range ( 4 ): ax . plot ( x , jn ( n , x ), label = r \"$J_ %d (x)$\" % n ) ax . legend (); plt . show () # zeros of Bessel functions n = 0 # order m = 4 # number of roots to compute jn_zeros ( n , m ) array([ 2.40482556, 5.52007811, 8.65372791, 11.79153444]) Integration Numerical integration: quadrature Evaluaci\u00f3n num\u00e9rica de una funci\u00f3n del tipo \\(\\displaystyle \\int_a^b f(x) dx\\) se llama cuadratura num\u00e9rica , o simplemente cuadratura . SciPy proporciona una serie de funciones para diferentes tipos de cuadratura, por ejemplo, quad , dblquad y tplquad para integrales simples, dobles y triples, respectivamente. from scipy.integrate import quad , dblquad , tplquad La funci\u00f3n quad toma una gran cantidad de argumentos opcionales, que se pueden usar para ajustar el comportamiento de la funci\u00f3n (prueba con help (quad) para obtener m\u00e1s detalles). El uso b\u00e1sico es el siguiente: # define a simple function for the integrand def f ( x ): return x x_lower = 0 # the lower limit of x x_upper = 1 # the upper limit of x val , abserr = quad ( f , x_lower , x_upper ) print ( f \"integral value = { val } absolute error = { abserr } \" ) integral value = 0.5 absolute error = 5.551115123125783e-15 Si necesitamos pasar argumentos adicionales a la funci\u00f3n integrando, podemos usar el argumento de palabra clave args : def integrand ( x , n ): \"\"\" Bessel function of first kind and order n. \"\"\" return jn ( n , x ) x_lower = 0 # the lower limit of x x_upper = 10 # the upper limit of x val , abserr = quad ( integrand , x_lower , x_upper , args = ( 3 ,)) print ( val , abserr ) 0.7366751370811073 9.389126882496403e-13 Para funciones simples, podemos usar una funci\u00f3n lambda (funci\u00f3n sin nombre) en lugar de definir expl\u00edcitamente una funci\u00f3n para el integrando: val , abserr = quad ( lambda x : np . exp ( - x ** 2 ), - Inf , Inf ) print ( f \"numerical = { val } { abserr } \" ) numerical = 1.7724538509055159 1.4202636780944923e-08 analytical = np . lib . scimath . sqrt ( pi ) print ( f \"analytical = { analytical } \" ) analytical = 1.7724538509055159 Como se muestra en el ejemplo anterior, tambi\u00e9n podemos usar Inf o -Inf como l\u00edmites integrales. La integraci\u00f3n de dimensiones superiores funciona de la misma manera: def integrand ( x , y ): return np . exp ( - x ** 2 - y ** 2 ) x_lower = 0 x_upper = 10 y_lower = 0 y_upper = 10 val , abserr = dblquad ( integrand , x_lower , x_upper , lambda x : y_lower , lambda x : y_upper ) print ( val , abserr ) 0.7853981633974476 1.3753098510218528e-08 Observe c\u00f3mo tuvimos que pasar funciones lambda para los l\u00edmites de la integraci\u00f3n \\(y\\) , ya que estas en general pueden ser funciones de \\(x\\) . Ordinary differential equations (ODEs) SciPy proporciona dos formas diferentes de resolver EDO: una API basada en la funci\u00f3n odeint y una API orientada a objetos basada en la clase ode . Por lo general, odeint es m\u00e1s f\u00e1cil de empezar, pero la clase ode ofrece un nivel de control m\u00e1s fino. Aqu\u00ed usaremos las funciones odeint . Para obtener m\u00e1s informaci\u00f3n sobre la clase ode , pruebe con help (ode) . Hace pr\u00e1cticamente lo mismo que odeint , pero de una manera orientada a objetos. Para usar odeint , primero imp\u00f3rtelo desde el m\u00f3dulo scipy.integrate from scipy.integrate import odeint , ode Un sistema de EDO se suele formular en forma est\u00e1ndar antes de ser atacado num\u00e9ricamente. La forma est\u00e1ndar es: \\(y' = f(y, t)\\) , donde: \\(y = [y_1(t), y_2(t), ..., y_n(t)]\\) y $ f $ es alguna funci\u00f3n que da las derivadas de la funci\u00f3n $ y_i (t) $. Para resolver una EDO necesitamos conocer la funci\u00f3n $ f $ y una condici\u00f3n inicial, $ y (0) $. Tenga en cuenta que las EDO de orden superior siempre se pueden escribir de esta forma introduciendo nuevas variables para las derivadas intermedias. Una vez que hemos definido la funci\u00f3n de Python f y la matriz y_0 (que es $ f $ y $ y (0) $ en la formulaci\u00f3n matem\u00e1tica), podemos usar la funci\u00f3n odeint como: y_t = odeint ( f , y_0 , t ) donde t es una matriz con coordenadas de tiempo para resolver el problema de ODE. y_t es una matriz con una fila para cada punto en el tiempo en t , donde cada columna corresponde a una soluci\u00f3n y_i (t) en ese momento. Veremos c\u00f3mo podemos implementar f e y_0 en c\u00f3digo Python en los ejemplos siguientes. Ejemplo: p\u00e9ndulo doble Consideremos un ejemplo f\u00edsico: el p\u00e9ndulo compuesto doble, descrito con cierto detalle aqu\u00ed: http://en.wikipedia.org/wiki/Double_pendulum Las ecuaciones de movimiento del p\u00e9ndulo se dan en la p\u00e1gina wiki: \\({\\dot \\theta_1} = \\frac{6}{m\\ell^2} \\frac{ 2 p_{\\theta_1} - 3 \\cos(\\theta_1-\\theta_2) p_{\\theta_2}}{16 - 9 \\cos^2(\\theta_1-\\theta_2)}\\) \\({\\dot \\theta_2} = \\frac{6}{m\\ell^2} \\frac{ 8 p_{\\theta_2} - 3 \\cos(\\theta_1-\\theta_2) p_{\\theta_1}}{16 - 9 \\cos^2(\\theta_1-\\theta_2)}.\\) \\({\\dot p_{\\theta_1}} = -\\frac{1}{2} m \\ell^2 \\left [ {\\dot \\theta_1} {\\dot \\theta_2} \\sin (\\theta_1-\\theta_2) + 3 \\frac{g}{\\ell} \\sin \\theta_1 \\right ]\\) \\({\\dot p_{\\theta_2}} = -\\frac{1}{2} m \\ell^2 \\left [ -{\\dot \\theta_1} {\\dot \\theta_2} \\sin (\\theta_1-\\theta_2) + \\frac{g}{\\ell} \\sin \\theta_2 \\right]\\) Para simplificar el seguimiento del c\u00f3digo Python, introduzcamos nuevos nombres de variables y la notaci\u00f3n vectorial: \\(x = [\\theta_1, \\theta_2, p_{\\theta_1}, p_{\\theta_2}]\\) \\({\\dot x_1} = \\frac{6}{m\\ell^2} \\frac{ 2 x_3 - 3 \\cos(x_1-x_2) x_4}{16 - 9 \\cos^2(x_1-x_2)}\\) \\({\\dot x_2} = \\frac{6}{m\\ell^2} \\frac{ 8 x_4 - 3 \\cos(x_1-x_2) x_3}{16 - 9 \\cos^2(x_1-x_2)}\\) \\({\\dot x_3} = -\\frac{1}{2} m \\ell^2 \\left [ {\\dot x_1} {\\dot x_2} \\sin (x_1-x_2) + 3 \\frac{g}{\\ell} \\sin x_1 \\right ]\\) \\({\\dot x_4} = -\\frac{1}{2} m \\ell^2 \\left [ -{\\dot x_1} {\\dot x_2} \\sin (x_1-x_2) + \\frac{g}{\\ell} \\sin x_2 \\right]\\) g = 9.82 L = 0.5 m = 0.1 def dx ( x , t ): \"\"\" The right-hand side of the pendulum ODE \"\"\" x1 , x2 , x3 , x4 = x [ 0 ], x [ 1 ], x [ 2 ], x [ 3 ] dx1 = 6.0 / ( m * L ** 2 ) * ( 2 * x3 - 3 * np . cos ( x1 - x2 ) * x4 ) / ( 16 - 9 * np . cos ( x1 - x2 ) ** 2 ) dx2 = 6.0 / ( m * L ** 2 ) * ( 8 * x4 - 3 * np . cos ( x1 - x2 ) * x3 ) / ( 16 - 9 * np . cos ( x1 - x2 ) ** 2 ) dx3 = - 0.5 * m * L ** 2 * ( dx1 * dx2 * np . sin ( x1 - x2 ) + 3 * ( g / L ) * np . sin ( x1 )) dx4 = - 0.5 * m * L ** 2 * ( - dx1 * dx2 * np . sin ( x1 - x2 ) + ( g / L ) * np . sin ( x2 )) return [ dx1 , dx2 , dx3 , dx4 ] # choose an initial state x0 = [ pi / 4 , pi / 2 , 0 , 0 ] # time coodinate to solve the ODE for: from 0 to 10 seconds t = np . linspace ( 0 , 10 , 250 ) # solve the ODE problem x = odeint ( dx , x0 , t ) # plot the angles as a function of time fig , axes = plt . subplots ( 1 , 2 , figsize = ( 12 , 4 )) axes [ 0 ] . plot ( t , x [:, 0 ], 'r' , label = \"theta1\" ) axes [ 0 ] . plot ( t , x [:, 1 ], 'b' , label = \"theta2\" ) x1 = + L * np . sin ( x [:, 0 ]) y1 = - L * np . cos ( x [:, 0 ]) x2 = x1 + L * np . sin ( x [:, 1 ]) y2 = y1 - L * np . cos ( x [:, 1 ]) axes [ 1 ] . plot ( x1 , y1 , 'r' , label = \"pendulum1\" ) axes [ 1 ] . plot ( x2 , y2 , 'b' , label = \"pendulum2\" ) axes [ 1 ] . set_ylim ([ - 1 , 0 ]) axes [ 1 ] . set_xlim ([ 1 , - 1 ]); Ejemplo: oscilador arm\u00f3nico amortiguado Los problemas de ODE son importantes en f\u00edsica computacional, por lo que veremos un ejemplo m\u00e1s: la oscilaci\u00f3n arm\u00f3nica amortiguada. Este problema est\u00e1 bien descrito en la p\u00e1gina wiki: http://en.wikipedia.org/wiki/Damping La ecuaci\u00f3n de movimiento del oscilador amortiguado es: \\(\\displaystyle \\frac{\\mathrm{d}^2x}{\\mathrm{d}t^2} + 2\\zeta\\omega_0\\frac{\\mathrm{d}x}{\\mathrm{d}t} + \\omega^2_0 x = 0\\) donde $ x $ es la posici\u00f3n del oscilador, $ \\ omega_0 $ es la frecuencia y $ \\ zeta $ es la relaci\u00f3n de amortiguaci\u00f3n. Para escribir esta EDO de segundo orden en forma est\u00e1ndar, introducimos $ p = \\ frac {\\ mathrm {d} x} {\\ mathrm {d} t} $: \\(\\displaystyle \\frac{\\mathrm{d}p}{\\mathrm{d}t} = - 2\\zeta\\omega_0 p - \\omega^2_0 x\\) \\(\\displaystyle \\frac{\\mathrm{d}x}{\\mathrm{d}t} = p\\) En la implementaci\u00f3n de este ejemplo, agregaremos argumentos adicionales a la funci\u00f3n RHS para la ODE, en lugar de usar variables globales como hicimos en el ejemplo anterior. Como consecuencia de los argumentos adicionales al RHS, necesitamos pasar un argumento de palabra clave args a la funci\u00f3n odeint : def dy ( y , t , zeta , w0 ): \"\"\" The right-hand side of the damped oscillator ODE \"\"\" x , p = y [ 0 ], y [ 1 ] dx = p dp = - 2 * zeta * w0 * p - w0 ** 2 * x return [ dx , dp ] # initial state: y0 = [ 1.0 , 0.0 ] # time coodinate to solve the ODE for t = np . linspace ( 0 , 10 , 1000 ) w0 = 2 * pi * 1.0 # solve the ODE problem for three different values of the damping ratio y1 = odeint ( dy , y0 , t , args = ( 0.0 , w0 )) # undamped y2 = odeint ( dy , y0 , t , args = ( 0.2 , w0 )) # under damped y3 = odeint ( dy , y0 , t , args = ( 1.0 , w0 )) # critial damping y4 = odeint ( dy , y0 , t , args = ( 5.0 , w0 )) # over damped fig , ax = plt . subplots () ax . plot ( t , y1 [:, 0 ], 'k' , label = \"undamped\" , linewidth = 0.25 ) ax . plot ( t , y2 [:, 0 ], 'r' , label = \"under damped\" ) ax . plot ( t , y3 [:, 0 ], 'b' , label = r \"critical damping\" ) ax . plot ( t , y4 [:, 0 ], 'g' , label = \"over damped\" ) ax . legend (); Fourier transform Las transformadas de Fourier son una de las herramientas universales de la f\u00edsica computacional, que aparecen una y otra vez en diferentes contextos. SciPy proporciona funciones para acceder a la biblioteca cl\u00e1sica [FFTPACK] (http://www.netlib.org/fftpack/) de NetLib, que es una biblioteca FFT eficiente y bien probada escrita en FORTRAN. La API SciPy tiene algunas funciones de conveniencia adicionales, pero en general, la API est\u00e1 estrechamente relacionada con la biblioteca FORTRAN original. Para usar el m\u00f3dulo fftpack en un programa Python, incl\u00fayalo usando: from numpy.fft import fftfreq from scipy.fftpack import * Para demostrar c\u00f3mo hacer una transformada r\u00e1pida de Fourier con SciPy, veamos la FFT de la soluci\u00f3n al oscilador amortiguado de la secci\u00f3n anterior: N = len ( t ) dt = t [ 1 ] - t [ 0 ] # calculate the fast fourier transform # y2 is the solution to the under-damped oscillator from the previous section F = fft ( y2 [:, 0 ]) # calculate the frequencies for the components in F w = fftfreq ( N , dt ) fig , ax = plt . subplots ( figsize = ( 9 , 3 )) ax . plot ( w , abs ( F )); Dado que la se\u00f1al es real, el espectro es sim\u00e9trico. Por lo tanto, solo necesitamos trazar la parte que corresponde a las frecuencias positivas. Para extraer esa parte de w y F podemos usar algunos de los trucos de indexaci\u00f3n para matrices NumPy que vimos en la lecci\u00f3n 2: indices = np . where ( w > 0 ) # select only indices for elements that corresponds to positive frequencies w_pos = w [ indices ] F_pos = F [ indices ] fig , ax = plt . subplots ( figsize = ( 9 , 3 )) ax . plot ( w_pos , abs ( F_pos )) ax . set_xlim ( 0 , 5 ); Como era de esperar, ahora vemos un pico en el espectro que se centra alrededor de 1, que es la frecuencia que usamos en el ejemplo del oscilador amortiguado. Sparse matrices Las matrices dispersas suelen ser \u00fatiles en simulaciones num\u00e9ricas que tratan con sistemas grandes, si el problema se puede describir en forma de matriz donde las matrices o vectores contienen en su mayor\u00eda ceros. Scipy tiene un buen soporte para matrices dispersas, con operaciones b\u00e1sicas de \u00e1lgebra lineal (como resoluci\u00f3n de ecuaciones, c\u00e1lculos de valores propios, etc.). Hay muchas estrategias posibles para almacenar matrices dispersas de manera eficiente. Algunos de los m\u00e1s comunes son el llamado formulario de coordenadas (COO), formulario de lista de lista (LIL) y CSC de columna comprimida y dispersa (y fila, CSR). Cada formato tiene algunas ventajas y desventajas. La mayor\u00eda de los algoritmos computacionales (resoluci\u00f3n de ecuaciones, multiplicaci\u00f3n de matriz-matriz, etc.) se pueden implementar de manera eficiente usando formatos CSR o CSC, pero no son tan intuitivos ni tan f\u00e1ciles de inicializar. Muy a menudo, una matriz dispersa se crea inicialmente en formato COO o LIL (donde podemos agregar elementos de manera eficiente a los datos de la matriz dispersa), y luego se convierte a CSC o CSR antes de usarse en c\u00e1lculos reales. Para obtener m\u00e1s informaci\u00f3n sobre estos formatos dispersos, consulte p. Ej. http://en.wikipedia.org/wiki/Sparse_matrix Cuando creamos una matriz dispersa, tenemos que elegir en qu\u00e9 formato se debe almacenar. Por ejemplo, from scipy.sparse import * # dense matrix M = np . array ([[ 1 , 0 , 0 , 0 ], [ 0 , 3 , 0 , 0 ], [ 0 , 1 , 1 , 0 ], [ 1 , 0 , 0 , 1 ]]); M array([[1, 0, 0, 0], [0, 3, 0, 0], [0, 1, 1, 0], [1, 0, 0, 1]]) # convert from dense to sparse A = csr_matrix ( M ); A <4x4 sparse matrix of type '<class 'numpy.int64'>' with 6 stored elements in Compressed Sparse Row format> # convert from sparse to dense A . todense () matrix([[1, 0, 0, 0], [0, 3, 0, 0], [0, 1, 1, 0], [1, 0, 0, 1]]) Una forma m\u00e1s eficiente de crear matrices dispersas: cree una matriz vac\u00eda y complete con el uso de indexaci\u00f3n matricial (evita crear una matriz densa potencialmente grande) A = lil_matrix (( 4 , 4 )) # empty 4x4 sparse matrix A [ 0 , 0 ] = 1 A [ 1 , 1 ] = 3 A [ 2 , 2 ] = A [ 2 , 1 ] = 1 A [ 3 , 3 ] = A [ 3 , 0 ] = 1 A <4x4 sparse matrix of type '<class 'numpy.float64'>' with 6 stored elements in List of Lists format> A . todense () matrix([[1., 0., 0., 0.], [0., 3., 0., 0.], [0., 1., 1., 0.], [1., 0., 0., 1.]]) Conversi\u00f3n entre diferentes formatos de matriz dispersa: A <4x4 sparse matrix of type '<class 'numpy.float64'>' with 6 stored elements in List of Lists format> A = csr_matrix ( A ); A <4x4 sparse matrix of type '<class 'numpy.float64'>' with 6 stored elements in Compressed Sparse Row format> A = csc_matrix ( A ); A <4x4 sparse matrix of type '<class 'numpy.float64'>' with 6 stored elements in Compressed Sparse Column format> Podemos calcular con matrices dispersas como con matrices densas: A . todense () matrix([[1., 0., 0., 0.], [0., 3., 0., 0.], [0., 1., 1., 0.], [1., 0., 0., 1.]]) ( A * A ) . todense () matrix([[1., 0., 0., 0.], [0., 9., 0., 0.], [0., 4., 1., 0.], [2., 0., 0., 1.]]) A . todense () matrix([[1., 0., 0., 0.], [0., 3., 0., 0.], [0., 1., 1., 0.], [1., 0., 0., 1.]]) A . dot ( A ) . todense () matrix([[1., 0., 0., 0.], [0., 9., 0., 0.], [0., 4., 1., 0.], [2., 0., 0., 1.]]) v = np . array ([ 1 , 2 , 3 , 4 ])[:, newaxis ]; v array([[1], [2], [3], [4]]) # sparse matrix - dense vector multiplication A * v array([[1.], [6.], [5.], [5.]]) # same result with dense matrix - dense vector multiplcation A . todense () * v matrix([[1.], [6.], [5.], [5.]]) Optimization La optimizaci\u00f3n (encontrar m\u00ednimos o m\u00e1ximos de una funci\u00f3n) es un campo amplio en matem\u00e1ticas, y la optimizaci\u00f3n de funciones complicadas o en muchas variables puede estar bastante involucrada. Aqu\u00ed solo veremos algunos casos muy simples. Para obtener una introducci\u00f3n m\u00e1s detallada a la optimizaci\u00f3n con SciPy, consulte: http://scipy-lectures.github.com/advanced/mathematical_optimization/index.html Para usar el m\u00f3dulo de optimizaci\u00f3n en scipy, primero incluya el m\u00f3dulo optimize : from scipy import optimize Encontrar un m\u00ednimo Primero veamos c\u00f3mo encontrar los m\u00ednimos de una funci\u00f3n simple de una sola variable: def f ( x ): return 4 * x ** 3 + ( x - 2 ) ** 2 + x ** 4 fig , ax = plt . subplots () x = np . linspace ( - 5 , 3 , 100 ) ax . plot ( x , f ( x )); Podemos usar la funci\u00f3n fmin_bfgs para encontrar los m\u00ednimos de una funci\u00f3n: x_min = optimize . fmin_bfgs ( f , - 2 ) x_min Optimization terminated successfully. Current function value: -3.506641 Iterations: 5 Function evaluations: 16 Gradient evaluations: 8 array([-2.67298155]) optimize . fmin_bfgs ( f , 0.5 ) Optimization terminated successfully. Current function value: 2.804988 Iterations: 3 Function evaluations: 10 Gradient evaluations: 5 array([0.46961745]) Tambi\u00e9n podemos usar las funciones brent o fminbound . Tienen una sintaxis un poco diferente y usan diferentes algoritmos. optimize . brent ( f ) 0.46961743402759754 optimize . fminbound ( f , - 4 , 2 ) -2.6729822917513886 Encontrar una soluci\u00f3n a una funci\u00f3n Para encontrar la ra\u00edz de una funci\u00f3n de la forma $ f (x) = 0 $ podemos usar la funci\u00f3n fsolve . Requiere una suposici\u00f3n inicial: omega_c = 3.0 def f ( omega ): # a transcendental equation: resonance frequencies of a low-Q SQUID terminated microwave resonator return np . tan ( 2 * pi * omega ) - omega_c / omega fig , ax = plt . subplots ( figsize = ( 10 , 4 )) x = np . linspace ( 0 , 3 , 1000 ) y = f ( x ) mask = np . where ( abs ( y ) > 50 ) x [ mask ] = y [ mask ] = NaN # get rid of vertical line when the function flip sign ax . plot ( x , y ) ax . plot ([ 0 , 3 ], [ 0 , 0 ], 'k' ) ax . set_ylim ( - 5 , 5 ); <ipython-input-54-64c57d5fd427>:4: RuntimeWarning: divide by zero encountered in true_divide return np.tan(2*pi*omega) - omega_c/omega optimize . fsolve ( f , 0.1 ) array([0.23743014]) optimize . fsolve ( f , 0.6 ) array([0.71286972]) optimize . fsolve ( f , 1.1 ) array([1.18990285]) Interpolation La interpolaci\u00f3n es simple y conveniente en scipy: la funci\u00f3n interp1d , cuando se dan matrices que describen datos \\(X\\) e \\(Y\\) , devuelve un objeto que se comporta como una funci\u00f3n que se puede llamar para un valor arbitrario de \\(x\\) (en el rango cubierto por \\(X\\) ), y devuelve el valor de \\(y\\) interpolado correspondiente: from scipy.interpolate import * def f ( x ): return np . sin ( x ) n = np . arange ( 0 , 10 ) x = np . linspace ( 0 , 9 , 100 ) y_meas = f ( n ) + 0.1 * np . random . randn ( len ( n )) # simulate measurement with noise y_real = f ( x ) linear_interpolation = interp1d ( n , y_meas ) y_interp1 = linear_interpolation ( x ) cubic_interpolation = interp1d ( n , y_meas , kind = 'cubic' ) y_interp2 = cubic_interpolation ( x ) fig , ax = plt . subplots ( figsize = ( 10 , 4 )) ax . plot ( n , y_meas , 'bs' , label = 'noisy data' ) ax . plot ( x , y_real , 'k' , lw = 2 , label = 'true function' ) ax . plot ( x , y_interp1 , 'r' , label = 'linear interp' ) ax . plot ( x , y_interp2 , 'g' , label = 'cubic interp' ) ax . legend ( loc = 3 ); Statistics El m\u00f3dulo scipy.stats contiene una gran cantidad de distribuciones estad\u00edsticas, funciones estad\u00edsticas y pruebas. Para obtener una documentaci\u00f3n completa de sus funciones, consulte http://docs.scipy.org/doc/scipy/reference/stats.html. Tambi\u00e9n hay un paquete de Python muy poderoso para modelado estad\u00edstico llamado statsmodels. Consulte http://statsmodels.sourceforge.net para obtener m\u00e1s detalles. from scipy import stats # create a (discreet) random variable with poissionian distribution X = stats . poisson ( 3.5 ) # photon distribution for a coherent state with n=3.5 photons n = np . arange ( 0 , 15 ) fig , axes = plt . subplots ( 3 , 1 , sharex = True ) # plot the probability mass function (PMF) axes [ 0 ] . step ( n , X . pmf ( n )) # plot the commulative distribution function (CDF) axes [ 1 ] . step ( n , X . cdf ( n )) # plot histogram of 1000 random realizations of the stochastic variable X axes [ 2 ] . hist ( X . rvs ( size = 1000 )); # create a (continous) random variable with normal distribution Y = stats . norm () x = np . linspace ( - 5 , 5 , 100 ) fig , axes = plt . subplots ( 3 , 1 , sharex = True ) # plot the probability distribution function (PDF) axes [ 0 ] . plot ( x , Y . pdf ( x )) # plot the commulative distributin function (CDF) axes [ 1 ] . plot ( x , Y . cdf ( x )); # plot histogram of 1000 random realizations of the stochastic variable Y axes [ 2 ] . hist ( Y . rvs ( size = 1000 ), bins = 50 ); Estad\u00edsticas X . mean (), X . std (), X . var () # poission distribution (3.5, 1.8708286933869707, 3.5) Y . mean (), Y . std (), Y . var () # normal distribution (0.0, 1.0, 1.0) Statistical tests Pruebe si dos conjuntos de datos aleatorios (independientes) provienen de la misma distribuci\u00f3n: t_statistic , p_value = stats . ttest_ind ( X . rvs ( size = 1000 ), X . rvs ( size = 1000 )) print ( f \"t-statistic = { t_statistic } \" ) print ( f \"p-value = { p_value } \" ) t-statistic = -0.3175173118125565 p-value = 0.7508842832858349 Dado que el valor \\(p\\) es muy grande, no podemos rechazar la hip\u00f3tesis de que los dos conjuntos de datos aleatorios tienen medias * diferentes *. Para probar si la media de una sola muestra de datos tiene una media de 0,1 (la media verdadera es 0,0): stats . ttest_1samp ( Y . rvs ( size = 1000 ), 0.1 ) Ttest_1sampResult(statistic=-3.3260245451111463, pvalue=0.0009130194599003766) Un valor \\(p\\) bajo significa que podemos rechazar la hip\u00f3tesis de que la media de \\(Y\\) es 0,1. Y . mean () 0.0 stats . ttest_1samp ( Y . rvs ( size = 1000 ), Y . mean ()) Ttest_1sampResult(statistic=0.7254565446907949, pvalue=0.468341895612603) Referencias A tutorial on how to get started using SciPy The SciPy source code","title":"Scipy"},{"location":"lectures/data_manipulation/scientific_computing/scipy/#scipy","text":"","title":"SciPy"},{"location":"lectures/data_manipulation/scientific_computing/scipy/#introduccion","text":"SciPy se basa en el marco NumPy de bajo nivel para matrices multidimensionales y proporciona una gran cantidad de algoritmos cient\u00edficos de alto nivel. Algunos de los temas que cubre SciPy son: Special functions ( scipy.special ) Integration ( scipy.integrate ) Optimization ( scipy.optimize ) Interpolation ( scipy.interpolate ) Fourier Transforms ( scipy.fftpack ) Signal Processing ( scipy.signal ) Sparse Eigenvalue Problems ( scipy.sparse ) Statistics ( scipy.stats ) Cada uno de estos subm\u00f3dulos proporciona una serie de funciones y clases que se pueden utilizar para resolver problemas en sus respectivos temas. En esta lecci\u00f3n veremos c\u00f3mo usar algunos de estos subpaquetes. Para acceder al paquete SciPy en un programa Python, comenzamos importando todo desde el m\u00f3dulo scipy . from scipy import * Si solo necesitamos usar parte del marco SciPy, podemos incluir selectivamente solo aquellos m\u00f3dulos que nos interesan. Por ejemplo, para incluir el paquete de \u00e1lgebra lineal bajo el nombre la , podemos hacer: import scipy.linalg as la","title":"Introducci\u00f3n"},{"location":"lectures/data_manipulation/scientific_computing/scipy/#special-functions","text":"Un gran n\u00famero de funciones matem\u00e1ticas especiales son importantes para muchos problemas de f\u00edsica computacional. SciPy proporciona implementaciones de un conjunto muy extenso de funciones especiales. Para obtener m\u00e1s detalles, consulte la lista de funciones en el documento de referencia en http://docs.scipy.org/doc/scipy/reference/special.html#module-scipy.special. Para demostrar el uso t\u00edpico de funciones especiales, veremos con m\u00e1s detalle las funciones de Bessel: # # The scipy.special module includes a large number of Bessel-functions # Here we will use the functions jn and yn, which are the Bessel functions # of the first and second kind and real-valued order. We also include the # function jn_zeros and yn_zeros that gives the zeroes of the functions jn # and yn. # from scipy.special import jn , yn , jn_zeros , yn_zeros import numpy as np import matplotlib.pyplot as plt n = 0 # order x = 0.0 # Bessel function of first kind print ( f \"J_ { n } ( { x } ) = { jn ( n , x ) } \" ) J_0(0.0) = 1.0 x = 1.0 # Bessel function of second kind print ( f \"Y_ { n } ( { x } ) = { yn ( n , x ) } \" ) Y_0(1.0) = 0.08825696421567697 x = np . linspace ( 0 , 10 , 100 ) fig , ax = plt . subplots () for n in range ( 4 ): ax . plot ( x , jn ( n , x ), label = r \"$J_ %d (x)$\" % n ) ax . legend (); plt . show () # zeros of Bessel functions n = 0 # order m = 4 # number of roots to compute jn_zeros ( n , m ) array([ 2.40482556, 5.52007811, 8.65372791, 11.79153444])","title":"Special functions"},{"location":"lectures/data_manipulation/scientific_computing/scipy/#integration","text":"","title":"Integration"},{"location":"lectures/data_manipulation/scientific_computing/scipy/#numerical-integration-quadrature","text":"Evaluaci\u00f3n num\u00e9rica de una funci\u00f3n del tipo \\(\\displaystyle \\int_a^b f(x) dx\\) se llama cuadratura num\u00e9rica , o simplemente cuadratura . SciPy proporciona una serie de funciones para diferentes tipos de cuadratura, por ejemplo, quad , dblquad y tplquad para integrales simples, dobles y triples, respectivamente. from scipy.integrate import quad , dblquad , tplquad La funci\u00f3n quad toma una gran cantidad de argumentos opcionales, que se pueden usar para ajustar el comportamiento de la funci\u00f3n (prueba con help (quad) para obtener m\u00e1s detalles). El uso b\u00e1sico es el siguiente: # define a simple function for the integrand def f ( x ): return x x_lower = 0 # the lower limit of x x_upper = 1 # the upper limit of x val , abserr = quad ( f , x_lower , x_upper ) print ( f \"integral value = { val } absolute error = { abserr } \" ) integral value = 0.5 absolute error = 5.551115123125783e-15 Si necesitamos pasar argumentos adicionales a la funci\u00f3n integrando, podemos usar el argumento de palabra clave args : def integrand ( x , n ): \"\"\" Bessel function of first kind and order n. \"\"\" return jn ( n , x ) x_lower = 0 # the lower limit of x x_upper = 10 # the upper limit of x val , abserr = quad ( integrand , x_lower , x_upper , args = ( 3 ,)) print ( val , abserr ) 0.7366751370811073 9.389126882496403e-13 Para funciones simples, podemos usar una funci\u00f3n lambda (funci\u00f3n sin nombre) en lugar de definir expl\u00edcitamente una funci\u00f3n para el integrando: val , abserr = quad ( lambda x : np . exp ( - x ** 2 ), - Inf , Inf ) print ( f \"numerical = { val } { abserr } \" ) numerical = 1.7724538509055159 1.4202636780944923e-08 analytical = np . lib . scimath . sqrt ( pi ) print ( f \"analytical = { analytical } \" ) analytical = 1.7724538509055159 Como se muestra en el ejemplo anterior, tambi\u00e9n podemos usar Inf o -Inf como l\u00edmites integrales. La integraci\u00f3n de dimensiones superiores funciona de la misma manera: def integrand ( x , y ): return np . exp ( - x ** 2 - y ** 2 ) x_lower = 0 x_upper = 10 y_lower = 0 y_upper = 10 val , abserr = dblquad ( integrand , x_lower , x_upper , lambda x : y_lower , lambda x : y_upper ) print ( val , abserr ) 0.7853981633974476 1.3753098510218528e-08 Observe c\u00f3mo tuvimos que pasar funciones lambda para los l\u00edmites de la integraci\u00f3n \\(y\\) , ya que estas en general pueden ser funciones de \\(x\\) .","title":"Numerical integration: quadrature"},{"location":"lectures/data_manipulation/scientific_computing/scipy/#ordinary-differential-equations-odes","text":"SciPy proporciona dos formas diferentes de resolver EDO: una API basada en la funci\u00f3n odeint y una API orientada a objetos basada en la clase ode . Por lo general, odeint es m\u00e1s f\u00e1cil de empezar, pero la clase ode ofrece un nivel de control m\u00e1s fino. Aqu\u00ed usaremos las funciones odeint . Para obtener m\u00e1s informaci\u00f3n sobre la clase ode , pruebe con help (ode) . Hace pr\u00e1cticamente lo mismo que odeint , pero de una manera orientada a objetos. Para usar odeint , primero imp\u00f3rtelo desde el m\u00f3dulo scipy.integrate from scipy.integrate import odeint , ode Un sistema de EDO se suele formular en forma est\u00e1ndar antes de ser atacado num\u00e9ricamente. La forma est\u00e1ndar es: \\(y' = f(y, t)\\) , donde: \\(y = [y_1(t), y_2(t), ..., y_n(t)]\\) y $ f $ es alguna funci\u00f3n que da las derivadas de la funci\u00f3n $ y_i (t) $. Para resolver una EDO necesitamos conocer la funci\u00f3n $ f $ y una condici\u00f3n inicial, $ y (0) $. Tenga en cuenta que las EDO de orden superior siempre se pueden escribir de esta forma introduciendo nuevas variables para las derivadas intermedias. Una vez que hemos definido la funci\u00f3n de Python f y la matriz y_0 (que es $ f $ y $ y (0) $ en la formulaci\u00f3n matem\u00e1tica), podemos usar la funci\u00f3n odeint como: y_t = odeint ( f , y_0 , t ) donde t es una matriz con coordenadas de tiempo para resolver el problema de ODE. y_t es una matriz con una fila para cada punto en el tiempo en t , donde cada columna corresponde a una soluci\u00f3n y_i (t) en ese momento. Veremos c\u00f3mo podemos implementar f e y_0 en c\u00f3digo Python en los ejemplos siguientes.","title":"Ordinary differential equations (ODEs)"},{"location":"lectures/data_manipulation/scientific_computing/scipy/#ejemplo-pendulo-doble","text":"Consideremos un ejemplo f\u00edsico: el p\u00e9ndulo compuesto doble, descrito con cierto detalle aqu\u00ed: http://en.wikipedia.org/wiki/Double_pendulum Las ecuaciones de movimiento del p\u00e9ndulo se dan en la p\u00e1gina wiki: \\({\\dot \\theta_1} = \\frac{6}{m\\ell^2} \\frac{ 2 p_{\\theta_1} - 3 \\cos(\\theta_1-\\theta_2) p_{\\theta_2}}{16 - 9 \\cos^2(\\theta_1-\\theta_2)}\\) \\({\\dot \\theta_2} = \\frac{6}{m\\ell^2} \\frac{ 8 p_{\\theta_2} - 3 \\cos(\\theta_1-\\theta_2) p_{\\theta_1}}{16 - 9 \\cos^2(\\theta_1-\\theta_2)}.\\) \\({\\dot p_{\\theta_1}} = -\\frac{1}{2} m \\ell^2 \\left [ {\\dot \\theta_1} {\\dot \\theta_2} \\sin (\\theta_1-\\theta_2) + 3 \\frac{g}{\\ell} \\sin \\theta_1 \\right ]\\) \\({\\dot p_{\\theta_2}} = -\\frac{1}{2} m \\ell^2 \\left [ -{\\dot \\theta_1} {\\dot \\theta_2} \\sin (\\theta_1-\\theta_2) + \\frac{g}{\\ell} \\sin \\theta_2 \\right]\\) Para simplificar el seguimiento del c\u00f3digo Python, introduzcamos nuevos nombres de variables y la notaci\u00f3n vectorial: \\(x = [\\theta_1, \\theta_2, p_{\\theta_1}, p_{\\theta_2}]\\) \\({\\dot x_1} = \\frac{6}{m\\ell^2} \\frac{ 2 x_3 - 3 \\cos(x_1-x_2) x_4}{16 - 9 \\cos^2(x_1-x_2)}\\) \\({\\dot x_2} = \\frac{6}{m\\ell^2} \\frac{ 8 x_4 - 3 \\cos(x_1-x_2) x_3}{16 - 9 \\cos^2(x_1-x_2)}\\) \\({\\dot x_3} = -\\frac{1}{2} m \\ell^2 \\left [ {\\dot x_1} {\\dot x_2} \\sin (x_1-x_2) + 3 \\frac{g}{\\ell} \\sin x_1 \\right ]\\) \\({\\dot x_4} = -\\frac{1}{2} m \\ell^2 \\left [ -{\\dot x_1} {\\dot x_2} \\sin (x_1-x_2) + \\frac{g}{\\ell} \\sin x_2 \\right]\\) g = 9.82 L = 0.5 m = 0.1 def dx ( x , t ): \"\"\" The right-hand side of the pendulum ODE \"\"\" x1 , x2 , x3 , x4 = x [ 0 ], x [ 1 ], x [ 2 ], x [ 3 ] dx1 = 6.0 / ( m * L ** 2 ) * ( 2 * x3 - 3 * np . cos ( x1 - x2 ) * x4 ) / ( 16 - 9 * np . cos ( x1 - x2 ) ** 2 ) dx2 = 6.0 / ( m * L ** 2 ) * ( 8 * x4 - 3 * np . cos ( x1 - x2 ) * x3 ) / ( 16 - 9 * np . cos ( x1 - x2 ) ** 2 ) dx3 = - 0.5 * m * L ** 2 * ( dx1 * dx2 * np . sin ( x1 - x2 ) + 3 * ( g / L ) * np . sin ( x1 )) dx4 = - 0.5 * m * L ** 2 * ( - dx1 * dx2 * np . sin ( x1 - x2 ) + ( g / L ) * np . sin ( x2 )) return [ dx1 , dx2 , dx3 , dx4 ] # choose an initial state x0 = [ pi / 4 , pi / 2 , 0 , 0 ] # time coodinate to solve the ODE for: from 0 to 10 seconds t = np . linspace ( 0 , 10 , 250 ) # solve the ODE problem x = odeint ( dx , x0 , t ) # plot the angles as a function of time fig , axes = plt . subplots ( 1 , 2 , figsize = ( 12 , 4 )) axes [ 0 ] . plot ( t , x [:, 0 ], 'r' , label = \"theta1\" ) axes [ 0 ] . plot ( t , x [:, 1 ], 'b' , label = \"theta2\" ) x1 = + L * np . sin ( x [:, 0 ]) y1 = - L * np . cos ( x [:, 0 ]) x2 = x1 + L * np . sin ( x [:, 1 ]) y2 = y1 - L * np . cos ( x [:, 1 ]) axes [ 1 ] . plot ( x1 , y1 , 'r' , label = \"pendulum1\" ) axes [ 1 ] . plot ( x2 , y2 , 'b' , label = \"pendulum2\" ) axes [ 1 ] . set_ylim ([ - 1 , 0 ]) axes [ 1 ] . set_xlim ([ 1 , - 1 ]);","title":"Ejemplo: p\u00e9ndulo doble"},{"location":"lectures/data_manipulation/scientific_computing/scipy/#ejemplo-oscilador-armonico-amortiguado","text":"Los problemas de ODE son importantes en f\u00edsica computacional, por lo que veremos un ejemplo m\u00e1s: la oscilaci\u00f3n arm\u00f3nica amortiguada. Este problema est\u00e1 bien descrito en la p\u00e1gina wiki: http://en.wikipedia.org/wiki/Damping La ecuaci\u00f3n de movimiento del oscilador amortiguado es: \\(\\displaystyle \\frac{\\mathrm{d}^2x}{\\mathrm{d}t^2} + 2\\zeta\\omega_0\\frac{\\mathrm{d}x}{\\mathrm{d}t} + \\omega^2_0 x = 0\\) donde $ x $ es la posici\u00f3n del oscilador, $ \\ omega_0 $ es la frecuencia y $ \\ zeta $ es la relaci\u00f3n de amortiguaci\u00f3n. Para escribir esta EDO de segundo orden en forma est\u00e1ndar, introducimos $ p = \\ frac {\\ mathrm {d} x} {\\ mathrm {d} t} $: \\(\\displaystyle \\frac{\\mathrm{d}p}{\\mathrm{d}t} = - 2\\zeta\\omega_0 p - \\omega^2_0 x\\) \\(\\displaystyle \\frac{\\mathrm{d}x}{\\mathrm{d}t} = p\\) En la implementaci\u00f3n de este ejemplo, agregaremos argumentos adicionales a la funci\u00f3n RHS para la ODE, en lugar de usar variables globales como hicimos en el ejemplo anterior. Como consecuencia de los argumentos adicionales al RHS, necesitamos pasar un argumento de palabra clave args a la funci\u00f3n odeint : def dy ( y , t , zeta , w0 ): \"\"\" The right-hand side of the damped oscillator ODE \"\"\" x , p = y [ 0 ], y [ 1 ] dx = p dp = - 2 * zeta * w0 * p - w0 ** 2 * x return [ dx , dp ] # initial state: y0 = [ 1.0 , 0.0 ] # time coodinate to solve the ODE for t = np . linspace ( 0 , 10 , 1000 ) w0 = 2 * pi * 1.0 # solve the ODE problem for three different values of the damping ratio y1 = odeint ( dy , y0 , t , args = ( 0.0 , w0 )) # undamped y2 = odeint ( dy , y0 , t , args = ( 0.2 , w0 )) # under damped y3 = odeint ( dy , y0 , t , args = ( 1.0 , w0 )) # critial damping y4 = odeint ( dy , y0 , t , args = ( 5.0 , w0 )) # over damped fig , ax = plt . subplots () ax . plot ( t , y1 [:, 0 ], 'k' , label = \"undamped\" , linewidth = 0.25 ) ax . plot ( t , y2 [:, 0 ], 'r' , label = \"under damped\" ) ax . plot ( t , y3 [:, 0 ], 'b' , label = r \"critical damping\" ) ax . plot ( t , y4 [:, 0 ], 'g' , label = \"over damped\" ) ax . legend ();","title":"Ejemplo: oscilador arm\u00f3nico amortiguado"},{"location":"lectures/data_manipulation/scientific_computing/scipy/#fourier-transform","text":"Las transformadas de Fourier son una de las herramientas universales de la f\u00edsica computacional, que aparecen una y otra vez en diferentes contextos. SciPy proporciona funciones para acceder a la biblioteca cl\u00e1sica [FFTPACK] (http://www.netlib.org/fftpack/) de NetLib, que es una biblioteca FFT eficiente y bien probada escrita en FORTRAN. La API SciPy tiene algunas funciones de conveniencia adicionales, pero en general, la API est\u00e1 estrechamente relacionada con la biblioteca FORTRAN original. Para usar el m\u00f3dulo fftpack en un programa Python, incl\u00fayalo usando: from numpy.fft import fftfreq from scipy.fftpack import * Para demostrar c\u00f3mo hacer una transformada r\u00e1pida de Fourier con SciPy, veamos la FFT de la soluci\u00f3n al oscilador amortiguado de la secci\u00f3n anterior: N = len ( t ) dt = t [ 1 ] - t [ 0 ] # calculate the fast fourier transform # y2 is the solution to the under-damped oscillator from the previous section F = fft ( y2 [:, 0 ]) # calculate the frequencies for the components in F w = fftfreq ( N , dt ) fig , ax = plt . subplots ( figsize = ( 9 , 3 )) ax . plot ( w , abs ( F )); Dado que la se\u00f1al es real, el espectro es sim\u00e9trico. Por lo tanto, solo necesitamos trazar la parte que corresponde a las frecuencias positivas. Para extraer esa parte de w y F podemos usar algunos de los trucos de indexaci\u00f3n para matrices NumPy que vimos en la lecci\u00f3n 2: indices = np . where ( w > 0 ) # select only indices for elements that corresponds to positive frequencies w_pos = w [ indices ] F_pos = F [ indices ] fig , ax = plt . subplots ( figsize = ( 9 , 3 )) ax . plot ( w_pos , abs ( F_pos )) ax . set_xlim ( 0 , 5 ); Como era de esperar, ahora vemos un pico en el espectro que se centra alrededor de 1, que es la frecuencia que usamos en el ejemplo del oscilador amortiguado.","title":"Fourier transform"},{"location":"lectures/data_manipulation/scientific_computing/scipy/#sparse-matrices","text":"Las matrices dispersas suelen ser \u00fatiles en simulaciones num\u00e9ricas que tratan con sistemas grandes, si el problema se puede describir en forma de matriz donde las matrices o vectores contienen en su mayor\u00eda ceros. Scipy tiene un buen soporte para matrices dispersas, con operaciones b\u00e1sicas de \u00e1lgebra lineal (como resoluci\u00f3n de ecuaciones, c\u00e1lculos de valores propios, etc.). Hay muchas estrategias posibles para almacenar matrices dispersas de manera eficiente. Algunos de los m\u00e1s comunes son el llamado formulario de coordenadas (COO), formulario de lista de lista (LIL) y CSC de columna comprimida y dispersa (y fila, CSR). Cada formato tiene algunas ventajas y desventajas. La mayor\u00eda de los algoritmos computacionales (resoluci\u00f3n de ecuaciones, multiplicaci\u00f3n de matriz-matriz, etc.) se pueden implementar de manera eficiente usando formatos CSR o CSC, pero no son tan intuitivos ni tan f\u00e1ciles de inicializar. Muy a menudo, una matriz dispersa se crea inicialmente en formato COO o LIL (donde podemos agregar elementos de manera eficiente a los datos de la matriz dispersa), y luego se convierte a CSC o CSR antes de usarse en c\u00e1lculos reales. Para obtener m\u00e1s informaci\u00f3n sobre estos formatos dispersos, consulte p. Ej. http://en.wikipedia.org/wiki/Sparse_matrix Cuando creamos una matriz dispersa, tenemos que elegir en qu\u00e9 formato se debe almacenar. Por ejemplo, from scipy.sparse import * # dense matrix M = np . array ([[ 1 , 0 , 0 , 0 ], [ 0 , 3 , 0 , 0 ], [ 0 , 1 , 1 , 0 ], [ 1 , 0 , 0 , 1 ]]); M array([[1, 0, 0, 0], [0, 3, 0, 0], [0, 1, 1, 0], [1, 0, 0, 1]]) # convert from dense to sparse A = csr_matrix ( M ); A <4x4 sparse matrix of type '<class 'numpy.int64'>' with 6 stored elements in Compressed Sparse Row format> # convert from sparse to dense A . todense () matrix([[1, 0, 0, 0], [0, 3, 0, 0], [0, 1, 1, 0], [1, 0, 0, 1]]) Una forma m\u00e1s eficiente de crear matrices dispersas: cree una matriz vac\u00eda y complete con el uso de indexaci\u00f3n matricial (evita crear una matriz densa potencialmente grande) A = lil_matrix (( 4 , 4 )) # empty 4x4 sparse matrix A [ 0 , 0 ] = 1 A [ 1 , 1 ] = 3 A [ 2 , 2 ] = A [ 2 , 1 ] = 1 A [ 3 , 3 ] = A [ 3 , 0 ] = 1 A <4x4 sparse matrix of type '<class 'numpy.float64'>' with 6 stored elements in List of Lists format> A . todense () matrix([[1., 0., 0., 0.], [0., 3., 0., 0.], [0., 1., 1., 0.], [1., 0., 0., 1.]]) Conversi\u00f3n entre diferentes formatos de matriz dispersa: A <4x4 sparse matrix of type '<class 'numpy.float64'>' with 6 stored elements in List of Lists format> A = csr_matrix ( A ); A <4x4 sparse matrix of type '<class 'numpy.float64'>' with 6 stored elements in Compressed Sparse Row format> A = csc_matrix ( A ); A <4x4 sparse matrix of type '<class 'numpy.float64'>' with 6 stored elements in Compressed Sparse Column format> Podemos calcular con matrices dispersas como con matrices densas: A . todense () matrix([[1., 0., 0., 0.], [0., 3., 0., 0.], [0., 1., 1., 0.], [1., 0., 0., 1.]]) ( A * A ) . todense () matrix([[1., 0., 0., 0.], [0., 9., 0., 0.], [0., 4., 1., 0.], [2., 0., 0., 1.]]) A . todense () matrix([[1., 0., 0., 0.], [0., 3., 0., 0.], [0., 1., 1., 0.], [1., 0., 0., 1.]]) A . dot ( A ) . todense () matrix([[1., 0., 0., 0.], [0., 9., 0., 0.], [0., 4., 1., 0.], [2., 0., 0., 1.]]) v = np . array ([ 1 , 2 , 3 , 4 ])[:, newaxis ]; v array([[1], [2], [3], [4]]) # sparse matrix - dense vector multiplication A * v array([[1.], [6.], [5.], [5.]]) # same result with dense matrix - dense vector multiplcation A . todense () * v matrix([[1.], [6.], [5.], [5.]])","title":"Sparse matrices"},{"location":"lectures/data_manipulation/scientific_computing/scipy/#optimization","text":"La optimizaci\u00f3n (encontrar m\u00ednimos o m\u00e1ximos de una funci\u00f3n) es un campo amplio en matem\u00e1ticas, y la optimizaci\u00f3n de funciones complicadas o en muchas variables puede estar bastante involucrada. Aqu\u00ed solo veremos algunos casos muy simples. Para obtener una introducci\u00f3n m\u00e1s detallada a la optimizaci\u00f3n con SciPy, consulte: http://scipy-lectures.github.com/advanced/mathematical_optimization/index.html Para usar el m\u00f3dulo de optimizaci\u00f3n en scipy, primero incluya el m\u00f3dulo optimize : from scipy import optimize","title":"Optimization"},{"location":"lectures/data_manipulation/scientific_computing/scipy/#encontrar-un-minimo","text":"Primero veamos c\u00f3mo encontrar los m\u00ednimos de una funci\u00f3n simple de una sola variable: def f ( x ): return 4 * x ** 3 + ( x - 2 ) ** 2 + x ** 4 fig , ax = plt . subplots () x = np . linspace ( - 5 , 3 , 100 ) ax . plot ( x , f ( x )); Podemos usar la funci\u00f3n fmin_bfgs para encontrar los m\u00ednimos de una funci\u00f3n: x_min = optimize . fmin_bfgs ( f , - 2 ) x_min Optimization terminated successfully. Current function value: -3.506641 Iterations: 5 Function evaluations: 16 Gradient evaluations: 8 array([-2.67298155]) optimize . fmin_bfgs ( f , 0.5 ) Optimization terminated successfully. Current function value: 2.804988 Iterations: 3 Function evaluations: 10 Gradient evaluations: 5 array([0.46961745]) Tambi\u00e9n podemos usar las funciones brent o fminbound . Tienen una sintaxis un poco diferente y usan diferentes algoritmos. optimize . brent ( f ) 0.46961743402759754 optimize . fminbound ( f , - 4 , 2 ) -2.6729822917513886","title":"Encontrar un m\u00ednimo"},{"location":"lectures/data_manipulation/scientific_computing/scipy/#encontrar-una-solucion-a-una-funcion","text":"Para encontrar la ra\u00edz de una funci\u00f3n de la forma $ f (x) = 0 $ podemos usar la funci\u00f3n fsolve . Requiere una suposici\u00f3n inicial: omega_c = 3.0 def f ( omega ): # a transcendental equation: resonance frequencies of a low-Q SQUID terminated microwave resonator return np . tan ( 2 * pi * omega ) - omega_c / omega fig , ax = plt . subplots ( figsize = ( 10 , 4 )) x = np . linspace ( 0 , 3 , 1000 ) y = f ( x ) mask = np . where ( abs ( y ) > 50 ) x [ mask ] = y [ mask ] = NaN # get rid of vertical line when the function flip sign ax . plot ( x , y ) ax . plot ([ 0 , 3 ], [ 0 , 0 ], 'k' ) ax . set_ylim ( - 5 , 5 ); <ipython-input-54-64c57d5fd427>:4: RuntimeWarning: divide by zero encountered in true_divide return np.tan(2*pi*omega) - omega_c/omega optimize . fsolve ( f , 0.1 ) array([0.23743014]) optimize . fsolve ( f , 0.6 ) array([0.71286972]) optimize . fsolve ( f , 1.1 ) array([1.18990285])","title":"Encontrar una soluci\u00f3n a una funci\u00f3n"},{"location":"lectures/data_manipulation/scientific_computing/scipy/#interpolation","text":"La interpolaci\u00f3n es simple y conveniente en scipy: la funci\u00f3n interp1d , cuando se dan matrices que describen datos \\(X\\) e \\(Y\\) , devuelve un objeto que se comporta como una funci\u00f3n que se puede llamar para un valor arbitrario de \\(x\\) (en el rango cubierto por \\(X\\) ), y devuelve el valor de \\(y\\) interpolado correspondiente: from scipy.interpolate import * def f ( x ): return np . sin ( x ) n = np . arange ( 0 , 10 ) x = np . linspace ( 0 , 9 , 100 ) y_meas = f ( n ) + 0.1 * np . random . randn ( len ( n )) # simulate measurement with noise y_real = f ( x ) linear_interpolation = interp1d ( n , y_meas ) y_interp1 = linear_interpolation ( x ) cubic_interpolation = interp1d ( n , y_meas , kind = 'cubic' ) y_interp2 = cubic_interpolation ( x ) fig , ax = plt . subplots ( figsize = ( 10 , 4 )) ax . plot ( n , y_meas , 'bs' , label = 'noisy data' ) ax . plot ( x , y_real , 'k' , lw = 2 , label = 'true function' ) ax . plot ( x , y_interp1 , 'r' , label = 'linear interp' ) ax . plot ( x , y_interp2 , 'g' , label = 'cubic interp' ) ax . legend ( loc = 3 );","title":"Interpolation"},{"location":"lectures/data_manipulation/scientific_computing/scipy/#statistics","text":"El m\u00f3dulo scipy.stats contiene una gran cantidad de distribuciones estad\u00edsticas, funciones estad\u00edsticas y pruebas. Para obtener una documentaci\u00f3n completa de sus funciones, consulte http://docs.scipy.org/doc/scipy/reference/stats.html. Tambi\u00e9n hay un paquete de Python muy poderoso para modelado estad\u00edstico llamado statsmodels. Consulte http://statsmodels.sourceforge.net para obtener m\u00e1s detalles. from scipy import stats # create a (discreet) random variable with poissionian distribution X = stats . poisson ( 3.5 ) # photon distribution for a coherent state with n=3.5 photons n = np . arange ( 0 , 15 ) fig , axes = plt . subplots ( 3 , 1 , sharex = True ) # plot the probability mass function (PMF) axes [ 0 ] . step ( n , X . pmf ( n )) # plot the commulative distribution function (CDF) axes [ 1 ] . step ( n , X . cdf ( n )) # plot histogram of 1000 random realizations of the stochastic variable X axes [ 2 ] . hist ( X . rvs ( size = 1000 )); # create a (continous) random variable with normal distribution Y = stats . norm () x = np . linspace ( - 5 , 5 , 100 ) fig , axes = plt . subplots ( 3 , 1 , sharex = True ) # plot the probability distribution function (PDF) axes [ 0 ] . plot ( x , Y . pdf ( x )) # plot the commulative distributin function (CDF) axes [ 1 ] . plot ( x , Y . cdf ( x )); # plot histogram of 1000 random realizations of the stochastic variable Y axes [ 2 ] . hist ( Y . rvs ( size = 1000 ), bins = 50 ); Estad\u00edsticas X . mean (), X . std (), X . var () # poission distribution (3.5, 1.8708286933869707, 3.5) Y . mean (), Y . std (), Y . var () # normal distribution (0.0, 1.0, 1.0)","title":"Statistics"},{"location":"lectures/data_manipulation/scientific_computing/scipy/#statistical-tests","text":"Pruebe si dos conjuntos de datos aleatorios (independientes) provienen de la misma distribuci\u00f3n: t_statistic , p_value = stats . ttest_ind ( X . rvs ( size = 1000 ), X . rvs ( size = 1000 )) print ( f \"t-statistic = { t_statistic } \" ) print ( f \"p-value = { p_value } \" ) t-statistic = -0.3175173118125565 p-value = 0.7508842832858349 Dado que el valor \\(p\\) es muy grande, no podemos rechazar la hip\u00f3tesis de que los dos conjuntos de datos aleatorios tienen medias * diferentes *. Para probar si la media de una sola muestra de datos tiene una media de 0,1 (la media verdadera es 0,0): stats . ttest_1samp ( Y . rvs ( size = 1000 ), 0.1 ) Ttest_1sampResult(statistic=-3.3260245451111463, pvalue=0.0009130194599003766) Un valor \\(p\\) bajo significa que podemos rechazar la hip\u00f3tesis de que la media de \\(Y\\) es 0,1. Y . mean () 0.0 stats . ttest_1samp ( Y . rvs ( size = 1000 ), Y . mean ()) Ttest_1sampResult(statistic=0.7254565446907949, pvalue=0.468341895612603)","title":"Statistical tests"},{"location":"lectures/data_manipulation/scientific_computing/scipy/#referencias","text":"A tutorial on how to get started using SciPy The SciPy source code","title":"Referencias"},{"location":"lectures/data_manipulation/scientific_computing/sympy/","text":"Sympy Introducci\u00f3n Hay dos sistemas de \u00e1lgebra computarizada (CAS) notables para Python: SymPy : un m\u00f3dulo de Python que se puede utilizar en cualquier programa de Python, o en una sesi\u00f3n de IPython, que proporciona potentes funciones de CAS. Sage - Sage es un entorno CAS muy potente y con todas las funciones que tiene como objetivo proporcionar un sistema de c\u00f3digo abierto que compita con Mathematica y Maple. Sage no es un m\u00f3dulo Python normal, sino un entorno CAS que utiliza Python como lenguaje de programaci\u00f3n. Sage es en algunos aspectos m\u00e1s poderoso que SymPy , pero ambos ofrecen una funcionalidad CAS muy completa. La ventaja de SymPy es que es un m\u00f3dulo Python normal y se integra bien con el port\u00e1til IPython. Para comenzar a usar SymPy en un programa o cuaderno de Python, importe el m\u00f3dulo sympy : from sympy import * Para obtener una salida con formato $\\LaTeX $ atractiva, ejecute: init_printing () # or with older versions of sympy/ipython, load the IPython extension #%load_ext sympy.interactive.ipythonprinting # or #%load_ext sympyprinting Variables simb\u00f3licas En SymPy necesitamos crear s\u00edmbolos para las variables con las que queremos trabajar. Podemos crear un nuevo s\u00edmbolo usando la clase Symbol : x = Symbol ( 'x' ) ( pi + x ) ** 2 \\(\\displaystyle \\left(x + \\pi\\right)^{2}\\) # alternative way of defining symbols a , b , c = symbols ( \"a, b, c\" ) type ( a ) sympy.core.symbol.Symbol Podemos agregar suposiciones a los s\u00edmbolos cuando los creamos: x = Symbol ( 'x' , real = True ) x . is_imaginary False x = Symbol ( 'x' , positive = True ) x > 0 \\(\\displaystyle \\text{True}\\) N\u00fameros complejos La unidad imaginaria se denota \"I\" en Sympy . 1 + 1 * I \\(\\displaystyle 1 + i\\) I ** 2 \\(\\displaystyle -1\\) ( x * I + 1 ) ** 2 \\(\\displaystyle \\left(i x + 1\\right)^{2}\\) Numeros racionales Hay tres tipos num\u00e9ricos diferentes en SymPy: Real , Rational , \u02bbInteger`: r1 = Rational ( 4 , 5 ) r2 = Rational ( 5 , 4 ) r1 \\(\\displaystyle \\frac{4}{5}\\) r1 + r2 \\(\\displaystyle \\frac{41}{20}\\) r1 / r2 \\(\\displaystyle \\frac{16}{25}\\) Evaluaci\u00f3n num\u00e9rica SymPy usa una biblioteca para precisi\u00f3n art\u00edstica como backend num\u00e9rico, y tiene expresiones SymPy predefinidas para una serie de constantes matem\u00e1ticas, como: pi , \u02bbe , \u02bboo para infinito. Para evaluar una expresi\u00f3n num\u00e9ricamente podemos usar la funci\u00f3n evalf (o N ). Toma un argumento \"n\" que especifica el n\u00famero de d\u00edgitos significativos. pi . evalf ( n = 50 ) \\(\\displaystyle 3.1415926535897932384626433832795028841971693993751\\) y = ( x + pi ) ** 2 N ( y , 5 ) # same as evalf \\(\\displaystyle 9.8696 \\left(0.31831 x + 1\\right)^{2}\\) Cuando evaluamos num\u00e9ricamente expresiones algebraicas, a menudo queremos sustituir un s\u00edmbolo por un valor num\u00e9rico. En SymPy lo hacemos usando la funci\u00f3n subs : y . subs ( x , 1.5 ) \\(\\displaystyle \\left(1.5 + \\pi\\right)^{2}\\) N ( y . subs ( x , 1.5 )) \\(\\displaystyle 21.5443823618587\\) Por supuesto, la funci\u00f3n subs tambi\u00e9n se puede utilizar para sustituir s\u00edmbolos y expresiones: y . subs ( x , a + pi ) \\(\\displaystyle \\left(a + 2 \\pi\\right)^{2}\\) Tambi\u00e9n podemos combinar la evoluci\u00f3n num\u00e9rica de expresiones con matrices Numpy : import numpy import matplotlib.pyplot as plt x_vec = numpy . arange ( 0 , 10 , 0.1 ) y_vec = numpy . array ([ N ((( x + pi ) ** 2 ) . subs ( x , xx )) for xx in x_vec ]) fig , ax = plt . subplots () ax . plot ( x_vec , y_vec ); Sin embargo, este tipo de evoluci\u00f3n num\u00e9rica puede ser muy lenta, y hay una manera mucho m\u00e1s eficiente de hacerlo: use la funci\u00f3n lambdify para\" compilar \"una expresi\u00f3n Sympy en una funci\u00f3n que sea mucho m\u00e1s eficiente para evaluar num\u00e9ricamente: f = lambdify ([ x ], ( x + pi ) ** 2 , 'numpy' ) # the first argument is a list of variables that # f will be a function of: in this case only x -> f(x) y_vec = f ( x_vec ) # now we can directly pass a numpy array and f(x) is efficiently evaluated La aceleraci\u00f3n cuando se utilizan funciones lambdify en lugar de una evaluaci\u00f3n num\u00e9rica directa puede ser significativa, a menudo de varios \u00f3rdenes de magnitud. Incluso en este ejemplo simple obtenemos una velocidad significativa: %% timeit y_vec = numpy . array ([ N ((( x + pi ) ** 2 ) . subs ( x , xx )) for xx in x_vec ]) 16.9 ms \u00b1 473 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) %% timeit y_vec = f ( x_vec ) 2.89 \u00b5s \u00b1 48.3 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each) Manipulaciones algebraicas Uno de los usos principales de un CAS es realizar manipulaciones algebraicas de expresiones. Por ejemplo, podr\u00edamos querer expandir un producto, factorizar una expresi\u00f3n o simplemente una expresi\u00f3n. Las funciones para realizar estas operaciones b\u00e1sicas en SymPy se muestran en esta secci\u00f3n. Expandir y factorizar Los primeros pasos en una manipulaci\u00f3n algebraica ( x + 1 ) * ( x + 2 ) * ( x + 3 ) \\(\\displaystyle \\left(x + 1\\right) \\left(x + 2\\right) \\left(x + 3\\right)\\) expand (( x + 1 ) * ( x + 2 ) * ( x + 3 )) \\(\\displaystyle x^{3} + 6 x^{2} + 11 x + 6\\) La funci\u00f3n expand toma un n\u00famero de argumentos de palabras clave que podemos decirle a las funciones qu\u00e9 tipo de expansiones queremos que se realicen. Por ejemplo, para expandir expresiones trigonom\u00e9tricas, use el argumento de palabra clave trig = True : sin ( a + b ) \\(\\displaystyle \\sin{\\left(a + b \\right)}\\) expand ( sin ( a + b ), trig = True ) \\(\\displaystyle \\sin{\\left(a \\right)} \\cos{\\left(b \\right)} + \\sin{\\left(b \\right)} \\cos{\\left(a \\right)}\\) Consulte help (expand) para obtener una explicaci\u00f3n detallada de los distintos tipos de expansiones que pueden realizar las funciones de \u02bbexpand`. Lo contrario, una expansi\u00f3n de producto es, por supuesto, factorizaci\u00f3n. El factor de una expresi\u00f3n en SymPy usa la funci\u00f3n factor : factor ( x ** 3 + 6 * x ** 2 + 11 * x + 6 ) \\(\\displaystyle \\left(x + 1\\right) \\left(x + 2\\right) \\left(x + 3\\right)\\) Simplificar El \"simplificar\" intenta simplificar una expresi\u00f3n en una expresi\u00f3n agradable, utilizando varias t\u00e9cnicas. Tambi\u00e9n existen alternativas m\u00e1s espec\u00edficas a las funciones simplify : trigsimp , powsimp , logcombine , etc. Los usos b\u00e1sicos de estas funciones son los siguientes: # simplify expands a product simplify (( x + 1 ) * ( x + 2 ) * ( x + 3 )) \\(\\displaystyle \\left(x + 1\\right) \\left(x + 2\\right) \\left(x + 3\\right)\\) # simplify uses trigonometric identities simplify ( sin ( a ) ** 2 + cos ( a ) ** 2 ) \\(\\displaystyle 1\\) simplify ( cos ( x ) / sin ( x )) \\(\\displaystyle \\frac{1}{\\tan{\\left(x \\right)}}\\) Separados y juntos Para manipular expresiones simb\u00f3licas de fracciones, podemos usar las funciones apart y together : apart f1 = 1 / (( a + 1 ) * ( a + 2 )) f1 \\(\\displaystyle \\frac{1}{\\left(a + 1\\right) \\left(a + 2\\right)}\\) apart ( f1 ) \\(\\displaystyle - \\frac{1}{a + 2} + \\frac{1}{a + 1}\\) together f2 = 1 / ( a + 2 ) + 1 / ( a + 3 ) f2 \\(\\displaystyle \\frac{1}{a + 3} + \\frac{1}{a + 2}\\) together ( f2 ) \\(\\displaystyle \\frac{2 a + 5}{\\left(a + 2\\right) \\left(a + 3\\right)}\\) Simplificar generalmente combina fracciones pero no factoriza: simplify ( f2 ) \\(\\displaystyle \\frac{2 a + 5}{\\left(a + 2\\right) \\left(a + 3\\right)}\\) C\u00e1lculo Adem\u00e1s de las manipulaciones algebraicas, el otro uso principal de CAS es hacer c\u00e1lculo, como derivadas e integrales de expresiones algebraicas. Diferenciaci\u00f3n La diferenciaci\u00f3n suele ser sencilla. Utilice la funci\u00f3n diff . El primer argumento es la expresi\u00f3n para tomar la derivada y el segundo argumento es el s\u00edmbolo por el cual tomar la derivada: y \\(\\displaystyle \\left(x + \\pi\\right)^{2}\\) diff ( y ** 2 , x ) \\(\\displaystyle 4 \\left(x + \\pi\\right)^{3}\\) Para derivados de orden superior podemos hacer: diff ( y ** 2 , x , x ) \\(\\displaystyle 12 \\left(x + \\pi\\right)^{2}\\) diff ( y ** 2 , x , 2 ) # same as above \\(\\displaystyle 12 \\left(x + \\pi\\right)^{2}\\) Para calcular la derivada de una expresi\u00f3n multivariante, podemos hacer: x , y , z = symbols ( \"x,y,z\" ) f = sin ( x * y ) + cos ( y * z ) \\(\\frac{d^3f}{dxdy^2}\\) diff ( f , x , 1 , y , 2 ) \\(\\displaystyle - x \\left(x y \\cos{\\left(x y \\right)} + 2 \\sin{\\left(x y \\right)}\\right)\\) Integraci\u00f3n La integraci\u00f3n se realiza de manera similar: f \\(\\displaystyle \\sin{\\left(x y \\right)} + \\cos{\\left(y z \\right)}\\) integrate ( f , x ) \\(\\displaystyle x \\cos{\\left(y z \\right)} + \\begin{cases} - \\frac{\\cos{\\left(x y \\right)}}{y} & \\text{for}\\: y \\neq 0 \\\\0 & \\text{otherwise} \\end{cases}\\) Al proporcionar l\u00edmites para la variable de integraci\u00f3n, podemos evaluar integrales definidas: integrate ( f , ( x , - 1 , 1 )) \\(\\displaystyle 2 \\cos{\\left(y z \\right)}\\) y tambi\u00e9n integrales impropias: integrate ( exp ( - x ** 2 ), ( x , - oo , oo )) \\(\\displaystyle \\sqrt{\\pi}\\) Recuerde, oo es la notaci\u00f3n SymPy para infinito. Sumas y productos Podemos evaluar sumas y productos usando las funciones: 'Suma' n = Symbol ( \"n\" ) Sum ( 1 / n ** 2 , ( n , 1 , 10 )) \\(\\displaystyle \\sum_{n=1}^{10} \\frac{1}{n^{2}}\\) Sum ( 1 / n ** 2 , ( n , 1 , 10 )) . evalf () \\(\\displaystyle 1.54976773116654\\) Sum ( 1 / n ** 2 , ( n , 1 , oo )) . evalf () \\(\\displaystyle 1.64493406684823\\) Los productos funcionan de la misma manera: Product ( n , ( n , 1 , 10 )) # 10! \\(\\displaystyle \\prod_{n=1}^{10} n\\) L\u00edmites Los l\u00edmites se pueden evaluar utilizando la funci\u00f3n limit . Por ejemplo, limit ( sin ( x ) / x , x , 0 ) \\(\\displaystyle 1\\) Podemos usar limit para verificar el resultado de la derivaci\u00f3n usando la funci\u00f3n diff : f \\(\\displaystyle \\sin{\\left(x y \\right)} + \\cos{\\left(y z \\right)}\\) diff ( f , x ) \\(\\displaystyle y \\cos{\\left(x y \\right)}\\) \\(\\displaystyle \\frac{\\mathrm{d}f(x,y)}{\\mathrm{d}x} = \\frac{f(x+h,y)-f(x,y)}{h}\\) h = Symbol ( \"h\" ) limit (( f . subs ( x , x + h ) - f ) / h , h , 0 ) \\(\\displaystyle y \\cos{\\left(x y \\right)}\\) Podemos cambiar la direcci\u00f3n desde la que nos acercamos al punto l\u00edmite usando el argumento dir : limit ( 1 / x , x , 0 , dir = \"+\" ) \\(\\displaystyle \\infty\\) limit ( 1 / x , x , 0 , dir = \"-\" ) \\(\\displaystyle -\\infty\\) Serie La expansi\u00f3n de la serie tambi\u00e9n es una de las caracter\u00edsticas m\u00e1s \u00fatiles de un CAS. En SymPy podemos realizar una expansi\u00f3n en serie de una expresi\u00f3n usando la funci\u00f3n series : series ( exp ( x ), x ) \\(\\displaystyle 1 + x + \\frac{x^{2}}{2} + \\frac{x^{3}}{6} + \\frac{x^{4}}{24} + \\frac{x^{5}}{120} + O\\left(x^{6}\\right)\\) De forma predeterminada, expande la expresi\u00f3n alrededor de \\(x = 0\\) , pero podemos expandir alrededor de cualquier valor de \\(x\\) al incluir expl\u00edcitamente un valor en la llamada a la funci\u00f3n: series ( exp ( x ), x , 1 ) \\(\\displaystyle e + e \\left(x - 1\\right) + \\frac{e \\left(x - 1\\right)^{2}}{2} + \\frac{e \\left(x - 1\\right)^{3}}{6} + \\frac{e \\left(x - 1\\right)^{4}}{24} + \\frac{e \\left(x - 1\\right)^{5}}{120} + O\\left(\\left(x - 1\\right)^{6}; x\\rightarrow 1\\right)\\) Y podemos definir expl\u00edcitamente en qu\u00e9 orden se debe realizar la expansi\u00f3n de la serie: series ( exp ( x ), x , 1 , 10 ) \\(\\displaystyle e + e \\left(x - 1\\right) + \\frac{e \\left(x - 1\\right)^{2}}{2} + \\frac{e \\left(x - 1\\right)^{3}}{6} + \\frac{e \\left(x - 1\\right)^{4}}{24} + \\frac{e \\left(x - 1\\right)^{5}}{120} + \\frac{e \\left(x - 1\\right)^{6}}{720} + \\frac{e \\left(x - 1\\right)^{7}}{5040} + \\frac{e \\left(x - 1\\right)^{8}}{40320} + \\frac{e \\left(x - 1\\right)^{9}}{362880} + O\\left(\\left(x - 1\\right)^{10}; x\\rightarrow 1\\right)\\) La expansi\u00f3n de la serie incluye el orden de la aproximaci\u00f3n, lo cual es muy \u00fatil para realizar un seguimiento del orden de validez cuando hacemos c\u00e1lculos con expansiones de la serie de diferente orden: s1 = cos ( x ) . series ( x , 0 , 5 ) s1 \\(\\displaystyle 1 - \\frac{x^{2}}{2} + \\frac{x^{4}}{24} + O\\left(x^{5}\\right)\\) s2 = sin ( x ) . series ( x , 0 , 2 ) s2 \\(\\displaystyle x + O\\left(x^{2}\\right)\\) expand ( s1 * s2 ) \\(\\displaystyle x + O\\left(x^{2}\\right)\\) Si queremos deshacernos de la informaci\u00f3n del error, podemos usar el m\u00e9todo removeO : expand ( s1 . removeO () * s2 . removeO ()) \\(\\displaystyle \\frac{x^{5}}{24} - \\frac{x^{3}}{2} + x\\) Pero tenga en cuenta que esta no es la expansi\u00f3n correcta de $ \\cos(x) \\sin(x)$ a $ 5 $ \u00e9simo orden: ( cos ( x ) * sin ( x )) . series ( x , 0 , 6 ) \\(\\displaystyle x - \\frac{2 x^{3}}{3} + \\frac{2 x^{5}}{15} + O\\left(x^{6}\\right)\\) \u00c1lgebra lineal Matrices Las matrices se definen usando la clase Matrix : m11 , m12 , m21 , m22 = symbols ( \"m11, m12, m21, m22\" ) b1 , b2 = symbols ( \"b1, b2\" ) A = Matrix ([[ m11 , m12 ],[ m21 , m22 ]]) A \\(\\displaystyle \\left[\\begin{matrix}m_{11} & m_{12}\\\\m_{21} & m_{22}\\end{matrix}\\right]\\) b = Matrix ([[ b1 ], [ b2 ]]) b \\(\\displaystyle \\left[\\begin{matrix}b_{1}\\\\b_{2}\\end{matrix}\\right]\\) Con las instancias de la clase Matrix podemos hacer las operaciones habituales de \u00e1lgebra matricial: A ** 2 \\(\\displaystyle \\left[\\begin{matrix}m_{11}^{2} + m_{12} m_{21} & m_{11} m_{12} + m_{12} m_{22}\\\\m_{11} m_{21} + m_{21} m_{22} & m_{12} m_{21} + m_{22}^{2}\\end{matrix}\\right]\\) A * b \\(\\displaystyle \\left[\\begin{matrix}b_{1} m_{11} + b_{2} m_{12}\\\\b_{1} m_{21} + b_{2} m_{22}\\end{matrix}\\right]\\) Y calcular determinantes e inversas, y similares: A . det () \\(\\displaystyle m_{11} m_{22} - m_{12} m_{21}\\) A . inv () \\(\\displaystyle \\left[\\begin{matrix}\\frac{m_{22}}{m_{11} m_{22} - m_{12} m_{21}} & - \\frac{m_{12}}{m_{11} m_{22} - m_{12} m_{21}}\\\\- \\frac{m_{21}}{m_{11} m_{22} - m_{12} m_{21}} & \\frac{m_{11}}{m_{11} m_{22} - m_{12} m_{21}}\\end{matrix}\\right]\\) Resolver ecuaciones Para resolver ecuaciones y sistemas de ecuaciones podemos usar la funci\u00f3n resolver : solve ( x ** 2 - 1 , x ) \\(\\displaystyle \\left[ -1, \\ 1\\right]\\) solve ( x ** 4 - x ** 2 - 1 , x ) \\(\\displaystyle \\left[ - i \\sqrt{- \\frac{1}{2} + \\frac{\\sqrt{5}}{2}}, \\ i \\sqrt{- \\frac{1}{2} + \\frac{\\sqrt{5}}{2}}, \\ - \\sqrt{\\frac{1}{2} + \\frac{\\sqrt{5}}{2}}, \\ \\sqrt{\\frac{1}{2} + \\frac{\\sqrt{5}}{2}}\\right]\\) Sistema de ecuaciones: solve ([ x + y - 1 , x - y - 1 ], [ x , y ]) \\(\\displaystyle \\left\\{ x : 1, \\ y : 0\\right\\}\\) En cuanto a otras expresiones simb\u00f3licas: solve ([ x + y - a , x - y - c ], [ x , y ]) \\(\\displaystyle \\left\\{ x : \\frac{a}{2} + \\frac{c}{2}, \\ y : \\frac{a}{2} - \\frac{c}{2}\\right\\}\\) Referencias The SymPy projects web page The source code of SymPy Online version of SymPy for testing and demonstrations","title":"Sympy"},{"location":"lectures/data_manipulation/scientific_computing/sympy/#sympy","text":"","title":"Sympy"},{"location":"lectures/data_manipulation/scientific_computing/sympy/#introduccion","text":"Hay dos sistemas de \u00e1lgebra computarizada (CAS) notables para Python: SymPy : un m\u00f3dulo de Python que se puede utilizar en cualquier programa de Python, o en una sesi\u00f3n de IPython, que proporciona potentes funciones de CAS. Sage - Sage es un entorno CAS muy potente y con todas las funciones que tiene como objetivo proporcionar un sistema de c\u00f3digo abierto que compita con Mathematica y Maple. Sage no es un m\u00f3dulo Python normal, sino un entorno CAS que utiliza Python como lenguaje de programaci\u00f3n. Sage es en algunos aspectos m\u00e1s poderoso que SymPy , pero ambos ofrecen una funcionalidad CAS muy completa. La ventaja de SymPy es que es un m\u00f3dulo Python normal y se integra bien con el port\u00e1til IPython. Para comenzar a usar SymPy en un programa o cuaderno de Python, importe el m\u00f3dulo sympy : from sympy import * Para obtener una salida con formato $\\LaTeX $ atractiva, ejecute: init_printing () # or with older versions of sympy/ipython, load the IPython extension #%load_ext sympy.interactive.ipythonprinting # or #%load_ext sympyprinting","title":"Introducci\u00f3n"},{"location":"lectures/data_manipulation/scientific_computing/sympy/#variables-simbolicas","text":"En SymPy necesitamos crear s\u00edmbolos para las variables con las que queremos trabajar. Podemos crear un nuevo s\u00edmbolo usando la clase Symbol : x = Symbol ( 'x' ) ( pi + x ) ** 2 \\(\\displaystyle \\left(x + \\pi\\right)^{2}\\) # alternative way of defining symbols a , b , c = symbols ( \"a, b, c\" ) type ( a ) sympy.core.symbol.Symbol Podemos agregar suposiciones a los s\u00edmbolos cuando los creamos: x = Symbol ( 'x' , real = True ) x . is_imaginary False x = Symbol ( 'x' , positive = True ) x > 0 \\(\\displaystyle \\text{True}\\)","title":"Variables simb\u00f3licas"},{"location":"lectures/data_manipulation/scientific_computing/sympy/#numeros-complejos","text":"La unidad imaginaria se denota \"I\" en Sympy . 1 + 1 * I \\(\\displaystyle 1 + i\\) I ** 2 \\(\\displaystyle -1\\) ( x * I + 1 ) ** 2 \\(\\displaystyle \\left(i x + 1\\right)^{2}\\)","title":"N\u00fameros complejos"},{"location":"lectures/data_manipulation/scientific_computing/sympy/#numeros-racionales","text":"Hay tres tipos num\u00e9ricos diferentes en SymPy: Real , Rational , \u02bbInteger`: r1 = Rational ( 4 , 5 ) r2 = Rational ( 5 , 4 ) r1 \\(\\displaystyle \\frac{4}{5}\\) r1 + r2 \\(\\displaystyle \\frac{41}{20}\\) r1 / r2 \\(\\displaystyle \\frac{16}{25}\\)","title":"Numeros racionales"},{"location":"lectures/data_manipulation/scientific_computing/sympy/#evaluacion-numerica","text":"SymPy usa una biblioteca para precisi\u00f3n art\u00edstica como backend num\u00e9rico, y tiene expresiones SymPy predefinidas para una serie de constantes matem\u00e1ticas, como: pi , \u02bbe , \u02bboo para infinito. Para evaluar una expresi\u00f3n num\u00e9ricamente podemos usar la funci\u00f3n evalf (o N ). Toma un argumento \"n\" que especifica el n\u00famero de d\u00edgitos significativos. pi . evalf ( n = 50 ) \\(\\displaystyle 3.1415926535897932384626433832795028841971693993751\\) y = ( x + pi ) ** 2 N ( y , 5 ) # same as evalf \\(\\displaystyle 9.8696 \\left(0.31831 x + 1\\right)^{2}\\) Cuando evaluamos num\u00e9ricamente expresiones algebraicas, a menudo queremos sustituir un s\u00edmbolo por un valor num\u00e9rico. En SymPy lo hacemos usando la funci\u00f3n subs : y . subs ( x , 1.5 ) \\(\\displaystyle \\left(1.5 + \\pi\\right)^{2}\\) N ( y . subs ( x , 1.5 )) \\(\\displaystyle 21.5443823618587\\) Por supuesto, la funci\u00f3n subs tambi\u00e9n se puede utilizar para sustituir s\u00edmbolos y expresiones: y . subs ( x , a + pi ) \\(\\displaystyle \\left(a + 2 \\pi\\right)^{2}\\) Tambi\u00e9n podemos combinar la evoluci\u00f3n num\u00e9rica de expresiones con matrices Numpy : import numpy import matplotlib.pyplot as plt x_vec = numpy . arange ( 0 , 10 , 0.1 ) y_vec = numpy . array ([ N ((( x + pi ) ** 2 ) . subs ( x , xx )) for xx in x_vec ]) fig , ax = plt . subplots () ax . plot ( x_vec , y_vec ); Sin embargo, este tipo de evoluci\u00f3n num\u00e9rica puede ser muy lenta, y hay una manera mucho m\u00e1s eficiente de hacerlo: use la funci\u00f3n lambdify para\" compilar \"una expresi\u00f3n Sympy en una funci\u00f3n que sea mucho m\u00e1s eficiente para evaluar num\u00e9ricamente: f = lambdify ([ x ], ( x + pi ) ** 2 , 'numpy' ) # the first argument is a list of variables that # f will be a function of: in this case only x -> f(x) y_vec = f ( x_vec ) # now we can directly pass a numpy array and f(x) is efficiently evaluated La aceleraci\u00f3n cuando se utilizan funciones lambdify en lugar de una evaluaci\u00f3n num\u00e9rica directa puede ser significativa, a menudo de varios \u00f3rdenes de magnitud. Incluso en este ejemplo simple obtenemos una velocidad significativa: %% timeit y_vec = numpy . array ([ N ((( x + pi ) ** 2 ) . subs ( x , xx )) for xx in x_vec ]) 16.9 ms \u00b1 473 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) %% timeit y_vec = f ( x_vec ) 2.89 \u00b5s \u00b1 48.3 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)","title":"Evaluaci\u00f3n num\u00e9rica"},{"location":"lectures/data_manipulation/scientific_computing/sympy/#manipulaciones-algebraicas","text":"Uno de los usos principales de un CAS es realizar manipulaciones algebraicas de expresiones. Por ejemplo, podr\u00edamos querer expandir un producto, factorizar una expresi\u00f3n o simplemente una expresi\u00f3n. Las funciones para realizar estas operaciones b\u00e1sicas en SymPy se muestran en esta secci\u00f3n.","title":"Manipulaciones algebraicas"},{"location":"lectures/data_manipulation/scientific_computing/sympy/#expandir-y-factorizar","text":"Los primeros pasos en una manipulaci\u00f3n algebraica ( x + 1 ) * ( x + 2 ) * ( x + 3 ) \\(\\displaystyle \\left(x + 1\\right) \\left(x + 2\\right) \\left(x + 3\\right)\\) expand (( x + 1 ) * ( x + 2 ) * ( x + 3 )) \\(\\displaystyle x^{3} + 6 x^{2} + 11 x + 6\\) La funci\u00f3n expand toma un n\u00famero de argumentos de palabras clave que podemos decirle a las funciones qu\u00e9 tipo de expansiones queremos que se realicen. Por ejemplo, para expandir expresiones trigonom\u00e9tricas, use el argumento de palabra clave trig = True : sin ( a + b ) \\(\\displaystyle \\sin{\\left(a + b \\right)}\\) expand ( sin ( a + b ), trig = True ) \\(\\displaystyle \\sin{\\left(a \\right)} \\cos{\\left(b \\right)} + \\sin{\\left(b \\right)} \\cos{\\left(a \\right)}\\) Consulte help (expand) para obtener una explicaci\u00f3n detallada de los distintos tipos de expansiones que pueden realizar las funciones de \u02bbexpand`. Lo contrario, una expansi\u00f3n de producto es, por supuesto, factorizaci\u00f3n. El factor de una expresi\u00f3n en SymPy usa la funci\u00f3n factor : factor ( x ** 3 + 6 * x ** 2 + 11 * x + 6 ) \\(\\displaystyle \\left(x + 1\\right) \\left(x + 2\\right) \\left(x + 3\\right)\\)","title":"Expandir y factorizar"},{"location":"lectures/data_manipulation/scientific_computing/sympy/#simplificar","text":"El \"simplificar\" intenta simplificar una expresi\u00f3n en una expresi\u00f3n agradable, utilizando varias t\u00e9cnicas. Tambi\u00e9n existen alternativas m\u00e1s espec\u00edficas a las funciones simplify : trigsimp , powsimp , logcombine , etc. Los usos b\u00e1sicos de estas funciones son los siguientes: # simplify expands a product simplify (( x + 1 ) * ( x + 2 ) * ( x + 3 )) \\(\\displaystyle \\left(x + 1\\right) \\left(x + 2\\right) \\left(x + 3\\right)\\) # simplify uses trigonometric identities simplify ( sin ( a ) ** 2 + cos ( a ) ** 2 ) \\(\\displaystyle 1\\) simplify ( cos ( x ) / sin ( x )) \\(\\displaystyle \\frac{1}{\\tan{\\left(x \\right)}}\\)","title":"Simplificar"},{"location":"lectures/data_manipulation/scientific_computing/sympy/#separados-y-juntos","text":"Para manipular expresiones simb\u00f3licas de fracciones, podemos usar las funciones apart y together : apart f1 = 1 / (( a + 1 ) * ( a + 2 )) f1 \\(\\displaystyle \\frac{1}{\\left(a + 1\\right) \\left(a + 2\\right)}\\) apart ( f1 ) \\(\\displaystyle - \\frac{1}{a + 2} + \\frac{1}{a + 1}\\) together f2 = 1 / ( a + 2 ) + 1 / ( a + 3 ) f2 \\(\\displaystyle \\frac{1}{a + 3} + \\frac{1}{a + 2}\\) together ( f2 ) \\(\\displaystyle \\frac{2 a + 5}{\\left(a + 2\\right) \\left(a + 3\\right)}\\) Simplificar generalmente combina fracciones pero no factoriza: simplify ( f2 ) \\(\\displaystyle \\frac{2 a + 5}{\\left(a + 2\\right) \\left(a + 3\\right)}\\)","title":"Separados y juntos"},{"location":"lectures/data_manipulation/scientific_computing/sympy/#calculo","text":"Adem\u00e1s de las manipulaciones algebraicas, el otro uso principal de CAS es hacer c\u00e1lculo, como derivadas e integrales de expresiones algebraicas.","title":"C\u00e1lculo"},{"location":"lectures/data_manipulation/scientific_computing/sympy/#diferenciacion","text":"La diferenciaci\u00f3n suele ser sencilla. Utilice la funci\u00f3n diff . El primer argumento es la expresi\u00f3n para tomar la derivada y el segundo argumento es el s\u00edmbolo por el cual tomar la derivada: y \\(\\displaystyle \\left(x + \\pi\\right)^{2}\\) diff ( y ** 2 , x ) \\(\\displaystyle 4 \\left(x + \\pi\\right)^{3}\\) Para derivados de orden superior podemos hacer: diff ( y ** 2 , x , x ) \\(\\displaystyle 12 \\left(x + \\pi\\right)^{2}\\) diff ( y ** 2 , x , 2 ) # same as above \\(\\displaystyle 12 \\left(x + \\pi\\right)^{2}\\) Para calcular la derivada de una expresi\u00f3n multivariante, podemos hacer: x , y , z = symbols ( \"x,y,z\" ) f = sin ( x * y ) + cos ( y * z ) \\(\\frac{d^3f}{dxdy^2}\\) diff ( f , x , 1 , y , 2 ) \\(\\displaystyle - x \\left(x y \\cos{\\left(x y \\right)} + 2 \\sin{\\left(x y \\right)}\\right)\\)","title":"Diferenciaci\u00f3n"},{"location":"lectures/data_manipulation/scientific_computing/sympy/#integracion","text":"La integraci\u00f3n se realiza de manera similar: f \\(\\displaystyle \\sin{\\left(x y \\right)} + \\cos{\\left(y z \\right)}\\) integrate ( f , x ) \\(\\displaystyle x \\cos{\\left(y z \\right)} + \\begin{cases} - \\frac{\\cos{\\left(x y \\right)}}{y} & \\text{for}\\: y \\neq 0 \\\\0 & \\text{otherwise} \\end{cases}\\) Al proporcionar l\u00edmites para la variable de integraci\u00f3n, podemos evaluar integrales definidas: integrate ( f , ( x , - 1 , 1 )) \\(\\displaystyle 2 \\cos{\\left(y z \\right)}\\) y tambi\u00e9n integrales impropias: integrate ( exp ( - x ** 2 ), ( x , - oo , oo )) \\(\\displaystyle \\sqrt{\\pi}\\) Recuerde, oo es la notaci\u00f3n SymPy para infinito.","title":"Integraci\u00f3n"},{"location":"lectures/data_manipulation/scientific_computing/sympy/#sumas-y-productos","text":"Podemos evaluar sumas y productos usando las funciones: 'Suma' n = Symbol ( \"n\" ) Sum ( 1 / n ** 2 , ( n , 1 , 10 )) \\(\\displaystyle \\sum_{n=1}^{10} \\frac{1}{n^{2}}\\) Sum ( 1 / n ** 2 , ( n , 1 , 10 )) . evalf () \\(\\displaystyle 1.54976773116654\\) Sum ( 1 / n ** 2 , ( n , 1 , oo )) . evalf () \\(\\displaystyle 1.64493406684823\\) Los productos funcionan de la misma manera: Product ( n , ( n , 1 , 10 )) # 10! \\(\\displaystyle \\prod_{n=1}^{10} n\\)","title":"Sumas y productos"},{"location":"lectures/data_manipulation/scientific_computing/sympy/#limites","text":"Los l\u00edmites se pueden evaluar utilizando la funci\u00f3n limit . Por ejemplo, limit ( sin ( x ) / x , x , 0 ) \\(\\displaystyle 1\\) Podemos usar limit para verificar el resultado de la derivaci\u00f3n usando la funci\u00f3n diff : f \\(\\displaystyle \\sin{\\left(x y \\right)} + \\cos{\\left(y z \\right)}\\) diff ( f , x ) \\(\\displaystyle y \\cos{\\left(x y \\right)}\\) \\(\\displaystyle \\frac{\\mathrm{d}f(x,y)}{\\mathrm{d}x} = \\frac{f(x+h,y)-f(x,y)}{h}\\) h = Symbol ( \"h\" ) limit (( f . subs ( x , x + h ) - f ) / h , h , 0 ) \\(\\displaystyle y \\cos{\\left(x y \\right)}\\) Podemos cambiar la direcci\u00f3n desde la que nos acercamos al punto l\u00edmite usando el argumento dir : limit ( 1 / x , x , 0 , dir = \"+\" ) \\(\\displaystyle \\infty\\) limit ( 1 / x , x , 0 , dir = \"-\" ) \\(\\displaystyle -\\infty\\)","title":"L\u00edmites"},{"location":"lectures/data_manipulation/scientific_computing/sympy/#serie","text":"La expansi\u00f3n de la serie tambi\u00e9n es una de las caracter\u00edsticas m\u00e1s \u00fatiles de un CAS. En SymPy podemos realizar una expansi\u00f3n en serie de una expresi\u00f3n usando la funci\u00f3n series : series ( exp ( x ), x ) \\(\\displaystyle 1 + x + \\frac{x^{2}}{2} + \\frac{x^{3}}{6} + \\frac{x^{4}}{24} + \\frac{x^{5}}{120} + O\\left(x^{6}\\right)\\) De forma predeterminada, expande la expresi\u00f3n alrededor de \\(x = 0\\) , pero podemos expandir alrededor de cualquier valor de \\(x\\) al incluir expl\u00edcitamente un valor en la llamada a la funci\u00f3n: series ( exp ( x ), x , 1 ) \\(\\displaystyle e + e \\left(x - 1\\right) + \\frac{e \\left(x - 1\\right)^{2}}{2} + \\frac{e \\left(x - 1\\right)^{3}}{6} + \\frac{e \\left(x - 1\\right)^{4}}{24} + \\frac{e \\left(x - 1\\right)^{5}}{120} + O\\left(\\left(x - 1\\right)^{6}; x\\rightarrow 1\\right)\\) Y podemos definir expl\u00edcitamente en qu\u00e9 orden se debe realizar la expansi\u00f3n de la serie: series ( exp ( x ), x , 1 , 10 ) \\(\\displaystyle e + e \\left(x - 1\\right) + \\frac{e \\left(x - 1\\right)^{2}}{2} + \\frac{e \\left(x - 1\\right)^{3}}{6} + \\frac{e \\left(x - 1\\right)^{4}}{24} + \\frac{e \\left(x - 1\\right)^{5}}{120} + \\frac{e \\left(x - 1\\right)^{6}}{720} + \\frac{e \\left(x - 1\\right)^{7}}{5040} + \\frac{e \\left(x - 1\\right)^{8}}{40320} + \\frac{e \\left(x - 1\\right)^{9}}{362880} + O\\left(\\left(x - 1\\right)^{10}; x\\rightarrow 1\\right)\\) La expansi\u00f3n de la serie incluye el orden de la aproximaci\u00f3n, lo cual es muy \u00fatil para realizar un seguimiento del orden de validez cuando hacemos c\u00e1lculos con expansiones de la serie de diferente orden: s1 = cos ( x ) . series ( x , 0 , 5 ) s1 \\(\\displaystyle 1 - \\frac{x^{2}}{2} + \\frac{x^{4}}{24} + O\\left(x^{5}\\right)\\) s2 = sin ( x ) . series ( x , 0 , 2 ) s2 \\(\\displaystyle x + O\\left(x^{2}\\right)\\) expand ( s1 * s2 ) \\(\\displaystyle x + O\\left(x^{2}\\right)\\) Si queremos deshacernos de la informaci\u00f3n del error, podemos usar el m\u00e9todo removeO : expand ( s1 . removeO () * s2 . removeO ()) \\(\\displaystyle \\frac{x^{5}}{24} - \\frac{x^{3}}{2} + x\\) Pero tenga en cuenta que esta no es la expansi\u00f3n correcta de $ \\cos(x) \\sin(x)$ a $ 5 $ \u00e9simo orden: ( cos ( x ) * sin ( x )) . series ( x , 0 , 6 ) \\(\\displaystyle x - \\frac{2 x^{3}}{3} + \\frac{2 x^{5}}{15} + O\\left(x^{6}\\right)\\)","title":"Serie"},{"location":"lectures/data_manipulation/scientific_computing/sympy/#algebra-lineal","text":"","title":"\u00c1lgebra lineal"},{"location":"lectures/data_manipulation/scientific_computing/sympy/#matrices","text":"Las matrices se definen usando la clase Matrix : m11 , m12 , m21 , m22 = symbols ( \"m11, m12, m21, m22\" ) b1 , b2 = symbols ( \"b1, b2\" ) A = Matrix ([[ m11 , m12 ],[ m21 , m22 ]]) A \\(\\displaystyle \\left[\\begin{matrix}m_{11} & m_{12}\\\\m_{21} & m_{22}\\end{matrix}\\right]\\) b = Matrix ([[ b1 ], [ b2 ]]) b \\(\\displaystyle \\left[\\begin{matrix}b_{1}\\\\b_{2}\\end{matrix}\\right]\\) Con las instancias de la clase Matrix podemos hacer las operaciones habituales de \u00e1lgebra matricial: A ** 2 \\(\\displaystyle \\left[\\begin{matrix}m_{11}^{2} + m_{12} m_{21} & m_{11} m_{12} + m_{12} m_{22}\\\\m_{11} m_{21} + m_{21} m_{22} & m_{12} m_{21} + m_{22}^{2}\\end{matrix}\\right]\\) A * b \\(\\displaystyle \\left[\\begin{matrix}b_{1} m_{11} + b_{2} m_{12}\\\\b_{1} m_{21} + b_{2} m_{22}\\end{matrix}\\right]\\) Y calcular determinantes e inversas, y similares: A . det () \\(\\displaystyle m_{11} m_{22} - m_{12} m_{21}\\) A . inv () \\(\\displaystyle \\left[\\begin{matrix}\\frac{m_{22}}{m_{11} m_{22} - m_{12} m_{21}} & - \\frac{m_{12}}{m_{11} m_{22} - m_{12} m_{21}}\\\\- \\frac{m_{21}}{m_{11} m_{22} - m_{12} m_{21}} & \\frac{m_{11}}{m_{11} m_{22} - m_{12} m_{21}}\\end{matrix}\\right]\\)","title":"Matrices"},{"location":"lectures/data_manipulation/scientific_computing/sympy/#resolver-ecuaciones","text":"Para resolver ecuaciones y sistemas de ecuaciones podemos usar la funci\u00f3n resolver : solve ( x ** 2 - 1 , x ) \\(\\displaystyle \\left[ -1, \\ 1\\right]\\) solve ( x ** 4 - x ** 2 - 1 , x ) \\(\\displaystyle \\left[ - i \\sqrt{- \\frac{1}{2} + \\frac{\\sqrt{5}}{2}}, \\ i \\sqrt{- \\frac{1}{2} + \\frac{\\sqrt{5}}{2}}, \\ - \\sqrt{\\frac{1}{2} + \\frac{\\sqrt{5}}{2}}, \\ \\sqrt{\\frac{1}{2} + \\frac{\\sqrt{5}}{2}}\\right]\\) Sistema de ecuaciones: solve ([ x + y - 1 , x - y - 1 ], [ x , y ]) \\(\\displaystyle \\left\\{ x : 1, \\ y : 0\\right\\}\\) En cuanto a otras expresiones simb\u00f3licas: solve ([ x + y - a , x - y - c ], [ x , y ]) \\(\\displaystyle \\left\\{ x : \\frac{a}{2} + \\frac{c}{2}, \\ y : \\frac{a}{2} - \\frac{c}{2}\\right\\}\\)","title":"Resolver ecuaciones"},{"location":"lectures/data_manipulation/scientific_computing/sympy/#referencias","text":"The SymPy projects web page The source code of SymPy Online version of SymPy for testing and demonstrations","title":"Referencias"},{"location":"lectures/data_manipulation/visualization/declarativa/","text":"Visualizaci\u00f3n Declarativa Es un paradigma de visualizaci\u00f3n en donde se busca preocuparse de los datos y sus relaciones, m\u00e1s que en detalles sin mayor importancia. Algunas caracter\u00edsticas son: Se especifica lo que se desea hacer. Los detalles se determinan autom\u00e1ticamente. Especificaci\u00f3n y Ejecuci\u00f3n est\u00e1n separadas. A modo de resumen, se refiere a construir visualizaciones a partir de los siguientes elementos: Data Transformation Marks Encoding Scale Guides Diferencias entre enfoques Imperativa Declarativa Especificar c\u00f3mo se debe hacer algo Especificar qu\u00e9 se quiere hacer Especificaci\u00f3n y ejecuci\u00f3n entrelazadas Separar especificaci\u00f3n de ejecuci\u00f3n Colocar un c\u00edrculo rojo aqu\u00ed y un c\u00edrculo azul ac\u00e1 Mapear x como posici\u00f3n e y como el color Ejemplo El Iris dataset es un conjunto de datos que contine una muestras de tres especies de Iris (Iris setosa, Iris virginica e Iris versicolor). Se midi\u00f3 cuatro rasgos de cada muestra: el largo y ancho del s\u00e9palo y p\u00e9talo, en cent\u00edmetros. Este ejemplo servir\u00e1 para mostrar una de las mayores diferencias entre una visualizaci\u00f3n imperativa (como matplotlib ) versus una declarativa (como seaborn ). # librerias import os import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns pd . set_option ( 'display.max_columns' , 500 ) # Ver m\u00e1s columnas de los dataframes # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab % matplotlib inline # cargar datos iris_df = pd . read_csv ( os . path . join ( \"data\" , \"iris.csv\" )) iris_df . columns = [ 'sepalLength' , 'sepalWidth' , 'petalLength' , 'petalWidth' , 'species' ] iris_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepalLength sepalWidth petalLength petalWidth species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa El ejemplo cl\u00e1sico consiste en graficar sepalWidth versus petalLength y colorear por especie. Imperativo En matplotlib ser\u00eda algo as\u00ed: color_map = dict ( zip ( iris_df [ \"species\" ] . unique (), [ \"blue\" , \"green\" , \"red\" ])) plt . figure ( figsize = ( 10 , 6 )) for species , group in iris_df . groupby ( \"species\" ): plt . scatter ( group [ \"petalLength\" ], group [ \"sepalWidth\" ], color = color_map [ species ], alpha = 0.3 , edgecolor = None , label = species , ) plt . legend ( frameon = True , title = \"species\" ) plt . xlabel ( \"petalLength\" ) plt . ylabel ( \"sepalWidth\" ) plt . show () Declarativo En seaborn ser\u00eda algo as\u00ed: sns . set ( rc = { 'figure.figsize' :( 10 , 8 )}) sns . scatterplot ( x = 'petalLength' , y = 'sepalWidth' , data = iris_df , hue = 'species' , palette = [ 'blue' , 'green' , 'red' ] ) plt . show () --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-1-8a6710c6b0dd> in <module> ----> 1 sns.set(rc={'figure.figsize':(10,8)}) 2 3 sns.scatterplot( 4 x='petalLength', 5 y='sepalWidth', NameError: name 'sns' is not defined","title":"Visualizaci\u00f3n Declarativa"},{"location":"lectures/data_manipulation/visualization/declarativa/#visualizacion-declarativa","text":"Es un paradigma de visualizaci\u00f3n en donde se busca preocuparse de los datos y sus relaciones, m\u00e1s que en detalles sin mayor importancia. Algunas caracter\u00edsticas son: Se especifica lo que se desea hacer. Los detalles se determinan autom\u00e1ticamente. Especificaci\u00f3n y Ejecuci\u00f3n est\u00e1n separadas. A modo de resumen, se refiere a construir visualizaciones a partir de los siguientes elementos: Data Transformation Marks Encoding Scale Guides Diferencias entre enfoques Imperativa Declarativa Especificar c\u00f3mo se debe hacer algo Especificar qu\u00e9 se quiere hacer Especificaci\u00f3n y ejecuci\u00f3n entrelazadas Separar especificaci\u00f3n de ejecuci\u00f3n Colocar un c\u00edrculo rojo aqu\u00ed y un c\u00edrculo azul ac\u00e1 Mapear x como posici\u00f3n e y como el color Ejemplo El Iris dataset es un conjunto de datos que contine una muestras de tres especies de Iris (Iris setosa, Iris virginica e Iris versicolor). Se midi\u00f3 cuatro rasgos de cada muestra: el largo y ancho del s\u00e9palo y p\u00e9talo, en cent\u00edmetros. Este ejemplo servir\u00e1 para mostrar una de las mayores diferencias entre una visualizaci\u00f3n imperativa (como matplotlib ) versus una declarativa (como seaborn ). # librerias import os import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns pd . set_option ( 'display.max_columns' , 500 ) # Ver m\u00e1s columnas de los dataframes # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab % matplotlib inline # cargar datos iris_df = pd . read_csv ( os . path . join ( \"data\" , \"iris.csv\" )) iris_df . columns = [ 'sepalLength' , 'sepalWidth' , 'petalLength' , 'petalWidth' , 'species' ] iris_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepalLength sepalWidth petalLength petalWidth species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa El ejemplo cl\u00e1sico consiste en graficar sepalWidth versus petalLength y colorear por especie. Imperativo En matplotlib ser\u00eda algo as\u00ed: color_map = dict ( zip ( iris_df [ \"species\" ] . unique (), [ \"blue\" , \"green\" , \"red\" ])) plt . figure ( figsize = ( 10 , 6 )) for species , group in iris_df . groupby ( \"species\" ): plt . scatter ( group [ \"petalLength\" ], group [ \"sepalWidth\" ], color = color_map [ species ], alpha = 0.3 , edgecolor = None , label = species , ) plt . legend ( frameon = True , title = \"species\" ) plt . xlabel ( \"petalLength\" ) plt . ylabel ( \"sepalWidth\" ) plt . show () Declarativo En seaborn ser\u00eda algo as\u00ed: sns . set ( rc = { 'figure.figsize' :( 10 , 8 )}) sns . scatterplot ( x = 'petalLength' , y = 'sepalWidth' , data = iris_df , hue = 'species' , palette = [ 'blue' , 'green' , 'red' ] ) plt . show () --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-1-8a6710c6b0dd> in <module> ----> 1 sns.set(rc={'figure.figsize':(10,8)}) 2 3 sns.scatterplot( 4 x='petalLength', 5 y='sepalWidth', NameError: name 'sns' is not defined","title":"Visualizaci\u00f3n Declarativa"},{"location":"lectures/data_manipulation/visualization/introduccion/","text":"Introducci\u00f3n \u00bfPor qu\u00e9 aprender sobre visualizaci\u00f3n? Porque un resultado no sirve si no puede comunicarse correctamente. Porque una buena visualizaci\u00f3n dista de ser una tarea trivial. Porque un ingenierio necesita producir excelentes gr\u00e1ficos (pero nadie ense\u00f1a c\u00f3mo). No es exageraci\u00f3n Primeras visualizaciones Campa\u00f1a de Napole\u00f3n a Mosc\u00fa (Charles Minard, 1889). Mapa del c\u00f3lera (John Snow, 1855). \u00bfPor qu\u00e9 utilizar gr\u00e1ficos para representar datos? El 70 % de los receptores sensoriales del cuerpo humano est\u00e1 dedicado a la visi\u00f3n. Cerebro ha sido entrenado evolutivamente para interpretar la informaci\u00f3n visual de manera masiva. \u201cThe eye and the visual cortex of the brain form a massively parallel processor that provides the highest bandwidth channel into human cognitive centers\u201d \u2014 Colin Ware, Information Visualization, 2004. Cuarteto de ANSCOMBE Considere los siguientes 4 conjuntos de datos. \u00bfQu\u00e9 puede decir de los datos? import numpy as np import pandas as pd import os import matplotlib.pyplot as plt % matplotlib inline df = pd . read_csv ( os . path . join ( \"data\" , \"anscombe.csv\" )) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 y1 x2 y2 x3 y3 x4 y4 0 10 8.04 10 9.14 10 7.46 8 6.58 1 8 6.95 8 8.14 8 6.77 8 5.76 2 13 7.58 13 8.74 13 12.74 8 7.71 3 9 8.81 9 8.77 9 7.11 8 8.84 4 11 8.33 11 9.26 11 7.81 8 8.47 5 14 9.96 14 8.10 14 8.84 8 7.04 6 6 7.24 6 6.13 6 6.08 8 5.25 7 4 4.26 4 3.10 4 5.39 19 12.50 8 12 10.84 12 9.13 12 8.15 8 5.56 9 7 4.82 7 7.26 7 6.42 8 7.91 10 5 5.68 5 4.74 5 5.73 8 6.89 df . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 y1 x2 y2 x3 y3 x4 y4 count 11.000000 11.000000 11.000000 11.000000 11.000000 11.000000 11.000000 11.000000 mean 9.000000 7.500909 9.000000 7.500909 9.000000 7.500000 9.000000 7.500909 std 3.316625 2.031568 3.316625 2.031657 3.316625 2.030424 3.316625 2.030579 min 4.000000 4.260000 4.000000 3.100000 4.000000 5.390000 8.000000 5.250000 25% 6.500000 6.315000 6.500000 6.695000 6.500000 6.250000 8.000000 6.170000 50% 9.000000 7.580000 9.000000 8.140000 9.000000 7.110000 8.000000 7.040000 75% 11.500000 8.570000 11.500000 8.950000 11.500000 7.980000 8.000000 8.190000 max 14.000000 10.840000 14.000000 9.260000 14.000000 12.740000 19.000000 12.500000 \u00bfPor qu\u00e9 es un ejemplo cl\u00e1sico? for i in range ( 1 , 4 + 1 ): x = df . loc [:, f \"x { i } \" ] . values y = df . loc [:, f \"y { i } \" ] . values slope , intercept = np . polyfit ( x , y , 1 ) print ( f \"Grupo { i } : \\n\\t Tiene pendiente { slope : .2f } e intercepto { intercept : .2f } . \\n \" ) Grupo 1: Tiene pendiente 0.50 e intercepto 3.00. Grupo 2: Tiene pendiente 0.50 e intercepto 3.00. Grupo 3: Tiene pendiente 0.50 e intercepto 3.00. Grupo 4: Tiene pendiente 0.50 e intercepto 3.00. groups = range ( 1 , 4 + 1 ) x_columns = [ col for col in df if \"x\" in col ] x_aux = np . arange ( df . loc [:, x_columns ] . values . min () - 1 , df . loc [:, x_columns ] . values . max () + 2 ) fig , axs = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 16 , 8 ), sharex = True , sharey = True ) fig . suptitle ( \"Cuarteto de Anscombe\" ) for i , ax in zip ( groups , axs . ravel ()): x = df . loc [:, f \"x { i } \" ] . values y = df . loc [:, f \"y { i } \" ] . values m , b = np . polyfit ( x , y , 1 ) ax . plot ( x , y , 'o' ) ax . plot ( x_aux , m * x_aux + b , 'r' , lw = 2.0 ) ax . set_title ( f \"Grupo { i } \" ) Teor\u00eda Sistema visual humano Buenas noticias Gr\u00e1ficos entregan informaci\u00f3n que la estad\u00edstica podr\u00eda no revelar. Despliegue visual es esencial para comprensi\u00f3n. Malas noticias La atenci\u00f3n es selectiva y puede ser f\u00e1cilmente enga\u00f1ada. La atenci\u00f3n es selectiva y puede ser f\u00e1cilmente enga\u00f1ada. Consejos generales Noah Illinsky, en su charla \"Cuatro pilatres de la visualizaci\u00f3n\" ( es , en ), presenta buenos consejos sobre c\u00f3mo realizar una correcta visualizaci\u00f3n: * Prop\u00f3sito * Informaci\u00f3n/Contenido * Codificaci\u00f3n/Estructura * Formato Es altamente aconsejable ver el video, pero en resumen: Prop\u00f3sito o p\u00fablico tiene que ver con para qui\u00e9n se est\u00e1 preparando la viz y que utilidad se le dar\u00e1. Es muy diferente preparar un gr\u00e1fico orientado a informaci\u00f3n y toma de decisiones. Informaci\u00f3n/Contenido se refiere a contar con la informaci\u00f3n que se desea mostrar, en el formato necesario para su procesamiento. Codificaci\u00f3n/Estructura tiene que ver con la selecci\u00f3n correcta de la codificaci\u00f3n y estructura de la informaci\u00f3n. Formato tiene que ver con la elecci\u00f3n de fuentes, colores, tama\u00f1os relativos, etc. Lo anterior indica que una visualizaci\u00f3n no es el resultado de unos datos. Una visualizaci\u00f3n se dise\u00f1a, se piensa, y luego se buscan fuentes de informaci\u00f3n apropiadas. Elementos para la creaci\u00f3n de una buena visualizaci\u00f3n Honestidad : representaciones visuales no deben enga\u00f1ar al observador. Priorizaci\u00f3n : dato m\u00e1s importante debe utilizar elemento de mejor percepci\u00f3n. Expresividad : datos deben utilizar elementos con atribuciones adecuadas. Consistencia : codificaci\u00f3n visual debe permitir reproducir datos. El principio b\u00e1sico a respetar es que a partir del gr\u00e1fico uno debe poder reobtener f\u00e1cilmente los datos originales. Honestidad El ojo humano no tiene la misma precisi\u00f3n al estimar distintas atribuciones: * Largo : Bien estimado y sin sesgo, con un factor multiplicativo de 0.9 a 1.1. * \u00c1rea : Subestimado y con sesgo, con un factor multiplicativo de 0.6 a 0.9. * Volumen : Muy subestimado y con sesgo, con un factor multiplicativo de 0.5 a 0.8. Resulta inadecuado realizar gr\u00e1ficos de datos utilizando \u00e1reas o vol\u00famenes si no queda claro la atribuci\u00f3n utilizada. Una pseudo-excepci\u00f3n la constituyen los pie-chart o gr\u00e1ficos circulares, porque el ojo humano distingue bien \u00e1ngulos y segmentos de c\u00edrculo, y porque es posible indicar los porcentajes respectivos. ## Example from https://matplotlib.org/3.1.1/gallery/pie_and_polar_charts/pie_features.html#sphx-glr-gallery-pie-and-polar-charts-pie-features-py # Pie chart, where the slices will be ordered and plotted counter-clockwise: labels = 'Frogs' , 'Hogs' , 'Dogs' , 'Logs' sizes = [ 15 , 30 , 45 , 10 ] explode = ( 0 , 0.1 , 0 , 0 ) # only \"explode\" the 2nd slice (i.e. 'Hogs') fig1 , ax1 = plt . subplots ( figsize = ( 8 , 8 )) ax1 . pie ( sizes , explode = explode , labels = labels , autopct = ' %1.1f%% ' , shadow = True , startangle = 90 ) ax1 . axis ( 'equal' ) # Equal aspect ratio ensures that pie is drawn as a circle. plt . show () Priorizaci\u00f3n Dato m\u00e1s importante debe utilizar elemento de mejor percepci\u00f3n. np . random . seed ( 42 ) N = 31 x = np . arange ( N ) y1 = 80 + 20 * x / N + 5 * np . random . rand ( N ) y2 = 75 + 25 * x / N + 5 * np . random . rand ( N ) fig , axs = plt . subplots ( 2 , 2 , sharex = True , sharey = True , figsize = ( 16 , 8 )) axs [ 0 ][ 0 ] . plot ( x , y1 , 'ok' ) axs [ 0 ][ 0 ] . plot ( x , y2 , 'sk' ) axs [ 0 ][ 1 ] . plot ( x , y1 , 'ob' ) axs [ 0 ][ 1 ] . plot ( x , y2 , 'or' ) axs [ 1 ][ 0 ] . plot ( x , y1 , 'ob' ) axs [ 1 ][ 0 ] . plot ( x , y2 , '*k' ) axs [ 1 ][ 1 ] . plot ( x , y1 , 'sr' ) axs [ 1 ][ 1 ] . plot ( x , y2 , 'ob' ) plt . show () Elementos de mejor percepci\u00f3n No todos los elementos tienen la misma percepci\u00f3n a nivel del sistema visual. En particular, el color y la forma son elementos preatentivos: un color distinto o una forma distinta se reconocen de manera no conciente. Ejemplos de elementos preatentivos. \u00bfEn que orden creen que el sistema visual humano puede estimar los siguientes atributos visuales: * Color * Pendiente * Largo * \u00c1ngulo * Posici\u00f3n * \u00c1rea * Volumen El sistema visual humano puede estimar con precisi\u00f3n siguientes atributos visuales: 1. Posici\u00f3n 2. Largo 3. Pendiente 4. \u00c1ngulo 5. \u00c1rea 6. Volumen 7. Color Utilice el atributo que se estima con mayor precisi\u00f3n cuando sea posible. Colormaps Puesto que la percepci\u00f3n del color tiene muy baja precisi\u00f3n, resulta inadecuado tratar de representar un valor num\u00e9rico con colores. * \u00bfQu\u00e9 diferencia num\u00e9rica existe entre el verde y el rojo? * \u00bfQue asociaci\u00f3n preexistente posee el color rojo, el amarillo y el verde? * \u00bfCon cu\u00e1nta precisi\u00f3n podemos distinguir valores en una escala de grises? Algunos ejemplos de colormaps import matplotlib.cm as cm from scipy.stats import multivariate_normal x , y = np . mgrid [ - 3 : 3 : .025 , - 2 : 2 : .025 ] pos = np . empty ( x . shape + ( 2 ,)) pos [:, :, 0 ] = x pos [:, :, 1 ] = y z1 = multivariate_normal . pdf ( pos , mean = [ - 1.0 , - 1.0 ], cov = [[ 1.0 , 0.0 ], [ 0.0 , 0.1 ]] ) z2 = multivariate_normal . pdf ( pos , mean = [ 1.0 , 1.0 ], cov = [[ 1.5 , 0.0 ], [ 0.0 , 0.5 ]] ) z = 10 * ( z1 - z2 ) fig , axs = plt . subplots ( 2 , 2 , figsize = ( 16 , 8 ), sharex = True , sharey = True ) cmaps = [ cm . rainbow , cm . autumn , cm . coolwarm , cm . gray ] for i , ax in zip ( range ( len ( cmaps )), axs . ravel ()): im = ax . imshow ( z , interpolation = 'bilinear' , origin = 'lower' , cmap = cmaps [ i ], extent = ( - 3 , 3 , - 2 , 2 )) fig . colorbar ( im , ax = ax ) fig . show () Consejo: evite mientras pueda los colormaps. Por ejemplo, utilizando contour plots. fig , axs = plt . subplots ( 2 , 2 , figsize = ( 20 , 12 ), sharex = True , sharey = True ) cmaps = [ cm . rainbow , cm . autumn , cm . coolwarm , cm . gray ] countour_styles = [ { \"cmap\" : cm . rainbow }, { \"cmap\" : cm . rainbow }, { \"colors\" : \"k\" , \"linestyles\" : \"solid\" }, { \"colors\" : \"k\" , \"linestyles\" : \"dashed\" }, ] for i , ax in zip ( range ( len ( cmaps )), axs . ravel ()): cs = ax . contour ( x , y , z , 11 , ** countour_styles [ i ]) if i > 0 : ax . clabel ( cs , fontsize = 9 , inline = 1 ) if i == 3 : ax . grid ( alpha = 0.5 ) fig . show () Sobre la Expresividad Mostrar los datos y s\u00f3lo los datos. Los datos deben utilizar elementos con atribuciones adecuadas: Not all data is born equal . Clasificaci\u00f3n de datos: * Datos Cuantitativos : Cuantificaci\u00f3n absoluta. * Cantidad de az\u00facar en fruta: 50 [gr/kg] * Operaciones =, \\(\\neq\\) , <, >, +, \u2212, * , / * Datos Posicionales : Cuantificaci\u00f3n relativa. * Fecha de cosecha: 1 Agosto 2014, 2 Agosto 2014. * Operaciones =, \\(\\neq\\) , <, >, +, \u2212 * Datos Ordinales : Orden sin cuantificaci\u00f3n. * Calidad de la Fruta: baja, media, alta, exportaci\u00f3n. * Operaciones =, \\(\\neq\\) , <, > * Datos Nominales : Nombres o clasificaciones * Frutas: manzana, pera, kiwi, ... * Operaciones \\(=\\) , \\(\\neq\\) Ejemplo: Terremotos. \u00bfQue tipos de datos tenemos? * Ciudad m\u00e1s pr\u00f3xima * A\u00f1o * Magnitud en escala Richter * Magnitud en escala Mercalli * Latitud * Longitud Contraejemplo: Compa\u00f1\u00edas de computadores. Compan\u00eda Procedencia MSI Taiwan Asus Taiwan Acer Taiwan HP EEUU Dell EEUU Apple EEUU Sony Japon Toshiba Japon Lenovo Hong Kong Samsung Corea del Sur brands = { \"MSI\" : \"Taiwan\" , \"Asus\" : \"Taiwan\" , \"Acer\" : \"Taiwan\" , \"HP\" : \"EEUU\" , \"Dell\" : \"EEUU\" , \"Apple\" : \"EEUU\" , \"Sony\" : \"Japon\" , \"Toshiba\" : \"Japon\" , \"Lenovo\" : \"Hong Kong\" , \"Samsung\" : \"Corea del Sur\" } C2N = { \"Taiwan\" : 1 , \"EEUU\" : 2 , \"Japon\" : 3 , \"Hong Kong\" : 4 , \"Corea del Sur\" : 7 } x = np . arange ( len ( brands . keys ())) y = np . array ([ C2N [ val ] for val in brands . values ()]) width = 0.35 # the width of the bars fig , ax = plt . subplots ( figsize = ( 16 , 8 )) rects1 = ax . bar ( x , y , width , color = 'r' ) # add some text for labels, title and axes ticks ax . set_xticks ( x + 0.5 * width ) ax . set_xticklabels ( brands . keys (), rotation = \"90\" ) ax . set_yticks ( list ( C2N . values ())) ax . set_yticklabels ( C2N . keys ()) plt . xlim ([ - 1 , len ( x ) + 1 ]) plt . ylim ([ - 1 , y . max () + 1 ]) plt . show () Clasificaci\u00f3n de datos: * Datos Cuantitativos : Cuantificaci\u00f3n absoluta. * Cantidad de az\u00facar en fruta: 50 [gr/kg] * Operaciones =, \\(\\neq\\) , <, >, +, \u2212, * , / * Utilizar posici\u00f3n, largo, pendiente o \u00e1ngulo Datos Posicionales : Cuantificaci\u00f3n relativa. * Fecha de cosecha: 1 Agosto 2014, 2 Agosto 2014. * Operaciones =, \\(\\neq\\) , <, >, +, \u2212 * Utilizar posici\u00f3n, largo, pendiente o \u00e1ngulo * Datos Ordinales : Orden sin cuantificaci\u00f3n. * Calidad de la Fruta: baja, media, alta, exportaci\u00f3n. * Operaciones =, \\(\\neq\\) , <, > * Utilizar marcadores diferenciados en forma o tama\u00f1o, o mapa de colores apropiado * Datos Nominales : Nombres o clasificaciones * Frutas: manzana, pera, kiwi, ... * Operaciones \\(=\\) , \\(\\neq\\) * Utilizar forma o color * Consistencia La codificaci\u00f3n visual debe permitir reproducir datos. Para ello debemos: * Graficar datos que sean comparables. * Utilizar ejes escalados adecuadamente. * Utilizar la misma codificaci\u00f3n visual entre gr\u00e1ficos similares. Utilizar ejes escalados adecuadamente. x = list ( range ( 1 , 13 )) y = 80 + 20 * np . random . rand ( 12 ) x_ticks = list ( \"EFMAMJJASOND\" ) fig , ( ax1 , ax2 ) = plt . subplots ( ncols = 2 , figsize = ( 20 , 8 )) ax1 . plot ( x , y , 'o-' ) ax1 . set_xticks ( x ) ax1 . set_xticklabels ( x_ticks ) ax1 . grid ( alpha = 0.5 ) ax2 . plot ( x , y , 'o-' ) ax2 . set_xticks ( x ) ax2 . set_xticklabels ( x_ticks ) ax2 . set_ylim ([ 0 , 110 ]) ax2 . grid ( alpha = 0.5 ) fig . show () Utilizar la misma codificaci\u00f3n visual entre gr\u00e1ficos similares x = np . linspace ( 0 , 1 , 50 ) f1 = x ** 2 + .2 * np . random . rand ( 50 ) g1 = x + .2 * np . random . rand ( 50 ) f2 = 0.5 - 0.2 * x + .2 * np . random . rand ( 50 ) g2 = x ** 3 + .2 * np . random . rand ( 50 ) fig , ( ax1 , ax2 ) = plt . subplots ( nrows = 2 , figsize = ( 20 , 12 ), sharex = True ) ax1 . set_title ( \"Antes de MAT281\" ) ax1 . plot ( x , f1 , 'b' , label = 'Chile' , lw = 2.0 ) ax1 . plot ( x , g1 , 'g:' , label = 'OECD' , lw = 2.0 ) ax1 . legend ( loc = \"upper left\" ) ax2 . set_title ( \"Despues de MAT281\" ) ax2 . plot ( x , f2 , 'g:' , label = 'Chile' , lw = 2.0 ) ax2 . plot ( x , g2 , 'b' , label = 'OECD' , lw = 2.0 ) ax2 . legend () fig . show () Python Landscape Para empezar, PyViz es un sitio web que se dedica a ayudar a los usuarios a decidir dentro de las mejores herramientas de visualizaci\u00f3n open-source implementadas en Python, dependiendo de sus necesidades y objetivos. Mucho de lo que se menciona en esta secci\u00f3n est\u00e1 en detalle en la p\u00e1gina web del proyecto PyViz. Algunas de las librer\u00edas de visualizaci\u00f3n de Python m\u00e1s conocidas son: Este esquema es una adaptaci\u00f3n de uno presentado en la charla The Python Visualization Landscape realizada por Jake VanderPlas en la PyCon 2017. Cada una de estas librer\u00edas fue creada para satisfacer diferentes necesidades, algunas han ganado m\u00e1s adeptos que otras por uno u otro motivo. Tal como avanza la tecnolog\u00eda, estas librer\u00edas se actualizan o se crean nuevas, la importancia no recae en ser un experto en una, si no en saber adaptarse a las situaciones, tomar la mejor decicisi\u00f3n y escoger seg\u00fan nuestras necesidades y preferencias. Por ejemplo, matplotlib naci\u00f3 como una soluci\u00f3n para imitar los gr\u00e1ficos de MATLAB (puedes ver la historia completa aqu\u00ed ), manteniendo una sintaxis similar y con ello poder crear gr\u00e1ficos est\u00e1ticos de muy buen nivel. Debido al \u00e9xito de matplotlib en la comunidad, nacen librer\u00edas basadas ella. Algunos ejemplos son: seaborn se basa en matp\u013aotlib pero su nicho corresponde a las visualizaciones estad\u00edsticas. ggpy una suerte de copia a ggplot2 perteneciente al lenguaje de programaci\u00f3n R . networkx visualizaciones de grafos. pandas no es una librer\u00eda de visualizaci\u00f3n propiamente tal, pero utiliza a matplotplib como bakcned en los m\u00e9todos con tal de crear gr\u00e1ficos de manera muy r\u00e1pida, e.g. pandas.DataFrame.plot.bar() Por otro lado, con tal de crear visualizaciones interactivas aparecen librer\u00edas basadas en javascript , algunas de las m\u00e1s conocidas en Python son: bokeh tiene como objetivo proporcionar gr\u00e1ficos vers\u00e1tiles, elegantes e incluso interactivos, teniendo una gran performance con grandes datasets o incluso streaming de datos. plotly visualizaciones interactivas que en conjunto a Dash (de la misma empresa) permite crear aplicaciones webs, similar a shiny de R . D3.js a pesar de estar basado en javascript se ha ganado un lugar en el coraz\u00f3n de toda la comunidad, debido a la ilimitada cantidad de visualizaciones que son posibles de hacer, por ejemplo, la malla interactiva que hizo un estudiante de la UTFSM est\u00e1 hecha en D3.js . De las librer\u00edas m\u00e1s recientes est\u00e1 Altair , que consiste en visualizaciones declarativas (ya lo veremos en el pr\u00f3ximo laboratorio). Constru\u00edda sobre Vega-Lite , a su vez que est\u00e9 est\u00e1 sobre Vega y este finalmente sobre D3.js . Altair permite crear visualizaciones est\u00e1ticas e interactivas con pocas l\u00edneas de c\u00f3digo, sin embargo, al ser relativamente nueva, a\u00fan existen funcionalidades en desarrollo o que simplemente a\u00fan no existen en esta librer\u00eda pero en otras si. Clasificaci\u00f3n En lo concierne a nosotros, una de las principales clasificaciones para estas librer\u00edas es si crean visualizaciones est\u00e1tica y/o interactivas . La interactividad es un plus que permite adentrarse en los datos en distintos niveles, si agregamos que ciertas librer\u00edas permiten crear widgets (algo as\u00ed como complementos a las visualizaciones) su potencial aumenta. Por ejemplo, un widget podr\u00eda ser un filtro que permita escoger un pa\u00eds; en una librer\u00eda est\u00e1tica tendr\u00edas que crear un gr\u00e1fico por cada pa\u00eds (o combinaci\u00f3n de pa\u00edses) lo cual no se hace escalable y c\u00f3modo para trabajar. Spoilers Las pr\u00f3ximas clases se centrar\u00e1n en matplotlib y Seaborn , dado que son buenos exponentes de visualizaci\u00f3n imperativa y declarativa, respectivamente. Finalmente, siempre hay que tener en consideraci\u00f3n la manera en que se compartir\u00e1n las visualizaciones, por ejemplo, si es para un art\u00edculo cient\u00edfico bastar\u00eda que fuese de buena calidad y est\u00e1tico. Si es para una plataforma web es necesario que sea interactivo, aqu\u00ed es donde entran en juego los dashboards, que permiten la exploraci\u00f3n de datos de manera interactiva. En Python existen librer\u00edas como Dash o Panel , sin embargo, en el mundo empresarial se suele utilizar software dedicado a esto, como Power BI o Tableau .","title":"Introducci\u00f3n"},{"location":"lectures/data_manipulation/visualization/introduccion/#introduccion","text":"","title":"Introducci\u00f3n"},{"location":"lectures/data_manipulation/visualization/introduccion/#por-que-aprender-sobre-visualizacion","text":"Porque un resultado no sirve si no puede comunicarse correctamente. Porque una buena visualizaci\u00f3n dista de ser una tarea trivial. Porque un ingenierio necesita producir excelentes gr\u00e1ficos (pero nadie ense\u00f1a c\u00f3mo). No es exageraci\u00f3n","title":"\u00bfPor qu\u00e9 aprender sobre visualizaci\u00f3n?"},{"location":"lectures/data_manipulation/visualization/introduccion/#primeras-visualizaciones","text":"","title":"Primeras visualizaciones"},{"location":"lectures/data_manipulation/visualization/introduccion/#campana-de-napoleon-a-moscu-charles-minard-1889","text":"","title":"Campa\u00f1a de Napole\u00f3n a Mosc\u00fa (Charles Minard, 1889)."},{"location":"lectures/data_manipulation/visualization/introduccion/#mapa-del-colera-john-snow-1855","text":"","title":"Mapa del c\u00f3lera (John Snow, 1855)."},{"location":"lectures/data_manipulation/visualization/introduccion/#por-que-utilizar-graficos-para-representar-datos","text":"El 70 % de los receptores sensoriales del cuerpo humano est\u00e1 dedicado a la visi\u00f3n. Cerebro ha sido entrenado evolutivamente para interpretar la informaci\u00f3n visual de manera masiva. \u201cThe eye and the visual cortex of the brain form a massively parallel processor that provides the highest bandwidth channel into human cognitive centers\u201d \u2014 Colin Ware, Information Visualization, 2004.","title":"\u00bfPor qu\u00e9 utilizar gr\u00e1ficos para representar datos?"},{"location":"lectures/data_manipulation/visualization/introduccion/#cuarteto-de-anscombe","text":"Considere los siguientes 4 conjuntos de datos. \u00bfQu\u00e9 puede decir de los datos? import numpy as np import pandas as pd import os import matplotlib.pyplot as plt % matplotlib inline df = pd . read_csv ( os . path . join ( \"data\" , \"anscombe.csv\" )) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 y1 x2 y2 x3 y3 x4 y4 0 10 8.04 10 9.14 10 7.46 8 6.58 1 8 6.95 8 8.14 8 6.77 8 5.76 2 13 7.58 13 8.74 13 12.74 8 7.71 3 9 8.81 9 8.77 9 7.11 8 8.84 4 11 8.33 11 9.26 11 7.81 8 8.47 5 14 9.96 14 8.10 14 8.84 8 7.04 6 6 7.24 6 6.13 6 6.08 8 5.25 7 4 4.26 4 3.10 4 5.39 19 12.50 8 12 10.84 12 9.13 12 8.15 8 5.56 9 7 4.82 7 7.26 7 6.42 8 7.91 10 5 5.68 5 4.74 5 5.73 8 6.89 df . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 y1 x2 y2 x3 y3 x4 y4 count 11.000000 11.000000 11.000000 11.000000 11.000000 11.000000 11.000000 11.000000 mean 9.000000 7.500909 9.000000 7.500909 9.000000 7.500000 9.000000 7.500909 std 3.316625 2.031568 3.316625 2.031657 3.316625 2.030424 3.316625 2.030579 min 4.000000 4.260000 4.000000 3.100000 4.000000 5.390000 8.000000 5.250000 25% 6.500000 6.315000 6.500000 6.695000 6.500000 6.250000 8.000000 6.170000 50% 9.000000 7.580000 9.000000 8.140000 9.000000 7.110000 8.000000 7.040000 75% 11.500000 8.570000 11.500000 8.950000 11.500000 7.980000 8.000000 8.190000 max 14.000000 10.840000 14.000000 9.260000 14.000000 12.740000 19.000000 12.500000 \u00bfPor qu\u00e9 es un ejemplo cl\u00e1sico? for i in range ( 1 , 4 + 1 ): x = df . loc [:, f \"x { i } \" ] . values y = df . loc [:, f \"y { i } \" ] . values slope , intercept = np . polyfit ( x , y , 1 ) print ( f \"Grupo { i } : \\n\\t Tiene pendiente { slope : .2f } e intercepto { intercept : .2f } . \\n \" ) Grupo 1: Tiene pendiente 0.50 e intercepto 3.00. Grupo 2: Tiene pendiente 0.50 e intercepto 3.00. Grupo 3: Tiene pendiente 0.50 e intercepto 3.00. Grupo 4: Tiene pendiente 0.50 e intercepto 3.00. groups = range ( 1 , 4 + 1 ) x_columns = [ col for col in df if \"x\" in col ] x_aux = np . arange ( df . loc [:, x_columns ] . values . min () - 1 , df . loc [:, x_columns ] . values . max () + 2 ) fig , axs = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 16 , 8 ), sharex = True , sharey = True ) fig . suptitle ( \"Cuarteto de Anscombe\" ) for i , ax in zip ( groups , axs . ravel ()): x = df . loc [:, f \"x { i } \" ] . values y = df . loc [:, f \"y { i } \" ] . values m , b = np . polyfit ( x , y , 1 ) ax . plot ( x , y , 'o' ) ax . plot ( x_aux , m * x_aux + b , 'r' , lw = 2.0 ) ax . set_title ( f \"Grupo { i } \" )","title":"Cuarteto de ANSCOMBE"},{"location":"lectures/data_manipulation/visualization/introduccion/#teoria","text":"","title":"Teor\u00eda"},{"location":"lectures/data_manipulation/visualization/introduccion/#sistema-visual-humano","text":"","title":"Sistema visual humano"},{"location":"lectures/data_manipulation/visualization/introduccion/#buenas-noticias","text":"Gr\u00e1ficos entregan informaci\u00f3n que la estad\u00edstica podr\u00eda no revelar. Despliegue visual es esencial para comprensi\u00f3n.","title":"Buenas noticias"},{"location":"lectures/data_manipulation/visualization/introduccion/#malas-noticias","text":"La atenci\u00f3n es selectiva y puede ser f\u00e1cilmente enga\u00f1ada.","title":"Malas noticias"},{"location":"lectures/data_manipulation/visualization/introduccion/#la-atencion-es-selectiva-y-puede-ser-facilmente-enganada","text":"","title":"La atenci\u00f3n es selectiva y puede ser f\u00e1cilmente enga\u00f1ada."},{"location":"lectures/data_manipulation/visualization/introduccion/#consejos-generales","text":"Noah Illinsky, en su charla \"Cuatro pilatres de la visualizaci\u00f3n\" ( es , en ), presenta buenos consejos sobre c\u00f3mo realizar una correcta visualizaci\u00f3n: * Prop\u00f3sito * Informaci\u00f3n/Contenido * Codificaci\u00f3n/Estructura * Formato Es altamente aconsejable ver el video, pero en resumen: Prop\u00f3sito o p\u00fablico tiene que ver con para qui\u00e9n se est\u00e1 preparando la viz y que utilidad se le dar\u00e1. Es muy diferente preparar un gr\u00e1fico orientado a informaci\u00f3n y toma de decisiones. Informaci\u00f3n/Contenido se refiere a contar con la informaci\u00f3n que se desea mostrar, en el formato necesario para su procesamiento. Codificaci\u00f3n/Estructura tiene que ver con la selecci\u00f3n correcta de la codificaci\u00f3n y estructura de la informaci\u00f3n. Formato tiene que ver con la elecci\u00f3n de fuentes, colores, tama\u00f1os relativos, etc. Lo anterior indica que una visualizaci\u00f3n no es el resultado de unos datos. Una visualizaci\u00f3n se dise\u00f1a, se piensa, y luego se buscan fuentes de informaci\u00f3n apropiadas.","title":"Consejos generales"},{"location":"lectures/data_manipulation/visualization/introduccion/#elementos-para-la-creacion-de-una-buena-visualizacion","text":"Honestidad : representaciones visuales no deben enga\u00f1ar al observador. Priorizaci\u00f3n : dato m\u00e1s importante debe utilizar elemento de mejor percepci\u00f3n. Expresividad : datos deben utilizar elementos con atribuciones adecuadas. Consistencia : codificaci\u00f3n visual debe permitir reproducir datos. El principio b\u00e1sico a respetar es que a partir del gr\u00e1fico uno debe poder reobtener f\u00e1cilmente los datos originales.","title":"Elementos para la creaci\u00f3n de una buena visualizaci\u00f3n"},{"location":"lectures/data_manipulation/visualization/introduccion/#honestidad","text":"El ojo humano no tiene la misma precisi\u00f3n al estimar distintas atribuciones: * Largo : Bien estimado y sin sesgo, con un factor multiplicativo de 0.9 a 1.1. * \u00c1rea : Subestimado y con sesgo, con un factor multiplicativo de 0.6 a 0.9. * Volumen : Muy subestimado y con sesgo, con un factor multiplicativo de 0.5 a 0.8. Resulta inadecuado realizar gr\u00e1ficos de datos utilizando \u00e1reas o vol\u00famenes si no queda claro la atribuci\u00f3n utilizada. Una pseudo-excepci\u00f3n la constituyen los pie-chart o gr\u00e1ficos circulares, porque el ojo humano distingue bien \u00e1ngulos y segmentos de c\u00edrculo, y porque es posible indicar los porcentajes respectivos. ## Example from https://matplotlib.org/3.1.1/gallery/pie_and_polar_charts/pie_features.html#sphx-glr-gallery-pie-and-polar-charts-pie-features-py # Pie chart, where the slices will be ordered and plotted counter-clockwise: labels = 'Frogs' , 'Hogs' , 'Dogs' , 'Logs' sizes = [ 15 , 30 , 45 , 10 ] explode = ( 0 , 0.1 , 0 , 0 ) # only \"explode\" the 2nd slice (i.e. 'Hogs') fig1 , ax1 = plt . subplots ( figsize = ( 8 , 8 )) ax1 . pie ( sizes , explode = explode , labels = labels , autopct = ' %1.1f%% ' , shadow = True , startangle = 90 ) ax1 . axis ( 'equal' ) # Equal aspect ratio ensures that pie is drawn as a circle. plt . show ()","title":"Honestidad"},{"location":"lectures/data_manipulation/visualization/introduccion/#priorizacion","text":"Dato m\u00e1s importante debe utilizar elemento de mejor percepci\u00f3n. np . random . seed ( 42 ) N = 31 x = np . arange ( N ) y1 = 80 + 20 * x / N + 5 * np . random . rand ( N ) y2 = 75 + 25 * x / N + 5 * np . random . rand ( N ) fig , axs = plt . subplots ( 2 , 2 , sharex = True , sharey = True , figsize = ( 16 , 8 )) axs [ 0 ][ 0 ] . plot ( x , y1 , 'ok' ) axs [ 0 ][ 0 ] . plot ( x , y2 , 'sk' ) axs [ 0 ][ 1 ] . plot ( x , y1 , 'ob' ) axs [ 0 ][ 1 ] . plot ( x , y2 , 'or' ) axs [ 1 ][ 0 ] . plot ( x , y1 , 'ob' ) axs [ 1 ][ 0 ] . plot ( x , y2 , '*k' ) axs [ 1 ][ 1 ] . plot ( x , y1 , 'sr' ) axs [ 1 ][ 1 ] . plot ( x , y2 , 'ob' ) plt . show ()","title":"Priorizaci\u00f3n"},{"location":"lectures/data_manipulation/visualization/introduccion/#elementos-de-mejor-percepcion","text":"No todos los elementos tienen la misma percepci\u00f3n a nivel del sistema visual. En particular, el color y la forma son elementos preatentivos: un color distinto o una forma distinta se reconocen de manera no conciente. Ejemplos de elementos preatentivos. \u00bfEn que orden creen que el sistema visual humano puede estimar los siguientes atributos visuales: * Color * Pendiente * Largo * \u00c1ngulo * Posici\u00f3n * \u00c1rea * Volumen El sistema visual humano puede estimar con precisi\u00f3n siguientes atributos visuales: 1. Posici\u00f3n 2. Largo 3. Pendiente 4. \u00c1ngulo 5. \u00c1rea 6. Volumen 7. Color Utilice el atributo que se estima con mayor precisi\u00f3n cuando sea posible.","title":"Elementos de mejor percepci\u00f3n"},{"location":"lectures/data_manipulation/visualization/introduccion/#colormaps","text":"Puesto que la percepci\u00f3n del color tiene muy baja precisi\u00f3n, resulta inadecuado tratar de representar un valor num\u00e9rico con colores. * \u00bfQu\u00e9 diferencia num\u00e9rica existe entre el verde y el rojo? * \u00bfQue asociaci\u00f3n preexistente posee el color rojo, el amarillo y el verde? * \u00bfCon cu\u00e1nta precisi\u00f3n podemos distinguir valores en una escala de grises? Algunos ejemplos de colormaps import matplotlib.cm as cm from scipy.stats import multivariate_normal x , y = np . mgrid [ - 3 : 3 : .025 , - 2 : 2 : .025 ] pos = np . empty ( x . shape + ( 2 ,)) pos [:, :, 0 ] = x pos [:, :, 1 ] = y z1 = multivariate_normal . pdf ( pos , mean = [ - 1.0 , - 1.0 ], cov = [[ 1.0 , 0.0 ], [ 0.0 , 0.1 ]] ) z2 = multivariate_normal . pdf ( pos , mean = [ 1.0 , 1.0 ], cov = [[ 1.5 , 0.0 ], [ 0.0 , 0.5 ]] ) z = 10 * ( z1 - z2 ) fig , axs = plt . subplots ( 2 , 2 , figsize = ( 16 , 8 ), sharex = True , sharey = True ) cmaps = [ cm . rainbow , cm . autumn , cm . coolwarm , cm . gray ] for i , ax in zip ( range ( len ( cmaps )), axs . ravel ()): im = ax . imshow ( z , interpolation = 'bilinear' , origin = 'lower' , cmap = cmaps [ i ], extent = ( - 3 , 3 , - 2 , 2 )) fig . colorbar ( im , ax = ax ) fig . show () Consejo: evite mientras pueda los colormaps. Por ejemplo, utilizando contour plots. fig , axs = plt . subplots ( 2 , 2 , figsize = ( 20 , 12 ), sharex = True , sharey = True ) cmaps = [ cm . rainbow , cm . autumn , cm . coolwarm , cm . gray ] countour_styles = [ { \"cmap\" : cm . rainbow }, { \"cmap\" : cm . rainbow }, { \"colors\" : \"k\" , \"linestyles\" : \"solid\" }, { \"colors\" : \"k\" , \"linestyles\" : \"dashed\" }, ] for i , ax in zip ( range ( len ( cmaps )), axs . ravel ()): cs = ax . contour ( x , y , z , 11 , ** countour_styles [ i ]) if i > 0 : ax . clabel ( cs , fontsize = 9 , inline = 1 ) if i == 3 : ax . grid ( alpha = 0.5 ) fig . show ()","title":"Colormaps"},{"location":"lectures/data_manipulation/visualization/introduccion/#sobre-la-expresividad","text":"Mostrar los datos y s\u00f3lo los datos. Los datos deben utilizar elementos con atribuciones adecuadas: Not all data is born equal . Clasificaci\u00f3n de datos: * Datos Cuantitativos : Cuantificaci\u00f3n absoluta. * Cantidad de az\u00facar en fruta: 50 [gr/kg] * Operaciones =, \\(\\neq\\) , <, >, +, \u2212, * , / * Datos Posicionales : Cuantificaci\u00f3n relativa. * Fecha de cosecha: 1 Agosto 2014, 2 Agosto 2014. * Operaciones =, \\(\\neq\\) , <, >, +, \u2212 * Datos Ordinales : Orden sin cuantificaci\u00f3n. * Calidad de la Fruta: baja, media, alta, exportaci\u00f3n. * Operaciones =, \\(\\neq\\) , <, > * Datos Nominales : Nombres o clasificaciones * Frutas: manzana, pera, kiwi, ... * Operaciones \\(=\\) , \\(\\neq\\) Ejemplo: Terremotos. \u00bfQue tipos de datos tenemos? * Ciudad m\u00e1s pr\u00f3xima * A\u00f1o * Magnitud en escala Richter * Magnitud en escala Mercalli * Latitud * Longitud Contraejemplo: Compa\u00f1\u00edas de computadores. Compan\u00eda Procedencia MSI Taiwan Asus Taiwan Acer Taiwan HP EEUU Dell EEUU Apple EEUU Sony Japon Toshiba Japon Lenovo Hong Kong Samsung Corea del Sur brands = { \"MSI\" : \"Taiwan\" , \"Asus\" : \"Taiwan\" , \"Acer\" : \"Taiwan\" , \"HP\" : \"EEUU\" , \"Dell\" : \"EEUU\" , \"Apple\" : \"EEUU\" , \"Sony\" : \"Japon\" , \"Toshiba\" : \"Japon\" , \"Lenovo\" : \"Hong Kong\" , \"Samsung\" : \"Corea del Sur\" } C2N = { \"Taiwan\" : 1 , \"EEUU\" : 2 , \"Japon\" : 3 , \"Hong Kong\" : 4 , \"Corea del Sur\" : 7 } x = np . arange ( len ( brands . keys ())) y = np . array ([ C2N [ val ] for val in brands . values ()]) width = 0.35 # the width of the bars fig , ax = plt . subplots ( figsize = ( 16 , 8 )) rects1 = ax . bar ( x , y , width , color = 'r' ) # add some text for labels, title and axes ticks ax . set_xticks ( x + 0.5 * width ) ax . set_xticklabels ( brands . keys (), rotation = \"90\" ) ax . set_yticks ( list ( C2N . values ())) ax . set_yticklabels ( C2N . keys ()) plt . xlim ([ - 1 , len ( x ) + 1 ]) plt . ylim ([ - 1 , y . max () + 1 ]) plt . show () Clasificaci\u00f3n de datos: * Datos Cuantitativos : Cuantificaci\u00f3n absoluta. * Cantidad de az\u00facar en fruta: 50 [gr/kg] * Operaciones =, \\(\\neq\\) , <, >, +, \u2212, * , / * Utilizar posici\u00f3n, largo, pendiente o \u00e1ngulo Datos Posicionales : Cuantificaci\u00f3n relativa. * Fecha de cosecha: 1 Agosto 2014, 2 Agosto 2014. * Operaciones =, \\(\\neq\\) , <, >, +, \u2212 * Utilizar posici\u00f3n, largo, pendiente o \u00e1ngulo * Datos Ordinales : Orden sin cuantificaci\u00f3n. * Calidad de la Fruta: baja, media, alta, exportaci\u00f3n. * Operaciones =, \\(\\neq\\) , <, > * Utilizar marcadores diferenciados en forma o tama\u00f1o, o mapa de colores apropiado * Datos Nominales : Nombres o clasificaciones * Frutas: manzana, pera, kiwi, ... * Operaciones \\(=\\) , \\(\\neq\\) * Utilizar forma o color *","title":"Sobre la Expresividad"},{"location":"lectures/data_manipulation/visualization/introduccion/#consistencia","text":"La codificaci\u00f3n visual debe permitir reproducir datos. Para ello debemos: * Graficar datos que sean comparables. * Utilizar ejes escalados adecuadamente. * Utilizar la misma codificaci\u00f3n visual entre gr\u00e1ficos similares.","title":"Consistencia"},{"location":"lectures/data_manipulation/visualization/introduccion/#utilizar-ejes-escalados-adecuadamente","text":"x = list ( range ( 1 , 13 )) y = 80 + 20 * np . random . rand ( 12 ) x_ticks = list ( \"EFMAMJJASOND\" ) fig , ( ax1 , ax2 ) = plt . subplots ( ncols = 2 , figsize = ( 20 , 8 )) ax1 . plot ( x , y , 'o-' ) ax1 . set_xticks ( x ) ax1 . set_xticklabels ( x_ticks ) ax1 . grid ( alpha = 0.5 ) ax2 . plot ( x , y , 'o-' ) ax2 . set_xticks ( x ) ax2 . set_xticklabels ( x_ticks ) ax2 . set_ylim ([ 0 , 110 ]) ax2 . grid ( alpha = 0.5 ) fig . show ()","title":"Utilizar ejes escalados adecuadamente."},{"location":"lectures/data_manipulation/visualization/introduccion/#utilizar-la-misma-codificacion-visual-entre-graficos-similares","text":"x = np . linspace ( 0 , 1 , 50 ) f1 = x ** 2 + .2 * np . random . rand ( 50 ) g1 = x + .2 * np . random . rand ( 50 ) f2 = 0.5 - 0.2 * x + .2 * np . random . rand ( 50 ) g2 = x ** 3 + .2 * np . random . rand ( 50 ) fig , ( ax1 , ax2 ) = plt . subplots ( nrows = 2 , figsize = ( 20 , 12 ), sharex = True ) ax1 . set_title ( \"Antes de MAT281\" ) ax1 . plot ( x , f1 , 'b' , label = 'Chile' , lw = 2.0 ) ax1 . plot ( x , g1 , 'g:' , label = 'OECD' , lw = 2.0 ) ax1 . legend ( loc = \"upper left\" ) ax2 . set_title ( \"Despues de MAT281\" ) ax2 . plot ( x , f2 , 'g:' , label = 'Chile' , lw = 2.0 ) ax2 . plot ( x , g2 , 'b' , label = 'OECD' , lw = 2.0 ) ax2 . legend () fig . show ()","title":"Utilizar la misma codificaci\u00f3n visual entre gr\u00e1ficos similares"},{"location":"lectures/data_manipulation/visualization/introduccion/#python-landscape","text":"Para empezar, PyViz es un sitio web que se dedica a ayudar a los usuarios a decidir dentro de las mejores herramientas de visualizaci\u00f3n open-source implementadas en Python, dependiendo de sus necesidades y objetivos. Mucho de lo que se menciona en esta secci\u00f3n est\u00e1 en detalle en la p\u00e1gina web del proyecto PyViz. Algunas de las librer\u00edas de visualizaci\u00f3n de Python m\u00e1s conocidas son: Este esquema es una adaptaci\u00f3n de uno presentado en la charla The Python Visualization Landscape realizada por Jake VanderPlas en la PyCon 2017. Cada una de estas librer\u00edas fue creada para satisfacer diferentes necesidades, algunas han ganado m\u00e1s adeptos que otras por uno u otro motivo. Tal como avanza la tecnolog\u00eda, estas librer\u00edas se actualizan o se crean nuevas, la importancia no recae en ser un experto en una, si no en saber adaptarse a las situaciones, tomar la mejor decicisi\u00f3n y escoger seg\u00fan nuestras necesidades y preferencias. Por ejemplo, matplotlib naci\u00f3 como una soluci\u00f3n para imitar los gr\u00e1ficos de MATLAB (puedes ver la historia completa aqu\u00ed ), manteniendo una sintaxis similar y con ello poder crear gr\u00e1ficos est\u00e1ticos de muy buen nivel. Debido al \u00e9xito de matplotlib en la comunidad, nacen librer\u00edas basadas ella. Algunos ejemplos son: seaborn se basa en matp\u013aotlib pero su nicho corresponde a las visualizaciones estad\u00edsticas. ggpy una suerte de copia a ggplot2 perteneciente al lenguaje de programaci\u00f3n R . networkx visualizaciones de grafos. pandas no es una librer\u00eda de visualizaci\u00f3n propiamente tal, pero utiliza a matplotplib como bakcned en los m\u00e9todos con tal de crear gr\u00e1ficos de manera muy r\u00e1pida, e.g. pandas.DataFrame.plot.bar() Por otro lado, con tal de crear visualizaciones interactivas aparecen librer\u00edas basadas en javascript , algunas de las m\u00e1s conocidas en Python son: bokeh tiene como objetivo proporcionar gr\u00e1ficos vers\u00e1tiles, elegantes e incluso interactivos, teniendo una gran performance con grandes datasets o incluso streaming de datos. plotly visualizaciones interactivas que en conjunto a Dash (de la misma empresa) permite crear aplicaciones webs, similar a shiny de R . D3.js a pesar de estar basado en javascript se ha ganado un lugar en el coraz\u00f3n de toda la comunidad, debido a la ilimitada cantidad de visualizaciones que son posibles de hacer, por ejemplo, la malla interactiva que hizo un estudiante de la UTFSM est\u00e1 hecha en D3.js . De las librer\u00edas m\u00e1s recientes est\u00e1 Altair , que consiste en visualizaciones declarativas (ya lo veremos en el pr\u00f3ximo laboratorio). Constru\u00edda sobre Vega-Lite , a su vez que est\u00e9 est\u00e1 sobre Vega y este finalmente sobre D3.js . Altair permite crear visualizaciones est\u00e1ticas e interactivas con pocas l\u00edneas de c\u00f3digo, sin embargo, al ser relativamente nueva, a\u00fan existen funcionalidades en desarrollo o que simplemente a\u00fan no existen en esta librer\u00eda pero en otras si.","title":"Python Landscape"},{"location":"lectures/data_manipulation/visualization/introduccion/#clasificacion","text":"En lo concierne a nosotros, una de las principales clasificaciones para estas librer\u00edas es si crean visualizaciones est\u00e1tica y/o interactivas . La interactividad es un plus que permite adentrarse en los datos en distintos niveles, si agregamos que ciertas librer\u00edas permiten crear widgets (algo as\u00ed como complementos a las visualizaciones) su potencial aumenta. Por ejemplo, un widget podr\u00eda ser un filtro que permita escoger un pa\u00eds; en una librer\u00eda est\u00e1tica tendr\u00edas que crear un gr\u00e1fico por cada pa\u00eds (o combinaci\u00f3n de pa\u00edses) lo cual no se hace escalable y c\u00f3modo para trabajar.","title":"Clasificaci\u00f3n"},{"location":"lectures/data_manipulation/visualization/introduccion/#spoilers","text":"Las pr\u00f3ximas clases se centrar\u00e1n en matplotlib y Seaborn , dado que son buenos exponentes de visualizaci\u00f3n imperativa y declarativa, respectivamente. Finalmente, siempre hay que tener en consideraci\u00f3n la manera en que se compartir\u00e1n las visualizaciones, por ejemplo, si es para un art\u00edculo cient\u00edfico bastar\u00eda que fuese de buena calidad y est\u00e1tico. Si es para una plataforma web es necesario que sea interactivo, aqu\u00ed es donde entran en juego los dashboards, que permiten la exploraci\u00f3n de datos de manera interactiva. En Python existen librer\u00edas como Dash o Panel , sin embargo, en el mundo empresarial se suele utilizar software dedicado a esto, como Power BI o Tableau .","title":"Spoilers"},{"location":"lectures/data_manipulation/visualization/introduction/","text":"Introducci\u00f3n \u00bfPor qu\u00e9 aprender sobre visualizaci\u00f3n? Porque un resultado no sirve si no puede comunicarse correctamente. Porque una buena visualizaci\u00f3n dista de ser una tarea trivial. Porque un ingenierio necesita producir excelentes gr\u00e1ficos (pero nadie ense\u00f1a c\u00f3mo). No es exageraci\u00f3n Primeras visualizaciones Campa\u00f1a de Napole\u00f3n a Mosc\u00fa (Charles Minard, 1889). Mapa del c\u00f3lera (John Snow, 1855). \u00bfPor qu\u00e9 utilizar gr\u00e1ficos para representar datos? El 70 % de los receptores sensoriales del cuerpo humano est\u00e1 dedicado a la visi\u00f3n. Cerebro ha sido entrenado evolutivamente para interpretar la informaci\u00f3n visual de manera masiva. \u201cThe eye and the visual cortex of the brain form a massively parallel processor that provides the highest bandwidth channel into human cognitive centers\u201d \u2014 Colin Ware, Information Visualization, 2004. Cuarteto de ANSCOMBE Considere los siguientes 4 conjuntos de datos. \u00bfQu\u00e9 puede decir de los datos? import numpy as np import pandas as pd import os import matplotlib.pyplot as plt % matplotlib inline df = pd . read_csv ( os . path . join ( \"data\" , \"anscombe.csv\" )) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 y1 x2 y2 x3 y3 x4 y4 0 10 8.04 10 9.14 10 7.46 8 6.58 1 8 6.95 8 8.14 8 6.77 8 5.76 2 13 7.58 13 8.74 13 12.74 8 7.71 3 9 8.81 9 8.77 9 7.11 8 8.84 4 11 8.33 11 9.26 11 7.81 8 8.47 5 14 9.96 14 8.10 14 8.84 8 7.04 6 6 7.24 6 6.13 6 6.08 8 5.25 7 4 4.26 4 3.10 4 5.39 19 12.50 8 12 10.84 12 9.13 12 8.15 8 5.56 9 7 4.82 7 7.26 7 6.42 8 7.91 10 5 5.68 5 4.74 5 5.73 8 6.89 df . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 y1 x2 y2 x3 y3 x4 y4 count 11.000000 11.000000 11.000000 11.000000 11.000000 11.000000 11.000000 11.000000 mean 9.000000 7.500909 9.000000 7.500909 9.000000 7.500000 9.000000 7.500909 std 3.316625 2.031568 3.316625 2.031657 3.316625 2.030424 3.316625 2.030579 min 4.000000 4.260000 4.000000 3.100000 4.000000 5.390000 8.000000 5.250000 25% 6.500000 6.315000 6.500000 6.695000 6.500000 6.250000 8.000000 6.170000 50% 9.000000 7.580000 9.000000 8.140000 9.000000 7.110000 8.000000 7.040000 75% 11.500000 8.570000 11.500000 8.950000 11.500000 7.980000 8.000000 8.190000 max 14.000000 10.840000 14.000000 9.260000 14.000000 12.740000 19.000000 12.500000 \u00bfPor qu\u00e9 es un ejemplo cl\u00e1sico? for i in range ( 1 , 4 + 1 ): x = df . loc [:, f \"x { i } \" ] . values y = df . loc [:, f \"y { i } \" ] . values slope , intercept = np . polyfit ( x , y , 1 ) print ( f \"Grupo { i } : \\n\\t Tiene pendiente { slope : .2f } e intercepto { intercept : .2f } . \\n \" ) Grupo 1: Tiene pendiente 0.50 e intercepto 3.00. Grupo 2: Tiene pendiente 0.50 e intercepto 3.00. Grupo 3: Tiene pendiente 0.50 e intercepto 3.00. Grupo 4: Tiene pendiente 0.50 e intercepto 3.00. groups = range ( 1 , 4 + 1 ) x_columns = [ col for col in df if \"x\" in col ] x_aux = np . arange ( df . loc [:, x_columns ] . values . min () - 1 , df . loc [:, x_columns ] . values . max () + 2 ) fig , axs = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 16 , 8 ), sharex = True , sharey = True ) fig . suptitle ( \"Cuarteto de Anscombe\" ) for i , ax in zip ( groups , axs . ravel ()): x = df . loc [:, f \"x { i } \" ] . values y = df . loc [:, f \"y { i } \" ] . values m , b = np . polyfit ( x , y , 1 ) ax . plot ( x , y , 'o' ) ax . plot ( x_aux , m * x_aux + b , 'r' , lw = 2.0 ) ax . set_title ( f \"Grupo { i } \" ) Teor\u00eda Sistema visual humano Buenas noticias Gr\u00e1ficos entregan informaci\u00f3n que la estad\u00edstica podr\u00eda no revelar. Despliegue visual es esencial para comprensi\u00f3n. Malas noticias La atenci\u00f3n es selectiva y puede ser f\u00e1cilmente enga\u00f1ada. La atenci\u00f3n es selectiva y puede ser f\u00e1cilmente enga\u00f1ada. Consejos generales Noah Illinsky, en su charla \"Cuatro pilatres de la visualizaci\u00f3n\" ( es , en ), presenta buenos consejos sobre c\u00f3mo realizar una correcta visualizaci\u00f3n: * Prop\u00f3sito * Informaci\u00f3n/Contenido * Codificaci\u00f3n/Estructura * Formato Es altamente aconsejable ver el video, pero en resumen: Prop\u00f3sito o p\u00fablico tiene que ver con para qui\u00e9n se est\u00e1 preparando la viz y que utilidad se le dar\u00e1. Es muy diferente preparar un gr\u00e1fico orientado a informaci\u00f3n y toma de decisiones. Informaci\u00f3n/Contenido se refiere a contar con la informaci\u00f3n que se desea mostrar, en el formato necesario para su procesamiento. Codificaci\u00f3n/Estructura tiene que ver con la selecci\u00f3n correcta de la codificaci\u00f3n y estructura de la informaci\u00f3n. Formato tiene que ver con la elecci\u00f3n de fuentes, colores, tama\u00f1os relativos, etc. Lo anterior indica que una visualizaci\u00f3n no es el resultado de unos datos. Una visualizaci\u00f3n se dise\u00f1a, se piensa, y luego se buscan fuentes de informaci\u00f3n apropiadas. Elementos para la creaci\u00f3n de una buena visualizaci\u00f3n Honestidad : representaciones visuales no deben enga\u00f1ar al observador. Priorizaci\u00f3n : dato m\u00e1s importante debe utilizar elemento de mejor percepci\u00f3n. Expresividad : datos deben utilizar elementos con atribuciones adecuadas. Consistencia : codificaci\u00f3n visual debe permitir reproducir datos. El principio b\u00e1sico a respetar es que a partir del gr\u00e1fico uno debe poder reobtener f\u00e1cilmente los datos originales. Honestidad El ojo humano no tiene la misma precisi\u00f3n al estimar distintas atribuciones: * Largo : Bien estimado y sin sesgo, con un factor multiplicativo de 0.9 a 1.1. * \u00c1rea : Subestimado y con sesgo, con un factor multiplicativo de 0.6 a 0.9. * Volumen : Muy subestimado y con sesgo, con un factor multiplicativo de 0.5 a 0.8. Resulta inadecuado realizar gr\u00e1ficos de datos utilizando \u00e1reas o vol\u00famenes si no queda claro la atribuci\u00f3n utilizada. Una pseudo-excepci\u00f3n la constituyen los pie-chart o gr\u00e1ficos circulares, porque el ojo humano distingue bien \u00e1ngulos y segmentos de c\u00edrculo, y porque es posible indicar los porcentajes respectivos. ## Example from https://matplotlib.org/3.1.1/gallery/pie_and_polar_charts/pie_features.html#sphx-glr-gallery-pie-and-polar-charts-pie-features-py # Pie chart, where the slices will be ordered and plotted counter-clockwise: labels = 'Frogs' , 'Hogs' , 'Dogs' , 'Logs' sizes = [ 15 , 30 , 45 , 10 ] explode = ( 0 , 0.1 , 0 , 0 ) # only \"explode\" the 2nd slice (i.e. 'Hogs') fig1 , ax1 = plt . subplots ( figsize = ( 8 , 8 )) ax1 . pie ( sizes , explode = explode , labels = labels , autopct = ' %1.1f%% ' , shadow = True , startangle = 90 ) ax1 . axis ( 'equal' ) # Equal aspect ratio ensures that pie is drawn as a circle. plt . show () Priorizaci\u00f3n Dato m\u00e1s importante debe utilizar elemento de mejor percepci\u00f3n. np . random . seed ( 42 ) N = 31 x = np . arange ( N ) y1 = 80 + 20 * x / N + 5 * np . random . rand ( N ) y2 = 75 + 25 * x / N + 5 * np . random . rand ( N ) fig , axs = plt . subplots ( 2 , 2 , sharex = True , sharey = True , figsize = ( 16 , 8 )) axs [ 0 ][ 0 ] . plot ( x , y1 , 'ok' ) axs [ 0 ][ 0 ] . plot ( x , y2 , 'sk' ) axs [ 0 ][ 1 ] . plot ( x , y1 , 'ob' ) axs [ 0 ][ 1 ] . plot ( x , y2 , 'or' ) axs [ 1 ][ 0 ] . plot ( x , y1 , 'ob' ) axs [ 1 ][ 0 ] . plot ( x , y2 , '*k' ) axs [ 1 ][ 1 ] . plot ( x , y1 , 'sr' ) axs [ 1 ][ 1 ] . plot ( x , y2 , 'ob' ) plt . show () Elementos de mejor percepci\u00f3n No todos los elementos tienen la misma percepci\u00f3n a nivel del sistema visual. En particular, el color y la forma son elementos preatentivos: un color distinto o una forma distinta se reconocen de manera no conciente. Ejemplos de elementos preatentivos. \u00bfEn que orden creen que el sistema visual humano puede estimar los siguientes atributos visuales: * Color * Pendiente * Largo * \u00c1ngulo * Posici\u00f3n * \u00c1rea * Volumen El sistema visual humano puede estimar con precisi\u00f3n siguientes atributos visuales: 1. Posici\u00f3n 2. Largo 3. Pendiente 4. \u00c1ngulo 5. \u00c1rea 6. Volumen 7. Color Utilice el atributo que se estima con mayor precisi\u00f3n cuando sea posible. Colormaps Puesto que la percepci\u00f3n del color tiene muy baja precisi\u00f3n, resulta inadecuado tratar de representar un valor num\u00e9rico con colores. * \u00bfQu\u00e9 diferencia num\u00e9rica existe entre el verde y el rojo? * \u00bfQue asociaci\u00f3n preexistente posee el color rojo, el amarillo y el verde? * \u00bfCon cu\u00e1nta precisi\u00f3n podemos distinguir valores en una escala de grises? Algunos ejemplos de colormaps import matplotlib.cm as cm from scipy.stats import multivariate_normal x , y = np . mgrid [ - 3 : 3 : .025 , - 2 : 2 : .025 ] pos = np . empty ( x . shape + ( 2 ,)) pos [:, :, 0 ] = x pos [:, :, 1 ] = y z1 = multivariate_normal . pdf ( pos , mean = [ - 1.0 , - 1.0 ], cov = [[ 1.0 , 0.0 ], [ 0.0 , 0.1 ]] ) z2 = multivariate_normal . pdf ( pos , mean = [ 1.0 , 1.0 ], cov = [[ 1.5 , 0.0 ], [ 0.0 , 0.5 ]] ) z = 10 * ( z1 - z2 ) fig , axs = plt . subplots ( 2 , 2 , figsize = ( 16 , 8 ), sharex = True , sharey = True ) cmaps = [ cm . rainbow , cm . autumn , cm . coolwarm , cm . gray ] for i , ax in zip ( range ( len ( cmaps )), axs . ravel ()): im = ax . imshow ( z , interpolation = 'bilinear' , origin = 'lower' , cmap = cmaps [ i ], extent = ( - 3 , 3 , - 2 , 2 )) fig . colorbar ( im , ax = ax ) fig . show () Consejo: evite mientras pueda los colormaps. Por ejemplo, utilizando contour plots. fig , axs = plt . subplots ( 2 , 2 , figsize = ( 20 , 12 ), sharex = True , sharey = True ) cmaps = [ cm . rainbow , cm . autumn , cm . coolwarm , cm . gray ] countour_styles = [ { \"cmap\" : cm . rainbow }, { \"cmap\" : cm . rainbow }, { \"colors\" : \"k\" , \"linestyles\" : \"solid\" }, { \"colors\" : \"k\" , \"linestyles\" : \"dashed\" }, ] for i , ax in zip ( range ( len ( cmaps )), axs . ravel ()): cs = ax . contour ( x , y , z , 11 , ** countour_styles [ i ]) if i > 0 : ax . clabel ( cs , fontsize = 9 , inline = 1 ) if i == 3 : ax . grid ( alpha = 0.5 ) fig . show () Sobre la Expresividad Mostrar los datos y s\u00f3lo los datos. Los datos deben utilizar elementos con atribuciones adecuadas: Not all data is born equal . Clasificaci\u00f3n de datos: * Datos Cuantitativos : Cuantificaci\u00f3n absoluta. * Cantidad de az\u00facar en fruta: 50 [gr/kg] * Operaciones =, \\(\\neq\\) , <, >, +, \u2212, * , / * Datos Posicionales : Cuantificaci\u00f3n relativa. * Fecha de cosecha: 1 Agosto 2014, 2 Agosto 2014. * Operaciones =, \\(\\neq\\) , <, >, +, \u2212 * Datos Ordinales : Orden sin cuantificaci\u00f3n. * Calidad de la Fruta: baja, media, alta, exportaci\u00f3n. * Operaciones =, \\(\\neq\\) , <, > * Datos Nominales : Nombres o clasificaciones * Frutas: manzana, pera, kiwi, ... * Operaciones \\(=\\) , \\(\\neq\\) Ejemplo: Terremotos. \u00bfQue tipos de datos tenemos? * Ciudad m\u00e1s pr\u00f3xima * A\u00f1o * Magnitud en escala Richter * Magnitud en escala Mercalli * Latitud * Longitud Contraejemplo: Compa\u00f1\u00edas de computadores. Compan\u00eda Procedencia MSI Taiwan Asus Taiwan Acer Taiwan HP EEUU Dell EEUU Apple EEUU Sony Japon Toshiba Japon Lenovo Hong Kong Samsung Corea del Sur brands = { \"MSI\" : \"Taiwan\" , \"Asus\" : \"Taiwan\" , \"Acer\" : \"Taiwan\" , \"HP\" : \"EEUU\" , \"Dell\" : \"EEUU\" , \"Apple\" : \"EEUU\" , \"Sony\" : \"Japon\" , \"Toshiba\" : \"Japon\" , \"Lenovo\" : \"Hong Kong\" , \"Samsung\" : \"Corea del Sur\" } C2N = { \"Taiwan\" : 1 , \"EEUU\" : 2 , \"Japon\" : 3 , \"Hong Kong\" : 4 , \"Corea del Sur\" : 7 } x = np . arange ( len ( brands . keys ())) y = np . array ([ C2N [ val ] for val in brands . values ()]) width = 0.35 # the width of the bars fig , ax = plt . subplots ( figsize = ( 16 , 8 )) rects1 = ax . bar ( x , y , width , color = 'r' ) # add some text for labels, title and axes ticks ax . set_xticks ( x + 0.5 * width ) ax . set_xticklabels ( brands . keys (), rotation = \"90\" ) ax . set_yticks ( list ( C2N . values ())) ax . set_yticklabels ( C2N . keys ()) plt . xlim ([ - 1 , len ( x ) + 1 ]) plt . ylim ([ - 1 , y . max () + 1 ]) plt . show () Clasificaci\u00f3n de datos: * Datos Cuantitativos : Cuantificaci\u00f3n absoluta. * Cantidad de az\u00facar en fruta: 50 [gr/kg] * Operaciones =, \\(\\neq\\) , <, >, +, \u2212, * , / * Utilizar posici\u00f3n, largo, pendiente o \u00e1ngulo Datos Posicionales : Cuantificaci\u00f3n relativa. * Fecha de cosecha: 1 Agosto 2014, 2 Agosto 2014. * Operaciones =, \\(\\neq\\) , <, >, +, \u2212 * Utilizar posici\u00f3n, largo, pendiente o \u00e1ngulo * Datos Ordinales : Orden sin cuantificaci\u00f3n. * Calidad de la Fruta: baja, media, alta, exportaci\u00f3n. * Operaciones =, \\(\\neq\\) , <, > * Utilizar marcadores diferenciados en forma o tama\u00f1o, o mapa de colores apropiado * Datos Nominales : Nombres o clasificaciones * Frutas: manzana, pera, kiwi, ... * Operaciones \\(=\\) , \\(\\neq\\) * Utilizar forma o color * Consistencia La codificaci\u00f3n visual debe permitir reproducir datos. Para ello debemos: * Graficar datos que sean comparables. * Utilizar ejes escalados adecuadamente. * Utilizar la misma codificaci\u00f3n visual entre gr\u00e1ficos similares. Utilizar ejes escalados adecuadamente. x = list ( range ( 1 , 13 )) y = 80 + 20 * np . random . rand ( 12 ) x_ticks = list ( \"EFMAMJJASOND\" ) fig , ( ax1 , ax2 ) = plt . subplots ( ncols = 2 , figsize = ( 20 , 8 )) ax1 . plot ( x , y , 'o-' ) ax1 . set_xticks ( x ) ax1 . set_xticklabels ( x_ticks ) ax1 . grid ( alpha = 0.5 ) ax2 . plot ( x , y , 'o-' ) ax2 . set_xticks ( x ) ax2 . set_xticklabels ( x_ticks ) ax2 . set_ylim ([ 0 , 110 ]) ax2 . grid ( alpha = 0.5 ) fig . show () Utilizar la misma codificaci\u00f3n visual entre gr\u00e1ficos similares x = np . linspace ( 0 , 1 , 50 ) f1 = x ** 2 + .2 * np . random . rand ( 50 ) g1 = x + .2 * np . random . rand ( 50 ) f2 = 0.5 - 0.2 * x + .2 * np . random . rand ( 50 ) g2 = x ** 3 + .2 * np . random . rand ( 50 ) fig , ( ax1 , ax2 ) = plt . subplots ( nrows = 2 , figsize = ( 20 , 12 ), sharex = True ) ax1 . set_title ( \"Antes de MAT281\" ) ax1 . plot ( x , f1 , 'b' , label = 'Chile' , lw = 2.0 ) ax1 . plot ( x , g1 , 'g:' , label = 'OECD' , lw = 2.0 ) ax1 . legend ( loc = \"upper left\" ) ax2 . set_title ( \"Despues de MAT281\" ) ax2 . plot ( x , f2 , 'g:' , label = 'Chile' , lw = 2.0 ) ax2 . plot ( x , g2 , 'b' , label = 'OECD' , lw = 2.0 ) ax2 . legend () fig . show () Python Landscape Para empezar, PyViz es un sitio web que se dedica a ayudar a los usuarios a decidir dentro de las mejores herramientas de visualizaci\u00f3n open-source implementadas en Python, dependiendo de sus necesidades y objetivos. Mucho de lo que se menciona en esta secci\u00f3n est\u00e1 en detalle en la p\u00e1gina web del proyecto PyViz. Algunas de las librer\u00edas de visualizaci\u00f3n de Python m\u00e1s conocidas son: Este esquema es una adaptaci\u00f3n de uno presentado en la charla The Python Visualization Landscape realizada por Jake VanderPlas en la PyCon 2017. Cada una de estas librer\u00edas fue creada para satisfacer diferentes necesidades, algunas han ganado m\u00e1s adeptos que otras por uno u otro motivo. Tal como avanza la tecnolog\u00eda, estas librer\u00edas se actualizan o se crean nuevas, la importancia no recae en ser un experto en una, si no en saber adaptarse a las situaciones, tomar la mejor decicisi\u00f3n y escoger seg\u00fan nuestras necesidades y preferencias. Por ejemplo, matplotlib naci\u00f3 como una soluci\u00f3n para imitar los gr\u00e1ficos de MATLAB (puedes ver la historia completa aqu\u00ed ), manteniendo una sintaxis similar y con ello poder crear gr\u00e1ficos est\u00e1ticos de muy buen nivel. Debido al \u00e9xito de matplotlib en la comunidad, nacen librer\u00edas basadas ella. Algunos ejemplos son: seaborn se basa en matp\u013aotlib pero su nicho corresponde a las visualizaciones estad\u00edsticas. ggpy una suerte de copia a ggplot2 perteneciente al lenguaje de programaci\u00f3n R . networkx visualizaciones de grafos. pandas no es una librer\u00eda de visualizaci\u00f3n propiamente tal, pero utiliza a matplotplib como bakcned en los m\u00e9todos con tal de crear gr\u00e1ficos de manera muy r\u00e1pida, e.g. pandas.DataFrame.plot.bar() Por otro lado, con tal de crear visualizaciones interactivas aparecen librer\u00edas basadas en javascript , algunas de las m\u00e1s conocidas en Python son: bokeh tiene como objetivo proporcionar gr\u00e1ficos vers\u00e1tiles, elegantes e incluso interactivos, teniendo una gran performance con grandes datasets o incluso streaming de datos. plotly visualizaciones interactivas que en conjunto a Dash (de la misma empresa) permite crear aplicaciones webs, similar a shiny de R . D3.js a pesar de estar basado en javascript se ha ganado un lugar en el coraz\u00f3n de toda la comunidad, debido a la ilimitada cantidad de visualizaciones que son posibles de hacer, por ejemplo, la malla interactiva que hizo un estudiante de la UTFSM est\u00e1 hecha en D3.js . De las librer\u00edas m\u00e1s recientes est\u00e1 Altair , que consiste en visualizaciones declarativas (ya lo veremos en el pr\u00f3ximo laboratorio). Constru\u00edda sobre Vega-Lite , a su vez que est\u00e9 est\u00e1 sobre Vega y este finalmente sobre D3.js . Altair permite crear visualizaciones est\u00e1ticas e interactivas con pocas l\u00edneas de c\u00f3digo, sin embargo, al ser relativamente nueva, a\u00fan existen funcionalidades en desarrollo o que simplemente a\u00fan no existen en esta librer\u00eda pero en otras si. Clasificaci\u00f3n En lo concierne a nosotros, una de las principales clasificaciones para estas librer\u00edas es si crean visualizaciones est\u00e1tica y/o interactivas . La interactividad es un plus que permite adentrarse en los datos en distintos niveles, si agregamos que ciertas librer\u00edas permiten crear widgets (algo as\u00ed como complementos a las visualizaciones) su potencial aumenta. Por ejemplo, un widget podr\u00eda ser un filtro que permita escoger un pa\u00eds; en una librer\u00eda est\u00e1tica tendr\u00edas que crear un gr\u00e1fico por cada pa\u00eds (o combinaci\u00f3n de pa\u00edses) lo cual no se hace escalable y c\u00f3modo para trabajar. Spoilers Las pr\u00f3ximas clases se centrar\u00e1n en matplotlib y Seaborn , dado que son buenos exponentes de visualizaci\u00f3n imperativa y declarativa, respectivamente. Finalmente, siempre hay que tener en consideraci\u00f3n la manera en que se compartir\u00e1n las visualizaciones, por ejemplo, si es para un art\u00edculo cient\u00edfico bastar\u00eda que fuese de buena calidad y est\u00e1tico. Si es para una plataforma web es necesario que sea interactivo, aqu\u00ed es donde entran en juego los dashboards, que permiten la exploraci\u00f3n de datos de manera interactiva. En Python existen librer\u00edas como Dash o Panel , sin embargo, en el mundo empresarial se suele utilizar software dedicado a esto, como Power BI o Tableau .","title":"Introducci\u00f3n"},{"location":"lectures/data_manipulation/visualization/introduction/#introduccion","text":"","title":"Introducci\u00f3n"},{"location":"lectures/data_manipulation/visualization/introduction/#por-que-aprender-sobre-visualizacion","text":"Porque un resultado no sirve si no puede comunicarse correctamente. Porque una buena visualizaci\u00f3n dista de ser una tarea trivial. Porque un ingenierio necesita producir excelentes gr\u00e1ficos (pero nadie ense\u00f1a c\u00f3mo). No es exageraci\u00f3n","title":"\u00bfPor qu\u00e9 aprender sobre visualizaci\u00f3n?"},{"location":"lectures/data_manipulation/visualization/introduction/#primeras-visualizaciones","text":"","title":"Primeras visualizaciones"},{"location":"lectures/data_manipulation/visualization/introduction/#campana-de-napoleon-a-moscu-charles-minard-1889","text":"","title":"Campa\u00f1a de Napole\u00f3n a Mosc\u00fa (Charles Minard, 1889)."},{"location":"lectures/data_manipulation/visualization/introduction/#mapa-del-colera-john-snow-1855","text":"","title":"Mapa del c\u00f3lera (John Snow, 1855)."},{"location":"lectures/data_manipulation/visualization/introduction/#por-que-utilizar-graficos-para-representar-datos","text":"El 70 % de los receptores sensoriales del cuerpo humano est\u00e1 dedicado a la visi\u00f3n. Cerebro ha sido entrenado evolutivamente para interpretar la informaci\u00f3n visual de manera masiva. \u201cThe eye and the visual cortex of the brain form a massively parallel processor that provides the highest bandwidth channel into human cognitive centers\u201d \u2014 Colin Ware, Information Visualization, 2004.","title":"\u00bfPor qu\u00e9 utilizar gr\u00e1ficos para representar datos?"},{"location":"lectures/data_manipulation/visualization/introduction/#cuarteto-de-anscombe","text":"Considere los siguientes 4 conjuntos de datos. \u00bfQu\u00e9 puede decir de los datos? import numpy as np import pandas as pd import os import matplotlib.pyplot as plt % matplotlib inline df = pd . read_csv ( os . path . join ( \"data\" , \"anscombe.csv\" )) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 y1 x2 y2 x3 y3 x4 y4 0 10 8.04 10 9.14 10 7.46 8 6.58 1 8 6.95 8 8.14 8 6.77 8 5.76 2 13 7.58 13 8.74 13 12.74 8 7.71 3 9 8.81 9 8.77 9 7.11 8 8.84 4 11 8.33 11 9.26 11 7.81 8 8.47 5 14 9.96 14 8.10 14 8.84 8 7.04 6 6 7.24 6 6.13 6 6.08 8 5.25 7 4 4.26 4 3.10 4 5.39 19 12.50 8 12 10.84 12 9.13 12 8.15 8 5.56 9 7 4.82 7 7.26 7 6.42 8 7.91 10 5 5.68 5 4.74 5 5.73 8 6.89 df . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 y1 x2 y2 x3 y3 x4 y4 count 11.000000 11.000000 11.000000 11.000000 11.000000 11.000000 11.000000 11.000000 mean 9.000000 7.500909 9.000000 7.500909 9.000000 7.500000 9.000000 7.500909 std 3.316625 2.031568 3.316625 2.031657 3.316625 2.030424 3.316625 2.030579 min 4.000000 4.260000 4.000000 3.100000 4.000000 5.390000 8.000000 5.250000 25% 6.500000 6.315000 6.500000 6.695000 6.500000 6.250000 8.000000 6.170000 50% 9.000000 7.580000 9.000000 8.140000 9.000000 7.110000 8.000000 7.040000 75% 11.500000 8.570000 11.500000 8.950000 11.500000 7.980000 8.000000 8.190000 max 14.000000 10.840000 14.000000 9.260000 14.000000 12.740000 19.000000 12.500000 \u00bfPor qu\u00e9 es un ejemplo cl\u00e1sico? for i in range ( 1 , 4 + 1 ): x = df . loc [:, f \"x { i } \" ] . values y = df . loc [:, f \"y { i } \" ] . values slope , intercept = np . polyfit ( x , y , 1 ) print ( f \"Grupo { i } : \\n\\t Tiene pendiente { slope : .2f } e intercepto { intercept : .2f } . \\n \" ) Grupo 1: Tiene pendiente 0.50 e intercepto 3.00. Grupo 2: Tiene pendiente 0.50 e intercepto 3.00. Grupo 3: Tiene pendiente 0.50 e intercepto 3.00. Grupo 4: Tiene pendiente 0.50 e intercepto 3.00. groups = range ( 1 , 4 + 1 ) x_columns = [ col for col in df if \"x\" in col ] x_aux = np . arange ( df . loc [:, x_columns ] . values . min () - 1 , df . loc [:, x_columns ] . values . max () + 2 ) fig , axs = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 16 , 8 ), sharex = True , sharey = True ) fig . suptitle ( \"Cuarteto de Anscombe\" ) for i , ax in zip ( groups , axs . ravel ()): x = df . loc [:, f \"x { i } \" ] . values y = df . loc [:, f \"y { i } \" ] . values m , b = np . polyfit ( x , y , 1 ) ax . plot ( x , y , 'o' ) ax . plot ( x_aux , m * x_aux + b , 'r' , lw = 2.0 ) ax . set_title ( f \"Grupo { i } \" )","title":"Cuarteto de ANSCOMBE"},{"location":"lectures/data_manipulation/visualization/introduction/#teoria","text":"","title":"Teor\u00eda"},{"location":"lectures/data_manipulation/visualization/introduction/#sistema-visual-humano","text":"","title":"Sistema visual humano"},{"location":"lectures/data_manipulation/visualization/introduction/#buenas-noticias","text":"Gr\u00e1ficos entregan informaci\u00f3n que la estad\u00edstica podr\u00eda no revelar. Despliegue visual es esencial para comprensi\u00f3n.","title":"Buenas noticias"},{"location":"lectures/data_manipulation/visualization/introduction/#malas-noticias","text":"La atenci\u00f3n es selectiva y puede ser f\u00e1cilmente enga\u00f1ada.","title":"Malas noticias"},{"location":"lectures/data_manipulation/visualization/introduction/#la-atencion-es-selectiva-y-puede-ser-facilmente-enganada","text":"","title":"La atenci\u00f3n es selectiva y puede ser f\u00e1cilmente enga\u00f1ada."},{"location":"lectures/data_manipulation/visualization/introduction/#consejos-generales","text":"Noah Illinsky, en su charla \"Cuatro pilatres de la visualizaci\u00f3n\" ( es , en ), presenta buenos consejos sobre c\u00f3mo realizar una correcta visualizaci\u00f3n: * Prop\u00f3sito * Informaci\u00f3n/Contenido * Codificaci\u00f3n/Estructura * Formato Es altamente aconsejable ver el video, pero en resumen: Prop\u00f3sito o p\u00fablico tiene que ver con para qui\u00e9n se est\u00e1 preparando la viz y que utilidad se le dar\u00e1. Es muy diferente preparar un gr\u00e1fico orientado a informaci\u00f3n y toma de decisiones. Informaci\u00f3n/Contenido se refiere a contar con la informaci\u00f3n que se desea mostrar, en el formato necesario para su procesamiento. Codificaci\u00f3n/Estructura tiene que ver con la selecci\u00f3n correcta de la codificaci\u00f3n y estructura de la informaci\u00f3n. Formato tiene que ver con la elecci\u00f3n de fuentes, colores, tama\u00f1os relativos, etc. Lo anterior indica que una visualizaci\u00f3n no es el resultado de unos datos. Una visualizaci\u00f3n se dise\u00f1a, se piensa, y luego se buscan fuentes de informaci\u00f3n apropiadas.","title":"Consejos generales"},{"location":"lectures/data_manipulation/visualization/introduction/#elementos-para-la-creacion-de-una-buena-visualizacion","text":"Honestidad : representaciones visuales no deben enga\u00f1ar al observador. Priorizaci\u00f3n : dato m\u00e1s importante debe utilizar elemento de mejor percepci\u00f3n. Expresividad : datos deben utilizar elementos con atribuciones adecuadas. Consistencia : codificaci\u00f3n visual debe permitir reproducir datos. El principio b\u00e1sico a respetar es que a partir del gr\u00e1fico uno debe poder reobtener f\u00e1cilmente los datos originales.","title":"Elementos para la creaci\u00f3n de una buena visualizaci\u00f3n"},{"location":"lectures/data_manipulation/visualization/introduction/#honestidad","text":"El ojo humano no tiene la misma precisi\u00f3n al estimar distintas atribuciones: * Largo : Bien estimado y sin sesgo, con un factor multiplicativo de 0.9 a 1.1. * \u00c1rea : Subestimado y con sesgo, con un factor multiplicativo de 0.6 a 0.9. * Volumen : Muy subestimado y con sesgo, con un factor multiplicativo de 0.5 a 0.8. Resulta inadecuado realizar gr\u00e1ficos de datos utilizando \u00e1reas o vol\u00famenes si no queda claro la atribuci\u00f3n utilizada. Una pseudo-excepci\u00f3n la constituyen los pie-chart o gr\u00e1ficos circulares, porque el ojo humano distingue bien \u00e1ngulos y segmentos de c\u00edrculo, y porque es posible indicar los porcentajes respectivos. ## Example from https://matplotlib.org/3.1.1/gallery/pie_and_polar_charts/pie_features.html#sphx-glr-gallery-pie-and-polar-charts-pie-features-py # Pie chart, where the slices will be ordered and plotted counter-clockwise: labels = 'Frogs' , 'Hogs' , 'Dogs' , 'Logs' sizes = [ 15 , 30 , 45 , 10 ] explode = ( 0 , 0.1 , 0 , 0 ) # only \"explode\" the 2nd slice (i.e. 'Hogs') fig1 , ax1 = plt . subplots ( figsize = ( 8 , 8 )) ax1 . pie ( sizes , explode = explode , labels = labels , autopct = ' %1.1f%% ' , shadow = True , startangle = 90 ) ax1 . axis ( 'equal' ) # Equal aspect ratio ensures that pie is drawn as a circle. plt . show ()","title":"Honestidad"},{"location":"lectures/data_manipulation/visualization/introduction/#priorizacion","text":"Dato m\u00e1s importante debe utilizar elemento de mejor percepci\u00f3n. np . random . seed ( 42 ) N = 31 x = np . arange ( N ) y1 = 80 + 20 * x / N + 5 * np . random . rand ( N ) y2 = 75 + 25 * x / N + 5 * np . random . rand ( N ) fig , axs = plt . subplots ( 2 , 2 , sharex = True , sharey = True , figsize = ( 16 , 8 )) axs [ 0 ][ 0 ] . plot ( x , y1 , 'ok' ) axs [ 0 ][ 0 ] . plot ( x , y2 , 'sk' ) axs [ 0 ][ 1 ] . plot ( x , y1 , 'ob' ) axs [ 0 ][ 1 ] . plot ( x , y2 , 'or' ) axs [ 1 ][ 0 ] . plot ( x , y1 , 'ob' ) axs [ 1 ][ 0 ] . plot ( x , y2 , '*k' ) axs [ 1 ][ 1 ] . plot ( x , y1 , 'sr' ) axs [ 1 ][ 1 ] . plot ( x , y2 , 'ob' ) plt . show ()","title":"Priorizaci\u00f3n"},{"location":"lectures/data_manipulation/visualization/introduction/#elementos-de-mejor-percepcion","text":"No todos los elementos tienen la misma percepci\u00f3n a nivel del sistema visual. En particular, el color y la forma son elementos preatentivos: un color distinto o una forma distinta se reconocen de manera no conciente. Ejemplos de elementos preatentivos. \u00bfEn que orden creen que el sistema visual humano puede estimar los siguientes atributos visuales: * Color * Pendiente * Largo * \u00c1ngulo * Posici\u00f3n * \u00c1rea * Volumen El sistema visual humano puede estimar con precisi\u00f3n siguientes atributos visuales: 1. Posici\u00f3n 2. Largo 3. Pendiente 4. \u00c1ngulo 5. \u00c1rea 6. Volumen 7. Color Utilice el atributo que se estima con mayor precisi\u00f3n cuando sea posible.","title":"Elementos de mejor percepci\u00f3n"},{"location":"lectures/data_manipulation/visualization/introduction/#colormaps","text":"Puesto que la percepci\u00f3n del color tiene muy baja precisi\u00f3n, resulta inadecuado tratar de representar un valor num\u00e9rico con colores. * \u00bfQu\u00e9 diferencia num\u00e9rica existe entre el verde y el rojo? * \u00bfQue asociaci\u00f3n preexistente posee el color rojo, el amarillo y el verde? * \u00bfCon cu\u00e1nta precisi\u00f3n podemos distinguir valores en una escala de grises? Algunos ejemplos de colormaps import matplotlib.cm as cm from scipy.stats import multivariate_normal x , y = np . mgrid [ - 3 : 3 : .025 , - 2 : 2 : .025 ] pos = np . empty ( x . shape + ( 2 ,)) pos [:, :, 0 ] = x pos [:, :, 1 ] = y z1 = multivariate_normal . pdf ( pos , mean = [ - 1.0 , - 1.0 ], cov = [[ 1.0 , 0.0 ], [ 0.0 , 0.1 ]] ) z2 = multivariate_normal . pdf ( pos , mean = [ 1.0 , 1.0 ], cov = [[ 1.5 , 0.0 ], [ 0.0 , 0.5 ]] ) z = 10 * ( z1 - z2 ) fig , axs = plt . subplots ( 2 , 2 , figsize = ( 16 , 8 ), sharex = True , sharey = True ) cmaps = [ cm . rainbow , cm . autumn , cm . coolwarm , cm . gray ] for i , ax in zip ( range ( len ( cmaps )), axs . ravel ()): im = ax . imshow ( z , interpolation = 'bilinear' , origin = 'lower' , cmap = cmaps [ i ], extent = ( - 3 , 3 , - 2 , 2 )) fig . colorbar ( im , ax = ax ) fig . show () Consejo: evite mientras pueda los colormaps. Por ejemplo, utilizando contour plots. fig , axs = plt . subplots ( 2 , 2 , figsize = ( 20 , 12 ), sharex = True , sharey = True ) cmaps = [ cm . rainbow , cm . autumn , cm . coolwarm , cm . gray ] countour_styles = [ { \"cmap\" : cm . rainbow }, { \"cmap\" : cm . rainbow }, { \"colors\" : \"k\" , \"linestyles\" : \"solid\" }, { \"colors\" : \"k\" , \"linestyles\" : \"dashed\" }, ] for i , ax in zip ( range ( len ( cmaps )), axs . ravel ()): cs = ax . contour ( x , y , z , 11 , ** countour_styles [ i ]) if i > 0 : ax . clabel ( cs , fontsize = 9 , inline = 1 ) if i == 3 : ax . grid ( alpha = 0.5 ) fig . show ()","title":"Colormaps"},{"location":"lectures/data_manipulation/visualization/introduction/#sobre-la-expresividad","text":"Mostrar los datos y s\u00f3lo los datos. Los datos deben utilizar elementos con atribuciones adecuadas: Not all data is born equal . Clasificaci\u00f3n de datos: * Datos Cuantitativos : Cuantificaci\u00f3n absoluta. * Cantidad de az\u00facar en fruta: 50 [gr/kg] * Operaciones =, \\(\\neq\\) , <, >, +, \u2212, * , / * Datos Posicionales : Cuantificaci\u00f3n relativa. * Fecha de cosecha: 1 Agosto 2014, 2 Agosto 2014. * Operaciones =, \\(\\neq\\) , <, >, +, \u2212 * Datos Ordinales : Orden sin cuantificaci\u00f3n. * Calidad de la Fruta: baja, media, alta, exportaci\u00f3n. * Operaciones =, \\(\\neq\\) , <, > * Datos Nominales : Nombres o clasificaciones * Frutas: manzana, pera, kiwi, ... * Operaciones \\(=\\) , \\(\\neq\\) Ejemplo: Terremotos. \u00bfQue tipos de datos tenemos? * Ciudad m\u00e1s pr\u00f3xima * A\u00f1o * Magnitud en escala Richter * Magnitud en escala Mercalli * Latitud * Longitud Contraejemplo: Compa\u00f1\u00edas de computadores. Compan\u00eda Procedencia MSI Taiwan Asus Taiwan Acer Taiwan HP EEUU Dell EEUU Apple EEUU Sony Japon Toshiba Japon Lenovo Hong Kong Samsung Corea del Sur brands = { \"MSI\" : \"Taiwan\" , \"Asus\" : \"Taiwan\" , \"Acer\" : \"Taiwan\" , \"HP\" : \"EEUU\" , \"Dell\" : \"EEUU\" , \"Apple\" : \"EEUU\" , \"Sony\" : \"Japon\" , \"Toshiba\" : \"Japon\" , \"Lenovo\" : \"Hong Kong\" , \"Samsung\" : \"Corea del Sur\" } C2N = { \"Taiwan\" : 1 , \"EEUU\" : 2 , \"Japon\" : 3 , \"Hong Kong\" : 4 , \"Corea del Sur\" : 7 } x = np . arange ( len ( brands . keys ())) y = np . array ([ C2N [ val ] for val in brands . values ()]) width = 0.35 # the width of the bars fig , ax = plt . subplots ( figsize = ( 16 , 8 )) rects1 = ax . bar ( x , y , width , color = 'r' ) # add some text for labels, title and axes ticks ax . set_xticks ( x + 0.5 * width ) ax . set_xticklabels ( brands . keys (), rotation = \"90\" ) ax . set_yticks ( list ( C2N . values ())) ax . set_yticklabels ( C2N . keys ()) plt . xlim ([ - 1 , len ( x ) + 1 ]) plt . ylim ([ - 1 , y . max () + 1 ]) plt . show () Clasificaci\u00f3n de datos: * Datos Cuantitativos : Cuantificaci\u00f3n absoluta. * Cantidad de az\u00facar en fruta: 50 [gr/kg] * Operaciones =, \\(\\neq\\) , <, >, +, \u2212, * , / * Utilizar posici\u00f3n, largo, pendiente o \u00e1ngulo Datos Posicionales : Cuantificaci\u00f3n relativa. * Fecha de cosecha: 1 Agosto 2014, 2 Agosto 2014. * Operaciones =, \\(\\neq\\) , <, >, +, \u2212 * Utilizar posici\u00f3n, largo, pendiente o \u00e1ngulo * Datos Ordinales : Orden sin cuantificaci\u00f3n. * Calidad de la Fruta: baja, media, alta, exportaci\u00f3n. * Operaciones =, \\(\\neq\\) , <, > * Utilizar marcadores diferenciados en forma o tama\u00f1o, o mapa de colores apropiado * Datos Nominales : Nombres o clasificaciones * Frutas: manzana, pera, kiwi, ... * Operaciones \\(=\\) , \\(\\neq\\) * Utilizar forma o color *","title":"Sobre la Expresividad"},{"location":"lectures/data_manipulation/visualization/introduction/#consistencia","text":"La codificaci\u00f3n visual debe permitir reproducir datos. Para ello debemos: * Graficar datos que sean comparables. * Utilizar ejes escalados adecuadamente. * Utilizar la misma codificaci\u00f3n visual entre gr\u00e1ficos similares.","title":"Consistencia"},{"location":"lectures/data_manipulation/visualization/introduction/#utilizar-ejes-escalados-adecuadamente","text":"x = list ( range ( 1 , 13 )) y = 80 + 20 * np . random . rand ( 12 ) x_ticks = list ( \"EFMAMJJASOND\" ) fig , ( ax1 , ax2 ) = plt . subplots ( ncols = 2 , figsize = ( 20 , 8 )) ax1 . plot ( x , y , 'o-' ) ax1 . set_xticks ( x ) ax1 . set_xticklabels ( x_ticks ) ax1 . grid ( alpha = 0.5 ) ax2 . plot ( x , y , 'o-' ) ax2 . set_xticks ( x ) ax2 . set_xticklabels ( x_ticks ) ax2 . set_ylim ([ 0 , 110 ]) ax2 . grid ( alpha = 0.5 ) fig . show ()","title":"Utilizar ejes escalados adecuadamente."},{"location":"lectures/data_manipulation/visualization/introduction/#utilizar-la-misma-codificacion-visual-entre-graficos-similares","text":"x = np . linspace ( 0 , 1 , 50 ) f1 = x ** 2 + .2 * np . random . rand ( 50 ) g1 = x + .2 * np . random . rand ( 50 ) f2 = 0.5 - 0.2 * x + .2 * np . random . rand ( 50 ) g2 = x ** 3 + .2 * np . random . rand ( 50 ) fig , ( ax1 , ax2 ) = plt . subplots ( nrows = 2 , figsize = ( 20 , 12 ), sharex = True ) ax1 . set_title ( \"Antes de MAT281\" ) ax1 . plot ( x , f1 , 'b' , label = 'Chile' , lw = 2.0 ) ax1 . plot ( x , g1 , 'g:' , label = 'OECD' , lw = 2.0 ) ax1 . legend ( loc = \"upper left\" ) ax2 . set_title ( \"Despues de MAT281\" ) ax2 . plot ( x , f2 , 'g:' , label = 'Chile' , lw = 2.0 ) ax2 . plot ( x , g2 , 'b' , label = 'OECD' , lw = 2.0 ) ax2 . legend () fig . show ()","title":"Utilizar la misma codificaci\u00f3n visual entre gr\u00e1ficos similares"},{"location":"lectures/data_manipulation/visualization/introduction/#python-landscape","text":"Para empezar, PyViz es un sitio web que se dedica a ayudar a los usuarios a decidir dentro de las mejores herramientas de visualizaci\u00f3n open-source implementadas en Python, dependiendo de sus necesidades y objetivos. Mucho de lo que se menciona en esta secci\u00f3n est\u00e1 en detalle en la p\u00e1gina web del proyecto PyViz. Algunas de las librer\u00edas de visualizaci\u00f3n de Python m\u00e1s conocidas son: Este esquema es una adaptaci\u00f3n de uno presentado en la charla The Python Visualization Landscape realizada por Jake VanderPlas en la PyCon 2017. Cada una de estas librer\u00edas fue creada para satisfacer diferentes necesidades, algunas han ganado m\u00e1s adeptos que otras por uno u otro motivo. Tal como avanza la tecnolog\u00eda, estas librer\u00edas se actualizan o se crean nuevas, la importancia no recae en ser un experto en una, si no en saber adaptarse a las situaciones, tomar la mejor decicisi\u00f3n y escoger seg\u00fan nuestras necesidades y preferencias. Por ejemplo, matplotlib naci\u00f3 como una soluci\u00f3n para imitar los gr\u00e1ficos de MATLAB (puedes ver la historia completa aqu\u00ed ), manteniendo una sintaxis similar y con ello poder crear gr\u00e1ficos est\u00e1ticos de muy buen nivel. Debido al \u00e9xito de matplotlib en la comunidad, nacen librer\u00edas basadas ella. Algunos ejemplos son: seaborn se basa en matp\u013aotlib pero su nicho corresponde a las visualizaciones estad\u00edsticas. ggpy una suerte de copia a ggplot2 perteneciente al lenguaje de programaci\u00f3n R . networkx visualizaciones de grafos. pandas no es una librer\u00eda de visualizaci\u00f3n propiamente tal, pero utiliza a matplotplib como bakcned en los m\u00e9todos con tal de crear gr\u00e1ficos de manera muy r\u00e1pida, e.g. pandas.DataFrame.plot.bar() Por otro lado, con tal de crear visualizaciones interactivas aparecen librer\u00edas basadas en javascript , algunas de las m\u00e1s conocidas en Python son: bokeh tiene como objetivo proporcionar gr\u00e1ficos vers\u00e1tiles, elegantes e incluso interactivos, teniendo una gran performance con grandes datasets o incluso streaming de datos. plotly visualizaciones interactivas que en conjunto a Dash (de la misma empresa) permite crear aplicaciones webs, similar a shiny de R . D3.js a pesar de estar basado en javascript se ha ganado un lugar en el coraz\u00f3n de toda la comunidad, debido a la ilimitada cantidad de visualizaciones que son posibles de hacer, por ejemplo, la malla interactiva que hizo un estudiante de la UTFSM est\u00e1 hecha en D3.js . De las librer\u00edas m\u00e1s recientes est\u00e1 Altair , que consiste en visualizaciones declarativas (ya lo veremos en el pr\u00f3ximo laboratorio). Constru\u00edda sobre Vega-Lite , a su vez que est\u00e9 est\u00e1 sobre Vega y este finalmente sobre D3.js . Altair permite crear visualizaciones est\u00e1ticas e interactivas con pocas l\u00edneas de c\u00f3digo, sin embargo, al ser relativamente nueva, a\u00fan existen funcionalidades en desarrollo o que simplemente a\u00fan no existen en esta librer\u00eda pero en otras si.","title":"Python Landscape"},{"location":"lectures/data_manipulation/visualization/introduction/#clasificacion","text":"En lo concierne a nosotros, una de las principales clasificaciones para estas librer\u00edas es si crean visualizaciones est\u00e1tica y/o interactivas . La interactividad es un plus que permite adentrarse en los datos en distintos niveles, si agregamos que ciertas librer\u00edas permiten crear widgets (algo as\u00ed como complementos a las visualizaciones) su potencial aumenta. Por ejemplo, un widget podr\u00eda ser un filtro que permita escoger un pa\u00eds; en una librer\u00eda est\u00e1tica tendr\u00edas que crear un gr\u00e1fico por cada pa\u00eds (o combinaci\u00f3n de pa\u00edses) lo cual no se hace escalable y c\u00f3modo para trabajar.","title":"Clasificaci\u00f3n"},{"location":"lectures/data_manipulation/visualization/introduction/#spoilers","text":"Las pr\u00f3ximas clases se centrar\u00e1n en matplotlib y Seaborn , dado que son buenos exponentes de visualizaci\u00f3n imperativa y declarativa, respectivamente. Finalmente, siempre hay que tener en consideraci\u00f3n la manera en que se compartir\u00e1n las visualizaciones, por ejemplo, si es para un art\u00edculo cient\u00edfico bastar\u00eda que fuese de buena calidad y est\u00e1tico. Si es para una plataforma web es necesario que sea interactivo, aqu\u00ed es donde entran en juego los dashboards, que permiten la exploraci\u00f3n de datos de manera interactiva. En Python existen librer\u00edas como Dash o Panel , sin embargo, en el mundo empresarial se suele utilizar software dedicado a esto, como Power BI o Tableau .","title":"Spoilers"},{"location":"lectures/data_manipulation/visualization/matplotlib/","text":"Matplotlib Introducci\u00f3n Matplotlib es una biblioteca para la generaci\u00f3n de gr\u00e1ficos a partir de datos contenidos en listas o arrays en el lenguaje de programaci\u00f3n Python y su extensi\u00f3n matem\u00e1tica NumPy. Proporciona una API, pylab, dise\u00f1ada para recordar a la de MATLAB . En matplotlib todo est\u00e1 organizado en una jerarqu\u00eda: En la parte superior se encuentra el m\u00f3dulo matplotlib.pyplot . En este nivel, se utilizan funciones simples para agregar elementos de trazado (l\u00edneas, im\u00e1genes, texto, etc.) a los ejes actuales en la figura actual. El siguiente nivel en la jerarqu\u00eda es el primer nivel de la interfaz orientada a objetos, en la que pyplot se usa solo para algunas funciones, como la creaci\u00f3n de figuras, y el usuario crea y realiza un seguimiento expl\u00edcito de los objetos de figuras y ejes. En este nivel, el usuario usa pyplot para crear figuras, y a trav\u00e9s de esas figuras, se pueden crear uno o m\u00e1s objetos de ejes. Componentes de un gr\u00e1fico Figure Es la visualizaci\u00f3n completa. Figure realiza un seguimiento de todos los Axes hijos y el Canvas . Una figura puede tener cualquier n\u00famero de Axes , pero para ser \u00fatil debe tener al menos uno. La forma m\u00e1s f\u00e1cil de crear una nueva Figure es con pyplot: fig = plt . figure () # an empty figure with no axes fig , ax_lst = plt . subplots ( 2 , 2 ) # a figure with a 2x2 grid of Axes Axes Esto es lo que se puede pensar como 'un gr\u00e1fico', es la regi\u00f3n de la imagen con el espacio de datos. Un Figure dada puede contener muchos Axes , pero un objeto Axe dado solo puede estar en un Figure . Axes contiene dos (o tres en el caso de 3D) objetos Axis que se ocupan de los l\u00edmites de datos. Cada Axe tiene un t\u00edtulo, una etiqueta para el eje horizonal y una etiqueta para el eje vertical. La clase Axes y sus funciones son el punto de entrada principal para trabajar con la interfaz orientada a objetos. Axis Corresponden a los ejes, algo as\u00ed como l\u00edneas rectas. Se encargan de establecer los l\u00edmites del gr\u00e1fico y generar los ticks (las marcas en el eje) y los ticklabels ( strings que etiquetan los ticks). Gr\u00e1fico a Gr\u00e1fico A continuaci\u00f3n, mostraremos un amplia gama de gr\u00e1ficos que pueden ser desplegados con Matplotlib . Lo primero ser\u00e1 cargar las librerias para este m\u00f3dulo. import numpy as np import matplotlib.pyplot as plt import matplotlib.cm as cm from scipy.stats import multivariate_normal from mpl_heatmap import heatmap , annotate_heatmap % matplotlib inline Gr\u00e1fico de l\u00edneas # grafico simple # datos x = np . linspace ( 0 , 2 , 100 ) # grafico # tamano del grafico fig = plt . figure ( figsize = ( 10 , 5 )) # graficar plt . plot ( x , # eje x x , # eje y label = 'linea' , # etiquetado color = \"black\" , # color linewidth = 1 # tamano de la curva ) plt . legend () # agregar etiquetado plt . title ( \"grafico simple\" ) # agregar titulo plt . xlabel ( 'x' ) # nombre eje x plt . ylabel ( 'y' ) # nombre eje y plt . grid () # agregar grillado plt . show () # mostrar grafico # grafico compuesto # datos x = np . linspace ( 0 , 2 , 100 ) # grafico # tamano del grafico fig = plt . figure ( figsize = ( 10 , 5 )) # graficar # a) lineal plt . plot ( x , # eje x x , # eje y label = 'linea' , # etiquetado color = \"black\" , # color linewidth = 1 # tamano de la curva ) # b) cuadratica plt . plot ( x , # eje x x ** 2 , # eje y label = 'cuadratica' , # etiquetado color = \"b\" , # color linewidth = 1 # tamano de la curva ) # c) cubica plt . plot ( x , # eje x x ** 3 , # eje y label = 'cubica' , # etiquetado color = \"r\" , # color linewidth = 1 # tamano de la curva ) plt . legend () # agregar etiquetado plt . title ( \"grafico compuesto\" ) # agregar titulo plt . xlabel ( 'x' ) # nombre eje x plt . ylabel ( 'y' ) # nombre eje y plt . grid () # agregar grillado plt . show () # mostrar grafico \u00bfCu\u00e1ndo utilizar gr\u00e1fico de l\u00edneas? * x: Debe ser datos del tipo ordinal o cuantitativo. * y: Debe ser datos de tipo ordinal, posicional o cuantitativo. Gr\u00e1fico de Barras # datos np . random . seed ( 0 ) # fijar semilla people = ( 'Tom' , 'Dick' , 'Harry' , 'Slim' , 'Jim' ) y_pos = np . arange ( len ( people )) performance = 3 + 10 * np . random . rand ( len ( people )) error = np . random . rand ( len ( people )) # grafico fig = plt . figure ( figsize = ( 10 , 5 )) plt . bar ( y_pos , # eje x performance , # eje y yerr = error , # # error mostrado en eje y align = 'center' , # centrar nombre eje x color = \"blue\" , # color alpha = 0.6 # intensidad del color ) plt . xticks ( y_pos , people ) plt . xlabel ( 'People' ) plt . show () Ahora para realizar el mismo gr\u00e1fico pero con los ejes invertidos, se debe graficar con plt.barh # datos np . random . seed ( 0 ) # fijar semilla people = ( 'Tom' , 'Dick' , 'Harry' , 'Slim' , 'Jim' ) y_pos = np . arange ( len ( people )) performance = 3 + 10 * np . random . rand ( len ( people )) error = np . random . rand ( len ( people )) # grafico fig = plt . figure ( figsize = ( 10 , 5 )) plt . barh ( y_pos , # eje x performance , # eje y xerr = error , # error mostrado en eje x align = 'center' , # centrar nombre eje y color = \"blue\" , # color alpha = 0.4 # intensidad del color ) plt . yticks ( y_pos , people ) plt . xlabel ( 'People' ) plt . show () Ahora, si queremos poner ambos gr\u00e1ficos en una sola vista, debemos ejecutar la siguiente rutina: # datos np . random . seed ( 0 ) # fijar semilla people = ( 'Tom' , 'Dick' , 'Harry' , 'Slim' , 'Jim' ) y_pos = np . arange ( len ( people )) performance = 3 + 10 * np . random . rand ( len ( people )) error = np . random . rand ( len ( people )) # grafico fig = plt . figure ( figsize = ( 15 , 5 )) # ventana # grafico lado izquierdo plt . subplot ( 1 , 2 , 1 ) # sub-ventana plt . barh ( y_pos , performance , xerr = error , align = 'center' , color = \"blue\" , alpha = 0.4 ) plt . yticks ( y_pos , people ) plt . xlabel ( 'Performance' ) # grafico lado derecho plt . subplot ( 1 , 2 , 2 ) # sub-ventana plt . bar ( y_pos , performance , yerr = error , align = 'center' , color = \"blue\" , alpha = 0.6 ) plt . xticks ( y_pos , people ) plt . xlabel ( 'People' ) plt . ylabel ( 'Performance' ) plt . show () \u00bfCu\u00e1ndo utilizar gr\u00e1fico de barras? * x: Debe ser datos del tipo nominal o ordinal. * y: Debe ser datos de tipo ordinal, posicional o cuantitativo. Evitar: gr\u00e1fico de nominal vs nominal. Scatter Plot # datos np . random . seed ( 42 ) x = np . arange ( 0.0 , 50.0 , 2.0 ) y = x ** 1.3 + np . random . rand ( * x . shape ) * 30.0 s = np . random . rand ( * x . shape ) * 800 + 500 # grafico fig = plt . figure ( figsize = ( 10 , 5 )) # ventana plt . scatter ( x , # eje x y , # eje y s , # tamano de los puntos c = \"g\" , # color alpha = 0.7 , # intensidad color marker = r '$\\clubsuit$' , # forma de los puntos label = \"Suerte\" # etiquetdo fijando posicion ) plt . xlabel ( \"Duende\" ) plt . ylabel ( \"Oro\" ) plt . legend ( loc = 'upper left' ) plt . show () Ejercicio : Realizar un gr\u00e1fico que cumpla las siguientes restricciones: Valores de los ejes: \\(x,y \\in [0,1]\\) Gr\u00e1fico de l\u00ednea de una circunferencia de radio \\(r_0\\) Los puntos que se encuentren dentro de la circunferencia tengan forma de c\u00edrculos con color naranja y aquellos utnos que se encuentren fuera tengan forma de tri\u00e1ngulos con color azul. Los puntos graficados deben estar escalado por tama\u00f1o. # datos N = 100 r0 = 0.6 # radio inicial x = 0.9 * np . random . rand ( N ) # puntos aleatorios eje x y = 0.9 * np . random . rand ( N ) # puntos aleatorios eje y r = np . sqrt ( x ** 2 + y ** 2 ) # radio sacado de los puntos area = np . pi * ( 10 * np . random . rand ( N )) ** 2 # tamano area1 = np . ma . masked_where ( r < r0 , area ) # dentro del radio objetivo area2 = np . ma . masked_where ( r >= r0 , area ) # fuera del radio objetivo # grafico # a) circunferencia plt . figure ( figsize = ( 8 , 8 )) theta = np . arange ( 0 , np . pi / 2 , 0.01 ) plt . plot ( r0 * np . cos ( theta ), r0 * np . sin ( theta ), \"k--\" , lw = 1.0 ) # b) figuras dentro de la circuenferencia sc1 = plt . scatter ( x , y , s = area2 , marker = 'o' , c = \"orange\" , label = \"interior\" ) # b) figuras fuera de la circuenferencia sc2 = plt . scatter ( x , y , s = area1 , marker = '^' , c = \"b\" , label = \"exterior\" ) plt . xlabel ( \"x\" ) plt . ylabel ( \"y\" ) plt . legend ( loc = 'upper left' ) plt . show () \u00bfCu\u00e1ndo utilizar scatter plot? * x: Dato del tipo posicional o cuantitativo. * y: Dato del tipo posicional o cuantitativo. * z: Dato del tipo nominal u ordinal (opcional) OBSERVACION : Si hay pocos puntos, tambi\u00e9n puede usarse para z datos de tipo posicional o cuantitativo. Mapa de calor # datos vegetables = [ \"cucumber\" , \"tomato\" , \"lettuce\" , \"asparagus\" , \"potato\" , \"wheat\" , \"barley\" ] farmers = [ \"Farmer Joe\" , \"Upland Bros.\" , \"Smith Gardening\" , \"Agrifun\" , \"Organiculture\" , \"BioGoods Ltd.\" , \"Cornylee Corp.\" ] harvest = np . array ([[ 0.8 , 2.4 , 2.5 , 3.9 , 0.0 , 4.0 , 0.0 ], [ 2.4 , 0.0 , 4.0 , 1.0 , 2.7 , 0.0 , 0.0 ], [ 1.1 , 2.4 , 0.8 , 4.3 , 1.9 , 4.4 , 0.0 ], [ 0.6 , 0.0 , 0.3 , 0.0 , 3.1 , 0.0 , 0.0 ], [ 0.7 , 1.7 , 0.6 , 2.6 , 2.2 , 6.2 , 0.0 ], [ 1.3 , 1.2 , 0.0 , 0.0 , 0.0 , 3.2 , 5.1 ], [ 0.1 , 2.0 , 0.0 , 1.4 , 0.0 , 1.9 , 6.3 ]]) # graficos fig , ax = plt . subplots ( figsize = ( 10 , 10 )) im , cbar = heatmap ( harvest , # valores vegetables , # filas farmers , # columnas ax = ax , # ventana cmap = \"YlGn\" , # gama de colores cbarlabel = \"harvest [t/year]\" # nombre barra de colores ) texts = annotate_heatmap ( im , valfmt = \" {x:.1f} t\" ) fig . tight_layout () plt . show () Otros gr\u00e1ficos de inter\u00e9s Gr\u00e1fico de Barra de Error # datos x = np . arange ( 0.1 , 4 , 0.5 ) y = np . exp ( - x ) # graficos fig , ( ax1 , ax2 ) = plt . subplots ( ncols = 2 , figsize = ( 20 , 10 )) x_error = 0.1 + 0.2 * np . random . rand ( len ( x )) ax1 . errorbar ( x , y , xerr = x_error ) y_error = 0.1 + 0.2 * np . random . rand ( len ( x )) ax2 . errorbar ( x , y , yerr = y_error ) fig . show () /home/falfaro/.cache/pypoetry/virtualenvs/pymessi-xyyw3p3f-py3.6/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure. # This is added back by InteractiveShellApp.init_path() \u00bfCu\u00e1ndo utilizar gr\u00e1fico de barra de error? * x: Dato del tipo posicional o cuantitativo. * y: Dato del tipo posicional o cuantitativo. * z: Dato del tipo posicional o cuantitativo. Los valores de z tienen que tener las mismas unidades y. Countor Plot # datos x , y = np . mgrid [ - 3 : 3 : .025 , - 2 : 2 : .025 ] pos = np . empty ( x . shape + ( 2 ,)) pos [:, :, 0 ] = x pos [:, :, 1 ] = y z1 = multivariate_normal . pdf ( pos , mean = [ - 1.0 , - 1.0 ], cov = [[ 1.0 , 0.0 ], [ 0.0 , 0.1 ]] ) z2 = multivariate_normal . pdf ( pos , mean = [ 1.0 , 1.0 ], cov = [[ 1.5 , 0.0 ], [ 0.0 , 0.5 ]] ) z = 10 * ( z1 - z2 ) # grafico fig , axs = plt . subplots ( ncols = 2 , figsize = ( 20 , 10 ), sharex = True , sharey = True ) cmaps = [ cm . rainbow , cm . autumn , cm . coolwarm , cm . gray ] countour_styles = [ { \"colors\" : \"k\" , \"linestyles\" : \"solid\" }, { \"colors\" : \"k\" , \"linestyles\" : \"dashed\" }, ] for i , ax in zip ( range ( len ( cmaps )), axs . ravel ()): cs = ax . contour ( x , y , z , 11 , ** countour_styles [ i ]) if i > 0 : ax . clabel ( cs , fontsize = 9 , inline = 1 ) ax . grid ( alpha = 0.5 ) fig . show () /home/falfaro/.cache/pypoetry/virtualenvs/pymessi-xyyw3p3f-py3.6/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure. \u00bfCu\u00e1ndo se debe utiliar countour plot? x: Dato del tipo posicional o cuantitativo. y: Dato de tipo posicional o cuantitativo. z: Dato de tipo posicional o cuantitativo. OBSERVACION : Se debe tener suficiente densidad/regularidad de puntos como para poder obtener superficies de nivel. Campos de Vectores \u00bfPorqu\u00e9 se llama quiver al campo de vectores en ingl\u00e9s? def my_vector_field (): \"\"\" You can even define a new function. \"\"\" X , Y = np . meshgrid ( np . arange ( 0 , 2 * np . pi , .2 ), np . arange ( 0 , 2 * np . pi , .2 )) U = np . cos ( X ) V = np . sin ( Y ) fig , ( ax1 , ax2 ) = plt . subplots ( ncols = 2 , figsize = ( 20 , 10 )) Q1 = ax1 . quiver ( U , V ) qk1 = ax1 . quiverkey ( Q1 , 0.5 , 0.92 , 2 , r '$2 \\frac {m}{s} $' , labelpos = 'W' , fontproperties = { 'weight' : 'bold' } ) Q2 = ax2 . quiver ( X [:: 3 , :: 3 ], Y [:: 3 , :: 3 ], U [:: 3 , :: 3 ], V [:: 3 , :: 3 ], pivot = 'mid' , color = 'r' , units = 'inches' ) qk2 = ax2 . quiverkey ( Q2 , 0.5 , 0.03 , 1 , r '$1 \\frac {m}{s} $' , fontproperties = { 'weight' : 'bold' } ) ax2 . plot ( X [:: 3 , :: 3 ], Y [:: 3 , :: 3 ], 'k.' ) ax2 . set_title ( \"pivot='mid'; every third arrow; units='inches'\" ) fig . show () my_vector_field () /home/falfaro/.cache/pypoetry/virtualenvs/pymessi-xyyw3p3f-py3.6/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure. \u00bfCu\u00e1ndo utilizar campos de vectores? x: Debe ser datos del tipo posicional o cuantitativo. y: Debe ser datos de tipo posicional o cuantitativo. z: Pendiente debe ser dato de tipo posicional o cuantitativo. Evitar: gr\u00e1fico de campo de vectores si no es posible la interpretaci\u00f3n correspondiente. Referencia Gallery-matplotlib","title":"Matplotlib"},{"location":"lectures/data_manipulation/visualization/matplotlib/#matplotlib","text":"","title":"Matplotlib"},{"location":"lectures/data_manipulation/visualization/matplotlib/#introduccion","text":"Matplotlib es una biblioteca para la generaci\u00f3n de gr\u00e1ficos a partir de datos contenidos en listas o arrays en el lenguaje de programaci\u00f3n Python y su extensi\u00f3n matem\u00e1tica NumPy. Proporciona una API, pylab, dise\u00f1ada para recordar a la de MATLAB . En matplotlib todo est\u00e1 organizado en una jerarqu\u00eda: En la parte superior se encuentra el m\u00f3dulo matplotlib.pyplot . En este nivel, se utilizan funciones simples para agregar elementos de trazado (l\u00edneas, im\u00e1genes, texto, etc.) a los ejes actuales en la figura actual. El siguiente nivel en la jerarqu\u00eda es el primer nivel de la interfaz orientada a objetos, en la que pyplot se usa solo para algunas funciones, como la creaci\u00f3n de figuras, y el usuario crea y realiza un seguimiento expl\u00edcito de los objetos de figuras y ejes. En este nivel, el usuario usa pyplot para crear figuras, y a trav\u00e9s de esas figuras, se pueden crear uno o m\u00e1s objetos de ejes.","title":"Introducci\u00f3n"},{"location":"lectures/data_manipulation/visualization/matplotlib/#componentes-de-un-grafico","text":"","title":"Componentes de un gr\u00e1fico"},{"location":"lectures/data_manipulation/visualization/matplotlib/#figure","text":"Es la visualizaci\u00f3n completa. Figure realiza un seguimiento de todos los Axes hijos y el Canvas . Una figura puede tener cualquier n\u00famero de Axes , pero para ser \u00fatil debe tener al menos uno. La forma m\u00e1s f\u00e1cil de crear una nueva Figure es con pyplot: fig = plt . figure () # an empty figure with no axes fig , ax_lst = plt . subplots ( 2 , 2 ) # a figure with a 2x2 grid of Axes","title":"Figure"},{"location":"lectures/data_manipulation/visualization/matplotlib/#axes","text":"Esto es lo que se puede pensar como 'un gr\u00e1fico', es la regi\u00f3n de la imagen con el espacio de datos. Un Figure dada puede contener muchos Axes , pero un objeto Axe dado solo puede estar en un Figure . Axes contiene dos (o tres en el caso de 3D) objetos Axis que se ocupan de los l\u00edmites de datos. Cada Axe tiene un t\u00edtulo, una etiqueta para el eje horizonal y una etiqueta para el eje vertical. La clase Axes y sus funciones son el punto de entrada principal para trabajar con la interfaz orientada a objetos.","title":"Axes"},{"location":"lectures/data_manipulation/visualization/matplotlib/#axis","text":"Corresponden a los ejes, algo as\u00ed como l\u00edneas rectas. Se encargan de establecer los l\u00edmites del gr\u00e1fico y generar los ticks (las marcas en el eje) y los ticklabels ( strings que etiquetan los ticks).","title":"Axis"},{"location":"lectures/data_manipulation/visualization/matplotlib/#grafico-a-grafico","text":"A continuaci\u00f3n, mostraremos un amplia gama de gr\u00e1ficos que pueden ser desplegados con Matplotlib . Lo primero ser\u00e1 cargar las librerias para este m\u00f3dulo. import numpy as np import matplotlib.pyplot as plt import matplotlib.cm as cm from scipy.stats import multivariate_normal from mpl_heatmap import heatmap , annotate_heatmap % matplotlib inline","title":"Gr\u00e1fico a Gr\u00e1fico"},{"location":"lectures/data_manipulation/visualization/matplotlib/#grafico-de-lineas","text":"# grafico simple # datos x = np . linspace ( 0 , 2 , 100 ) # grafico # tamano del grafico fig = plt . figure ( figsize = ( 10 , 5 )) # graficar plt . plot ( x , # eje x x , # eje y label = 'linea' , # etiquetado color = \"black\" , # color linewidth = 1 # tamano de la curva ) plt . legend () # agregar etiquetado plt . title ( \"grafico simple\" ) # agregar titulo plt . xlabel ( 'x' ) # nombre eje x plt . ylabel ( 'y' ) # nombre eje y plt . grid () # agregar grillado plt . show () # mostrar grafico # grafico compuesto # datos x = np . linspace ( 0 , 2 , 100 ) # grafico # tamano del grafico fig = plt . figure ( figsize = ( 10 , 5 )) # graficar # a) lineal plt . plot ( x , # eje x x , # eje y label = 'linea' , # etiquetado color = \"black\" , # color linewidth = 1 # tamano de la curva ) # b) cuadratica plt . plot ( x , # eje x x ** 2 , # eje y label = 'cuadratica' , # etiquetado color = \"b\" , # color linewidth = 1 # tamano de la curva ) # c) cubica plt . plot ( x , # eje x x ** 3 , # eje y label = 'cubica' , # etiquetado color = \"r\" , # color linewidth = 1 # tamano de la curva ) plt . legend () # agregar etiquetado plt . title ( \"grafico compuesto\" ) # agregar titulo plt . xlabel ( 'x' ) # nombre eje x plt . ylabel ( 'y' ) # nombre eje y plt . grid () # agregar grillado plt . show () # mostrar grafico \u00bfCu\u00e1ndo utilizar gr\u00e1fico de l\u00edneas? * x: Debe ser datos del tipo ordinal o cuantitativo. * y: Debe ser datos de tipo ordinal, posicional o cuantitativo.","title":"Gr\u00e1fico de l\u00edneas"},{"location":"lectures/data_manipulation/visualization/matplotlib/#grafico-de-barras","text":"# datos np . random . seed ( 0 ) # fijar semilla people = ( 'Tom' , 'Dick' , 'Harry' , 'Slim' , 'Jim' ) y_pos = np . arange ( len ( people )) performance = 3 + 10 * np . random . rand ( len ( people )) error = np . random . rand ( len ( people )) # grafico fig = plt . figure ( figsize = ( 10 , 5 )) plt . bar ( y_pos , # eje x performance , # eje y yerr = error , # # error mostrado en eje y align = 'center' , # centrar nombre eje x color = \"blue\" , # color alpha = 0.6 # intensidad del color ) plt . xticks ( y_pos , people ) plt . xlabel ( 'People' ) plt . show () Ahora para realizar el mismo gr\u00e1fico pero con los ejes invertidos, se debe graficar con plt.barh # datos np . random . seed ( 0 ) # fijar semilla people = ( 'Tom' , 'Dick' , 'Harry' , 'Slim' , 'Jim' ) y_pos = np . arange ( len ( people )) performance = 3 + 10 * np . random . rand ( len ( people )) error = np . random . rand ( len ( people )) # grafico fig = plt . figure ( figsize = ( 10 , 5 )) plt . barh ( y_pos , # eje x performance , # eje y xerr = error , # error mostrado en eje x align = 'center' , # centrar nombre eje y color = \"blue\" , # color alpha = 0.4 # intensidad del color ) plt . yticks ( y_pos , people ) plt . xlabel ( 'People' ) plt . show () Ahora, si queremos poner ambos gr\u00e1ficos en una sola vista, debemos ejecutar la siguiente rutina: # datos np . random . seed ( 0 ) # fijar semilla people = ( 'Tom' , 'Dick' , 'Harry' , 'Slim' , 'Jim' ) y_pos = np . arange ( len ( people )) performance = 3 + 10 * np . random . rand ( len ( people )) error = np . random . rand ( len ( people )) # grafico fig = plt . figure ( figsize = ( 15 , 5 )) # ventana # grafico lado izquierdo plt . subplot ( 1 , 2 , 1 ) # sub-ventana plt . barh ( y_pos , performance , xerr = error , align = 'center' , color = \"blue\" , alpha = 0.4 ) plt . yticks ( y_pos , people ) plt . xlabel ( 'Performance' ) # grafico lado derecho plt . subplot ( 1 , 2 , 2 ) # sub-ventana plt . bar ( y_pos , performance , yerr = error , align = 'center' , color = \"blue\" , alpha = 0.6 ) plt . xticks ( y_pos , people ) plt . xlabel ( 'People' ) plt . ylabel ( 'Performance' ) plt . show () \u00bfCu\u00e1ndo utilizar gr\u00e1fico de barras? * x: Debe ser datos del tipo nominal o ordinal. * y: Debe ser datos de tipo ordinal, posicional o cuantitativo. Evitar: gr\u00e1fico de nominal vs nominal.","title":"Gr\u00e1fico de Barras"},{"location":"lectures/data_manipulation/visualization/matplotlib/#scatter-plot","text":"# datos np . random . seed ( 42 ) x = np . arange ( 0.0 , 50.0 , 2.0 ) y = x ** 1.3 + np . random . rand ( * x . shape ) * 30.0 s = np . random . rand ( * x . shape ) * 800 + 500 # grafico fig = plt . figure ( figsize = ( 10 , 5 )) # ventana plt . scatter ( x , # eje x y , # eje y s , # tamano de los puntos c = \"g\" , # color alpha = 0.7 , # intensidad color marker = r '$\\clubsuit$' , # forma de los puntos label = \"Suerte\" # etiquetdo fijando posicion ) plt . xlabel ( \"Duende\" ) plt . ylabel ( \"Oro\" ) plt . legend ( loc = 'upper left' ) plt . show () Ejercicio : Realizar un gr\u00e1fico que cumpla las siguientes restricciones: Valores de los ejes: \\(x,y \\in [0,1]\\) Gr\u00e1fico de l\u00ednea de una circunferencia de radio \\(r_0\\) Los puntos que se encuentren dentro de la circunferencia tengan forma de c\u00edrculos con color naranja y aquellos utnos que se encuentren fuera tengan forma de tri\u00e1ngulos con color azul. Los puntos graficados deben estar escalado por tama\u00f1o. # datos N = 100 r0 = 0.6 # radio inicial x = 0.9 * np . random . rand ( N ) # puntos aleatorios eje x y = 0.9 * np . random . rand ( N ) # puntos aleatorios eje y r = np . sqrt ( x ** 2 + y ** 2 ) # radio sacado de los puntos area = np . pi * ( 10 * np . random . rand ( N )) ** 2 # tamano area1 = np . ma . masked_where ( r < r0 , area ) # dentro del radio objetivo area2 = np . ma . masked_where ( r >= r0 , area ) # fuera del radio objetivo # grafico # a) circunferencia plt . figure ( figsize = ( 8 , 8 )) theta = np . arange ( 0 , np . pi / 2 , 0.01 ) plt . plot ( r0 * np . cos ( theta ), r0 * np . sin ( theta ), \"k--\" , lw = 1.0 ) # b) figuras dentro de la circuenferencia sc1 = plt . scatter ( x , y , s = area2 , marker = 'o' , c = \"orange\" , label = \"interior\" ) # b) figuras fuera de la circuenferencia sc2 = plt . scatter ( x , y , s = area1 , marker = '^' , c = \"b\" , label = \"exterior\" ) plt . xlabel ( \"x\" ) plt . ylabel ( \"y\" ) plt . legend ( loc = 'upper left' ) plt . show () \u00bfCu\u00e1ndo utilizar scatter plot? * x: Dato del tipo posicional o cuantitativo. * y: Dato del tipo posicional o cuantitativo. * z: Dato del tipo nominal u ordinal (opcional) OBSERVACION : Si hay pocos puntos, tambi\u00e9n puede usarse para z datos de tipo posicional o cuantitativo.","title":"Scatter Plot"},{"location":"lectures/data_manipulation/visualization/matplotlib/#mapa-de-calor","text":"# datos vegetables = [ \"cucumber\" , \"tomato\" , \"lettuce\" , \"asparagus\" , \"potato\" , \"wheat\" , \"barley\" ] farmers = [ \"Farmer Joe\" , \"Upland Bros.\" , \"Smith Gardening\" , \"Agrifun\" , \"Organiculture\" , \"BioGoods Ltd.\" , \"Cornylee Corp.\" ] harvest = np . array ([[ 0.8 , 2.4 , 2.5 , 3.9 , 0.0 , 4.0 , 0.0 ], [ 2.4 , 0.0 , 4.0 , 1.0 , 2.7 , 0.0 , 0.0 ], [ 1.1 , 2.4 , 0.8 , 4.3 , 1.9 , 4.4 , 0.0 ], [ 0.6 , 0.0 , 0.3 , 0.0 , 3.1 , 0.0 , 0.0 ], [ 0.7 , 1.7 , 0.6 , 2.6 , 2.2 , 6.2 , 0.0 ], [ 1.3 , 1.2 , 0.0 , 0.0 , 0.0 , 3.2 , 5.1 ], [ 0.1 , 2.0 , 0.0 , 1.4 , 0.0 , 1.9 , 6.3 ]]) # graficos fig , ax = plt . subplots ( figsize = ( 10 , 10 )) im , cbar = heatmap ( harvest , # valores vegetables , # filas farmers , # columnas ax = ax , # ventana cmap = \"YlGn\" , # gama de colores cbarlabel = \"harvest [t/year]\" # nombre barra de colores ) texts = annotate_heatmap ( im , valfmt = \" {x:.1f} t\" ) fig . tight_layout () plt . show ()","title":"Mapa de calor"},{"location":"lectures/data_manipulation/visualization/matplotlib/#otros-graficos-de-interes","text":"","title":"Otros gr\u00e1ficos de inter\u00e9s"},{"location":"lectures/data_manipulation/visualization/matplotlib/#grafico-de-barra-de-error","text":"# datos x = np . arange ( 0.1 , 4 , 0.5 ) y = np . exp ( - x ) # graficos fig , ( ax1 , ax2 ) = plt . subplots ( ncols = 2 , figsize = ( 20 , 10 )) x_error = 0.1 + 0.2 * np . random . rand ( len ( x )) ax1 . errorbar ( x , y , xerr = x_error ) y_error = 0.1 + 0.2 * np . random . rand ( len ( x )) ax2 . errorbar ( x , y , yerr = y_error ) fig . show () /home/falfaro/.cache/pypoetry/virtualenvs/pymessi-xyyw3p3f-py3.6/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure. # This is added back by InteractiveShellApp.init_path() \u00bfCu\u00e1ndo utilizar gr\u00e1fico de barra de error? * x: Dato del tipo posicional o cuantitativo. * y: Dato del tipo posicional o cuantitativo. * z: Dato del tipo posicional o cuantitativo. Los valores de z tienen que tener las mismas unidades y.","title":"Gr\u00e1fico de Barra de Error"},{"location":"lectures/data_manipulation/visualization/matplotlib/#countor-plot","text":"# datos x , y = np . mgrid [ - 3 : 3 : .025 , - 2 : 2 : .025 ] pos = np . empty ( x . shape + ( 2 ,)) pos [:, :, 0 ] = x pos [:, :, 1 ] = y z1 = multivariate_normal . pdf ( pos , mean = [ - 1.0 , - 1.0 ], cov = [[ 1.0 , 0.0 ], [ 0.0 , 0.1 ]] ) z2 = multivariate_normal . pdf ( pos , mean = [ 1.0 , 1.0 ], cov = [[ 1.5 , 0.0 ], [ 0.0 , 0.5 ]] ) z = 10 * ( z1 - z2 ) # grafico fig , axs = plt . subplots ( ncols = 2 , figsize = ( 20 , 10 ), sharex = True , sharey = True ) cmaps = [ cm . rainbow , cm . autumn , cm . coolwarm , cm . gray ] countour_styles = [ { \"colors\" : \"k\" , \"linestyles\" : \"solid\" }, { \"colors\" : \"k\" , \"linestyles\" : \"dashed\" }, ] for i , ax in zip ( range ( len ( cmaps )), axs . ravel ()): cs = ax . contour ( x , y , z , 11 , ** countour_styles [ i ]) if i > 0 : ax . clabel ( cs , fontsize = 9 , inline = 1 ) ax . grid ( alpha = 0.5 ) fig . show () /home/falfaro/.cache/pypoetry/virtualenvs/pymessi-xyyw3p3f-py3.6/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure. \u00bfCu\u00e1ndo se debe utiliar countour plot? x: Dato del tipo posicional o cuantitativo. y: Dato de tipo posicional o cuantitativo. z: Dato de tipo posicional o cuantitativo. OBSERVACION : Se debe tener suficiente densidad/regularidad de puntos como para poder obtener superficies de nivel.","title":"Countor Plot"},{"location":"lectures/data_manipulation/visualization/matplotlib/#campos-de-vectores","text":"\u00bfPorqu\u00e9 se llama quiver al campo de vectores en ingl\u00e9s? def my_vector_field (): \"\"\" You can even define a new function. \"\"\" X , Y = np . meshgrid ( np . arange ( 0 , 2 * np . pi , .2 ), np . arange ( 0 , 2 * np . pi , .2 )) U = np . cos ( X ) V = np . sin ( Y ) fig , ( ax1 , ax2 ) = plt . subplots ( ncols = 2 , figsize = ( 20 , 10 )) Q1 = ax1 . quiver ( U , V ) qk1 = ax1 . quiverkey ( Q1 , 0.5 , 0.92 , 2 , r '$2 \\frac {m}{s} $' , labelpos = 'W' , fontproperties = { 'weight' : 'bold' } ) Q2 = ax2 . quiver ( X [:: 3 , :: 3 ], Y [:: 3 , :: 3 ], U [:: 3 , :: 3 ], V [:: 3 , :: 3 ], pivot = 'mid' , color = 'r' , units = 'inches' ) qk2 = ax2 . quiverkey ( Q2 , 0.5 , 0.03 , 1 , r '$1 \\frac {m}{s} $' , fontproperties = { 'weight' : 'bold' } ) ax2 . plot ( X [:: 3 , :: 3 ], Y [:: 3 , :: 3 ], 'k.' ) ax2 . set_title ( \"pivot='mid'; every third arrow; units='inches'\" ) fig . show () my_vector_field () /home/falfaro/.cache/pypoetry/virtualenvs/pymessi-xyyw3p3f-py3.6/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure. \u00bfCu\u00e1ndo utilizar campos de vectores? x: Debe ser datos del tipo posicional o cuantitativo. y: Debe ser datos de tipo posicional o cuantitativo. z: Pendiente debe ser dato de tipo posicional o cuantitativo. Evitar: gr\u00e1fico de campo de vectores si no es posible la interpretaci\u00f3n correspondiente.","title":"Campos de Vectores"},{"location":"lectures/data_manipulation/visualization/matplotlib/#referencia","text":"Gallery-matplotlib","title":"Referencia"},{"location":"lectures/data_manipulation/visualization/seaborn/","text":"Seaborn Introducci\u00f3n Matplotlib ha demostrado ser una herramienta de visualizaci\u00f3n incre\u00edblemente \u00fatil y popular, pero incluso los usuarios entusiastas admitir\u00e1n que a menudo deja mucho que desear. Hay varias quejas v\u00e1lidas sobre Matplotlib que a menudo surgen: Antes de la versi\u00f3n 2.0, los valores predeterminados de Matplotlib no son exactamente las mejores opciones. Se bas\u00f3 en MATLAB alrededor de 1999, y esto a menudo se nota. La API de Matplotlib es de nivel relativamente bajo. Es posible realizar una visualizaci\u00f3n estad\u00edstica sofisticada, pero a menudo requiere mucho c\u00f3digo repetitivo. Matplotlib fue anterior a Pandas en m\u00e1s de una d\u00e9cada y, por lo tanto, no est\u00e1 dise\u00f1ado para su uso con Pandas DataFrames. Para visualizar datos de un Pandas DataFrame, debe extraer cada Serie y, a menudo, concatenarlas juntas en el formato correcto. Ser\u00eda mejor tener una biblioteca de trazado que pueda usar inteligentemente las etiquetas de DataFrame en un trazado. Una respuesta a estos problemas es Seaborn . Seaborn proporciona una API sobre Matplotlib que ofrece opciones sensatas para el estilo de trazado y los valores predeterminados de color, define funciones simples de alto nivel para tipos de trazado estad\u00edsticos comunes, y se integra con la funcionalidad proporcionada por Pandas DataFrames. Gr\u00e1fico a Gr\u00e1fico Para mostrar el funcionamiento de seaborn, se ocupa el conjunto de datos: pokemon.csv . Para el caso de seaborn se los gr\u00e1ficos ser\u00e1n generados directamente desde el dataframe. # librerias import os import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns pd . set_option ( 'display.max_columns' , 500 ) # Ver m\u00e1s columnas de los dataframes # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab % matplotlib inline # cargar datos pokemon_data = pd . read_csv ( os . path . join ( \"data\" , \"pokemon.csv\" ), sep = \",\" ) pokemon_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # Name Type 1 Type 2 HP Attack Defense Sp. Atk Sp. Def Speed Generation Legendary 0 1 Bulbasaur Grass Poison 45 49 49 65 65 45 1 False 1 2 Ivysaur Grass Poison 60 62 63 80 80 60 1 False 2 3 Venusaur Grass Poison 80 82 83 100 100 80 1 False 3 4 Mega Venusaur Grass Poison 80 100 123 122 120 80 1 False 4 5 Charmander Fire NaN 39 52 43 60 50 65 1 False Gr\u00e1fico de l\u00ednea y puntos Realizar un gr\u00e1fico de l\u00ednea y otro de puntos para analizar el ataque vs defensa de todos los pokemones separados por generaci\u00f3n. # grafico de linea plt . figure ( figsize = ( 10 , 6 )) palette = sns . color_palette ( \"hls\" , 6 ) sns . lineplot ( x = 'Attack' , y = 'Defense' , hue = 'Generation' , # color por Generation data = pokemon_data , ci = None , palette = palette ) plt . show () # grafico de puntos plt . figure ( figsize = ( 10 , 6 )) sns . scatterplot ( x = 'Attack' , y = 'Defense' , hue = 'Generation' , # color por Generation data = pokemon_data , palette = palette ) plt . show () Boxplot Realizar un gr\u00e1fico box plot sobre los stats de los pokemones. # Pre-format DataFrame stats_df = pokemon_data . drop ([ '#' , 'Generation' , 'Legendary' ], axis = 1 ) # New boxplot using stats_df plt . figure ( figsize = ( 10 , 6 )) sns . boxplot ( data = stats_df ) plt . show () Mapas de calor Realizar un mapa de calor sobre los stats de los pokemones. # Calculate correlations corr = stats_df . corr () # Heatmap plt . figure ( figsize = ( 10 , 6 )) sns . heatmap ( corr ) plt . show () Histogramas Realizar un histograma del stat attack . # Distribution Plot (a.k.a. Histogram) plt . figure ( figsize = ( 10 , 6 )) sns . histplot ( pokemon_data . Attack ) plt . show () Barplot Realizar un bar plot sobre la cantidad de pokemones que hay por generaci\u00f3n # realizar conteo de manera manual df_generation = pokemon_data . groupby ( 'Generation' ) . apply ( lambda x : len ( x )) . reset_index () df_generation . columns = [ 'Generation' , 'Count' ] df_generation . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Generation Count 0 1 166 1 2 106 2 3 160 3 4 121 4 5 165 # plot seaborn: barplot plt . figure ( figsize = ( 10 , 6 )) sns . barplot ( x = 'Generation' , y = 'Count' , data = df_generation ) plt . show () countplot Realizar un conteo sobre los distintos tipos Type 1 de pokemones. # Count Plot (a.k.a. Bar Plot) plt . figure ( figsize = ( 10 , 6 )) pkmn_type_colors = [ '#78C850' , # Grass '#F08030' , # Fire '#6890F0' , # Water '#A8B820' , # Bug '#A8A878' , # Normal '#A040A0' , # Poison '#F8D030' , # Electric '#E0C068' , # Ground '#EE99AC' , # Fairy '#C03028' , # Fighting '#F85888' , # Psychic '#B8A038' , # Rock '#705898' , # Ghost '#98D8D8' , # Ice '#7038F8' , # Dragon ] sns . countplot ( x = 'Type 1' , data = pokemon_data , palette = pkmn_type_colors ) # Rotate x-labels plt . xticks ( rotation =- 45 ) plt . show () Factor plot Realizar un catplot de los distintos tipos de pokemones para la generaci\u00f3n Type 1 , analizando si el pokem\u00f3n es legendario o no. # Factor Plot plt . figure ( figsize = ( 16 , 12 )) g = sns . catplot ( x = 'Type 1' , y = 'Attack' , data = pokemon_data , hue = 'Legendary' , # Color by stage col = 'Legendary' , # Separate by stage kind = 'swarm' , # Swarmplot s = 1.5 , palette = [ 'black' , 'blue' ]) # Rotate x-axis labels g . set_xticklabels ( rotation =- 45 ) # Doesn't work because only rotates last plot # plt.xticks(rotation=-45) plt . show () <Figure size 1152x864 with 0 Axes> Customizando con Matplotlib. Seaborn es una interfaz de alto nivel para Matplotlib. Seg\u00fan nuestra experiencia, Seaborn lo llevar\u00e1 a la mayor parte del camino, pero a veces necesitar\u00e1 traer Matplotlib. Establecer los l\u00edmites de los ejes es uno de esos momentos, pero el proceso es bastante simple: Primero, usar la funci\u00f3n lmplotde Seaborn de manera normal. Luego, use las funciones de customizaci\u00f3n de Matplotlib. En este caso, usaremos sus funciones ylim () y xlim () . Aqu\u00ed est\u00e1 nuestro nuevo diagrama de dispersi\u00f3n con l\u00edmites de ejes sensibles: # plot seaborn plt . figure ( figsize = ( 10 , 6 )) sns . lmplot ( x = 'Attack' , y = 'Defense' , data = pokemon_data , fit_reg = False , height = 8 , hue = 'Generation' ) # usar Matplotlib plt . ylim ( 0 , None ) plt . xlim ( 0 , None ) plt . show () <Figure size 720x432 with 0 Axes> Referencia Gallery-seaborn","title":"Seaborn"},{"location":"lectures/data_manipulation/visualization/seaborn/#seaborn","text":"","title":"Seaborn"},{"location":"lectures/data_manipulation/visualization/seaborn/#introduccion","text":"Matplotlib ha demostrado ser una herramienta de visualizaci\u00f3n incre\u00edblemente \u00fatil y popular, pero incluso los usuarios entusiastas admitir\u00e1n que a menudo deja mucho que desear. Hay varias quejas v\u00e1lidas sobre Matplotlib que a menudo surgen: Antes de la versi\u00f3n 2.0, los valores predeterminados de Matplotlib no son exactamente las mejores opciones. Se bas\u00f3 en MATLAB alrededor de 1999, y esto a menudo se nota. La API de Matplotlib es de nivel relativamente bajo. Es posible realizar una visualizaci\u00f3n estad\u00edstica sofisticada, pero a menudo requiere mucho c\u00f3digo repetitivo. Matplotlib fue anterior a Pandas en m\u00e1s de una d\u00e9cada y, por lo tanto, no est\u00e1 dise\u00f1ado para su uso con Pandas DataFrames. Para visualizar datos de un Pandas DataFrame, debe extraer cada Serie y, a menudo, concatenarlas juntas en el formato correcto. Ser\u00eda mejor tener una biblioteca de trazado que pueda usar inteligentemente las etiquetas de DataFrame en un trazado. Una respuesta a estos problemas es Seaborn . Seaborn proporciona una API sobre Matplotlib que ofrece opciones sensatas para el estilo de trazado y los valores predeterminados de color, define funciones simples de alto nivel para tipos de trazado estad\u00edsticos comunes, y se integra con la funcionalidad proporcionada por Pandas DataFrames.","title":"Introducci\u00f3n"},{"location":"lectures/data_manipulation/visualization/seaborn/#grafico-a-grafico","text":"Para mostrar el funcionamiento de seaborn, se ocupa el conjunto de datos: pokemon.csv . Para el caso de seaborn se los gr\u00e1ficos ser\u00e1n generados directamente desde el dataframe. # librerias import os import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns pd . set_option ( 'display.max_columns' , 500 ) # Ver m\u00e1s columnas de los dataframes # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab % matplotlib inline # cargar datos pokemon_data = pd . read_csv ( os . path . join ( \"data\" , \"pokemon.csv\" ), sep = \",\" ) pokemon_data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # Name Type 1 Type 2 HP Attack Defense Sp. Atk Sp. Def Speed Generation Legendary 0 1 Bulbasaur Grass Poison 45 49 49 65 65 45 1 False 1 2 Ivysaur Grass Poison 60 62 63 80 80 60 1 False 2 3 Venusaur Grass Poison 80 82 83 100 100 80 1 False 3 4 Mega Venusaur Grass Poison 80 100 123 122 120 80 1 False 4 5 Charmander Fire NaN 39 52 43 60 50 65 1 False","title":"Gr\u00e1fico a Gr\u00e1fico"},{"location":"lectures/data_manipulation/visualization/seaborn/#grafico-de-linea-y-puntos","text":"Realizar un gr\u00e1fico de l\u00ednea y otro de puntos para analizar el ataque vs defensa de todos los pokemones separados por generaci\u00f3n. # grafico de linea plt . figure ( figsize = ( 10 , 6 )) palette = sns . color_palette ( \"hls\" , 6 ) sns . lineplot ( x = 'Attack' , y = 'Defense' , hue = 'Generation' , # color por Generation data = pokemon_data , ci = None , palette = palette ) plt . show () # grafico de puntos plt . figure ( figsize = ( 10 , 6 )) sns . scatterplot ( x = 'Attack' , y = 'Defense' , hue = 'Generation' , # color por Generation data = pokemon_data , palette = palette ) plt . show ()","title":"Gr\u00e1fico de l\u00ednea y puntos"},{"location":"lectures/data_manipulation/visualization/seaborn/#boxplot","text":"Realizar un gr\u00e1fico box plot sobre los stats de los pokemones. # Pre-format DataFrame stats_df = pokemon_data . drop ([ '#' , 'Generation' , 'Legendary' ], axis = 1 ) # New boxplot using stats_df plt . figure ( figsize = ( 10 , 6 )) sns . boxplot ( data = stats_df ) plt . show ()","title":"Boxplot"},{"location":"lectures/data_manipulation/visualization/seaborn/#mapas-de-calor","text":"Realizar un mapa de calor sobre los stats de los pokemones. # Calculate correlations corr = stats_df . corr () # Heatmap plt . figure ( figsize = ( 10 , 6 )) sns . heatmap ( corr ) plt . show ()","title":"Mapas de calor"},{"location":"lectures/data_manipulation/visualization/seaborn/#histogramas","text":"Realizar un histograma del stat attack . # Distribution Plot (a.k.a. Histogram) plt . figure ( figsize = ( 10 , 6 )) sns . histplot ( pokemon_data . Attack ) plt . show ()","title":"Histogramas"},{"location":"lectures/data_manipulation/visualization/seaborn/#barplot","text":"Realizar un bar plot sobre la cantidad de pokemones que hay por generaci\u00f3n # realizar conteo de manera manual df_generation = pokemon_data . groupby ( 'Generation' ) . apply ( lambda x : len ( x )) . reset_index () df_generation . columns = [ 'Generation' , 'Count' ] df_generation . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Generation Count 0 1 166 1 2 106 2 3 160 3 4 121 4 5 165 # plot seaborn: barplot plt . figure ( figsize = ( 10 , 6 )) sns . barplot ( x = 'Generation' , y = 'Count' , data = df_generation ) plt . show ()","title":"Barplot"},{"location":"lectures/data_manipulation/visualization/seaborn/#countplot","text":"Realizar un conteo sobre los distintos tipos Type 1 de pokemones. # Count Plot (a.k.a. Bar Plot) plt . figure ( figsize = ( 10 , 6 )) pkmn_type_colors = [ '#78C850' , # Grass '#F08030' , # Fire '#6890F0' , # Water '#A8B820' , # Bug '#A8A878' , # Normal '#A040A0' , # Poison '#F8D030' , # Electric '#E0C068' , # Ground '#EE99AC' , # Fairy '#C03028' , # Fighting '#F85888' , # Psychic '#B8A038' , # Rock '#705898' , # Ghost '#98D8D8' , # Ice '#7038F8' , # Dragon ] sns . countplot ( x = 'Type 1' , data = pokemon_data , palette = pkmn_type_colors ) # Rotate x-labels plt . xticks ( rotation =- 45 ) plt . show ()","title":"countplot"},{"location":"lectures/data_manipulation/visualization/seaborn/#factor-plot","text":"Realizar un catplot de los distintos tipos de pokemones para la generaci\u00f3n Type 1 , analizando si el pokem\u00f3n es legendario o no. # Factor Plot plt . figure ( figsize = ( 16 , 12 )) g = sns . catplot ( x = 'Type 1' , y = 'Attack' , data = pokemon_data , hue = 'Legendary' , # Color by stage col = 'Legendary' , # Separate by stage kind = 'swarm' , # Swarmplot s = 1.5 , palette = [ 'black' , 'blue' ]) # Rotate x-axis labels g . set_xticklabels ( rotation =- 45 ) # Doesn't work because only rotates last plot # plt.xticks(rotation=-45) plt . show () <Figure size 1152x864 with 0 Axes>","title":"Factor plot"},{"location":"lectures/data_manipulation/visualization/seaborn/#customizando-con-matplotlib","text":"Seaborn es una interfaz de alto nivel para Matplotlib. Seg\u00fan nuestra experiencia, Seaborn lo llevar\u00e1 a la mayor parte del camino, pero a veces necesitar\u00e1 traer Matplotlib. Establecer los l\u00edmites de los ejes es uno de esos momentos, pero el proceso es bastante simple: Primero, usar la funci\u00f3n lmplotde Seaborn de manera normal. Luego, use las funciones de customizaci\u00f3n de Matplotlib. En este caso, usaremos sus funciones ylim () y xlim () . Aqu\u00ed est\u00e1 nuestro nuevo diagrama de dispersi\u00f3n con l\u00edmites de ejes sensibles: # plot seaborn plt . figure ( figsize = ( 10 , 6 )) sns . lmplot ( x = 'Attack' , y = 'Defense' , data = pokemon_data , fit_reg = False , height = 8 , hue = 'Generation' ) # usar Matplotlib plt . ylim ( 0 , None ) plt . xlim ( 0 , None ) plt . show () <Figure size 720x432 with 0 Axes>","title":"Customizando con Matplotlib."},{"location":"lectures/data_manipulation/visualization/seaborn/#referencia","text":"Gallery-seaborn","title":"Referencia"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/","text":"Clustering El Clustering es la tarea de agrupar objetos por similitud, en grupos o conjuntos de manera que los miembros del mismo grupo tengan caracter\u00edsticas similares. Es la tarea principal de la miner\u00eda de datos exploratoria y es una t\u00e9cnica com\u00fan en el an\u00e1lisis de datos estad\u00edsticos. K-means Teor\u00eda El algoritmo K-means (MacQueen, 1967) agrupa las observaciones en un n\u00famero predefinido de \\(k\\) clusters de forma que, la suma de las varianzas internas de los clusters, sea lo menor posible. Existen varias implementaciones de este algoritmo, la m\u00e1s com\u00fan de ellas se conoce como Lloyd\u2019s. En la bibliograf\u00eda es com\u00fan encontrar los t\u00e9rminos inertia, within-cluster sum-of-squares o varianza intra-cluster para referirse a la varianza interna de los clusters. Consid\u00e9rense \\(\ud835\udc36_1 ,..., \ud835\udc36_k\\) como los sets formados por los \u00edndices de las observaciones de cada uno de los clusters. Por ejemplo, el set \\(\ud835\udc36_1\\) contiene los \u00edndices de las observaciones agrupadas en el cluster 1. La nomenclatura empleada para indicar que la observaci\u00f3n \\(i\\) pertenece al cluster \\(k\\) es: \\(i \\in C_k\\) . Todos los sets satisfacen dos propiedades: $C_1 \\cup C_2 \\cup ... \\cup C_k = {1,...,n} $ . Significa que toda observaci\u00f3n pertenece a uno de los \\(k\\) clusters. $C_i \\cap C_{j} = \\emptyset $ para todo \\(i \\neq j\\) . Implica que los clusters no solapan, ninguna observaci\u00f3n pertenece a m\u00e1s de un cluster a la vez. El algoritmo consiste en reducir al m\u00ednimo la suma de las distancias cuadradas desde la media dentro del agrupamiento. Matem\u00e1ticamente: \\begin{align } (P) \\ \\textrm{Minimizar } f(C_l,\\mu_l) = \\sum_{l=1}^k \\sum_{x_n \\in C_l} ||x_n - \\mu_l ||^2 \\textrm{, respecto a } C_l, \\mu_l, \\end{align } donde \\(C_l\\) es el cluster l-\u00e9simo y \\(\\mu_l\\) es el centroide l-\u00e9simo. Algoritmo Especificar el n\u00famero \\(k\\) de clusters que se quieren crear. Seleccionar de forma aleatoria \\(k\\) observaciones del set de datos como centroides iniciales. Asignar cada una de las observaciones al centroide m\u00e1s cercano. Para cada uno de los \\(k\\) clusters generados en el paso 3, recalcular su centroide. Repetir los pasos 3 y 4 hasta que las asignaciones no cambien o se alcance el n\u00famero m\u00e1ximo de iteraciones establecido. El problema anterior es NP-hard (imposible de resolver en tiempo polinomial, del tipo m\u00e1s dif\u00edcil de los probleams NP). Ventajas y desventajas K-means es uno de los m\u00e9todos de clustering m\u00e1s utilizados. Destaca por la sencillez y velocidad de su algoritmo, sin embargo, presenta una serie de limitaciones que se deben tener en cuenta. Requiere que se indique de antemano el n\u00famero de clusters que se van a crear. Esto puede ser complicado si no se dispone de informaci\u00f3n adicional sobre los datos con los que se trabaja. Se han desarrollado varias estrategias para ayudar a identificar potenciales valores \u00f3ptimos de \\(k\\) (elbow, shilouette), pero todas ellas son orientativas. Dificultad para detectar clusters alargados o con formas irregulares. Las agrupaciones resultantes pueden variar dependiendo de la asignaci\u00f3n aleatoria inicial de los centroides. Para minimizar este problema, se recomienda repetir el proceso de clustering entre 25-50 veces y seleccionar como resultado definitivo el que tenga menor suma total de varianza interna. Aun as\u00ed, solo se puede garantizar la reproducibilidad de los resultados si se emplean semillas. Presenta problemas de robustez frente a outliers. Aplicaci\u00f3n Veamos un ejemplo de an\u00e1lisis no supervisado ocupando el algoritmo k-means . # librerias import os import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.datasets import make_blobs pd . set_option ( 'display.max_columns' , 500 ) # Ver m\u00e1s columnas de los dataframes # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab % matplotlib inline def init_blobs ( N , k , seed = 42 ): X , y = make_blobs ( n_samples = N , centers = k , random_state = seed , cluster_std = 0.60 ) return X # generar datos data = init_blobs ( 10000 , 6 , seed = 43 ) df = pd . DataFrame ( data , columns = [ \"x\" , \"y\" ]) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y 0 -6.953617 -4.989933 1 -2.681117 7.583914 2 -1.510161 4.933676 3 -9.748491 5.479457 4 -7.438017 -4.597754 Debido a que trabajamos con el concepto de distancia, muchas veces las columnas del dataframe pueden estar en distintas escalas, lo cual puede complicar a los algoritmos ocupados (al menos con sklearn ). En estos casos, se suele normalizar los atributos, es decir, dejar los valores en una escala acotada y/o con estimadores fijos. Por ejemplo, en *sklearn podemos encontrar las siguientes formas de normalizar: StandardScaler : se normaliza restando la media y escalando por su desviaci\u00f3n estanda. \\( \\(x_{prep} = \\dfrac{x-u}{s}\\) \\) La ventaja es que la media del nuevo conjunto de datos cumple con la propiedad que su media \\(\\mu\\) es igual a cero y su desviaci\u00f3n estandar \\(s\\) es igual a 1. * MinMaxScaler : se normaliza ocupando los valores de los m\u00ednimos y m\u00e1ximo del conjunto de datos. \\( \\(x_{prep} = \\dfrac{x-x_{min}}{x_{min}-x_{max}}\\) \\) Esta forma de normalizar resulta \u00fatil cuando la desviaci\u00f3n estandar \\(s\\) es muy peque\u00f1a (cercana) a cero, por lo que lo convierte en un estimador m\u00e1s roubusto que el StandardScaler . from sklearn.preprocessing import StandardScaler scaler = StandardScaler () columns = [ 'x' , 'y' ] df [ columns ] = scaler . fit_transform ( df [ columns ]) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y 0 -0.579033 -1.831435 1 0.408821 1.194578 2 0.679560 0.556774 3 -1.225241 0.688121 4 -0.691032 -1.737053 # comprobar resultados del estimador df . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y count 1.000000e+04 1.000000e+04 mean 2.060574e-16 -2.285105e-15 std 1.000050e+00 1.000050e+00 min -1.638247e+00 -2.410317e+00 25% -8.015576e-01 -4.418042e-01 50% -2.089351e-01 1.863259e-01 75% 5.480066e-01 8.159808e-01 max 2.243358e+00 1.639547e+00 Con esta parametrizaci\u00f3n procedemos a graficar nuestros resultados: # graficar sns . set ( rc = { 'figure.figsize' :( 11.7 , 8.27 )}) ax = sns . scatterplot ( data = df , x = \"x\" , y = \"y\" ) Ahora ajustamos el algoritmo KMeans de sklearn . Primero, comprendamos los hiperpar\u00e1metros m\u00e1s importantes: n_clusters : El n\u00famero de clusters a crear, o sea K . Por defecto es 8 init : M\u00e9todo de inicializaci\u00f3n. Un problema que tiene el algoritmo K-Medias es que la solucci\u00f3n alcanzada varia seg\u00fan la inicializaci\u00f3n de los centroides. sklearn empieza usando el m\u00e9todo kmeans++ que es una versi\u00f3n m\u00e1s moderna y que proporciona mejores resultados que la inicializaci\u00f3n aleatoria (random) n_init : El n\u00famero de inicializaciones a probar. B\u00e1sicamente KMeans aplica el algoritmo n_init veces y elige los clusters que minimizan la inercia. max_iter : M\u00e1ximo n\u00famero de iteraciones para llegar al criterio de parada. random_state : semilla para garantizar la reproducibilidad de los resultados. tol : Tolerancia para declarar criterio de parada (cuanto m\u00e1s grande, antes parar\u00e1 el algoritmo). # ajustar modelo: k-means from sklearn.cluster import KMeans X = np . array ( df ) kmeans = KMeans ( n_clusters = 6 , n_init = 25 , random_state = 123 ) kmeans . fit ( X ) centroids = kmeans . cluster_centers_ # centros clusters = kmeans . labels_ # clusters # etiquetar los datos con los clusters encontrados df [ \"cluster\" ] = clusters df [ \"cluster\" ] = df [ \"cluster\" ] . astype ( 'category' ) centroids_df = pd . DataFrame ( centroids , columns = [ \"x\" , \"y\" ]) centroids_df [ \"cluster\" ] = [ 1 , 2 , 3 , 4 , 5 , 6 ] # graficar los datos etiquetados con k-means fig , ax = plt . subplots ( figsize = ( 11 , 8.5 )) sns . scatterplot ( data = df , x = \"x\" , y = \"y\" , hue = \"cluster\" , legend = 'full' , palette = \"Set2\" ) sns . scatterplot ( x = \"x\" , y = \"y\" , s = 100 , color = \"black\" , marker = \"x\" , data = centroids_df ) plt . show () Ahora la pregunta que surge de manera natural es ... \u00bf c\u00f3mo escoger el mejor n\u00famero de clusters?. No existe un criterio objetivo ni ampliamente v\u00e1lido para la elecci\u00f3n de un n\u00famero \u00f3ptimo de clusters. Aunque no exista un criterio objetivo para la selecci\u00f3n del n\u00famero de clusters, si que se han implementado diferentes m\u00e9todos que nos ayudan a elegir un n\u00famero apropiado de clusters para agrupar los datos; como son, m\u00e9todo del codo (elbow method) criterio de Calinsky Affinity Propagation (AP) Gap (tambi\u00e9n con su versi\u00f3n estad\u00edstica) Dendrogramas etc. Regla del codo Este m\u00e9todo utiliza los valores de la funci\u00f3n de perdida, \\(f(C_l,\\mu_l)\\) , obtenidos tras aplicar el \\(K\\) -means a diferente n\u00famero de Clusters (desde 1 a \\(N\\) clusters). Una vez obtenidos los valores de la funci\u00f3n de p\u00e9rdida tras aplicar el K-means de 1 a \\(N\\) clusters, representamos en una gr\u00e1fica lineal la funci\u00f3n de p\u00e9rdida respecto del n\u00famero de clusters. En esta gr\u00e1fica se deber\u00eda de apreciar un cambio brusco en la evoluci\u00f3n de la funci\u00f3n de p\u00e9rdida, teniendo la l\u00ednea representada una forma similar a la de un brazo y su codo. El punto en el que se observa ese cambio brusco en la funci\u00f3n de p\u00e9rdida nos dir\u00e1 el n\u00famero \u00f3ptimo de clusters a seleccionar para ese data set; o dicho de otra manera: el punto que representar\u00eda al codo del brazo ser\u00e1 el n\u00famero \u00f3ptimo de clusters para ese data set . # implementaci\u00f3n de la regla del codo Nc = range ( 1 , 15 ) kmeans = [ KMeans ( n_clusters = i ) for i in Nc ] score = [ kmeans [ i ] . fit ( df ) . inertia_ for i in range ( len ( kmeans ))] df_Elbow = pd . DataFrame ({ 'Number of Clusters' : Nc , 'Score' : score }) df_Elbow . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Number of Clusters Score 0 1 49337.951600 1 2 20004.858535 2 3 12733.014667 3 4 6760.679396 4 5 3139.657771 # graficar los datos etiquetados con k-means fig , ax = plt . subplots ( figsize = ( 11 , 8.5 )) plt . title ( 'Elbow Curve' ) sns . lineplot ( x = \"Number of Clusters\" , y = \"Score\" , data = df_Elbow ) sns . scatterplot ( x = \"Number of Clusters\" , y = \"Score\" , data = df_Elbow ) plt . show () A partir de 4 clusters la reducci\u00f3n en la suma total de cuadrados internos parece estabilizarse, indicando que \\(k\\) = 4 es una buena opci\u00f3n. Hierarchical clustering Teor\u00eda Hierarchical clustering es una alternativa a los m\u00e9todos de partitioning clustering que no requiere que se pre-especifique el n\u00famero de clusters. Los m\u00e9todos que engloba el hierarchical clustering se subdividen en dos tipos dependiendo de la estrategia seguida para crear los grupos: Aglomerativo (agglomerative clustering o bottom-up): el agrupamiento se inicia con todas las observaciones separadas, cada una formando un cluster individual. Los clusters se van combinado a medida que la estructura crece hasta converger en uno solo. Divisivo (divisive clustering o top-down): es la estrategia opuesta al aglomerativo. Se inicia con todas las observaciones contenidas en un mismo cluster y se suceden divisiones hasta que cada observaci\u00f3n forma un cluster* individual. En ambos casos, los resultados pueden representarse de forma muy intuitiva en una estructura de \u00e1rbol llamada dendrograma. Algoritmo Aglomerativo El algoritmo seguido para por el clustering aglomerativo es: Considerar cada una de las n observaciones como un cluster individual, formando as\u00ed la base del dendrograma (hojas). Proceso iterativo hasta que todas las observaciones pertenecen a un \u00fanico cluster: Calcular la distancia entre cada posible par de los n clusters. El investigador debe determinar el tipo de medida empleada para cuantificar la similitud entre observaciones o grupos (distancia y linkage). Los dos clusters m\u00e1s similares se fusionan, de forma que quedan n-1 clusters. Cortar la estructura de \u00e1rbol generada (dendrograma) a una determinada altura para crear los clusters finales. Para que el proceso de agrupamiento pueda llevarse a cabo tal como indica el algoritmo anterior, es necesario definir c\u00f3mo se cuantifica la similitud entre dos clusters. Es decir, se tiene que extender el concepto de distancia entre pares de observaciones para que sea aplicable a pares de grupos, cada uno formado por varias observaciones. A este proceso se le conoce como linkage. A continuaci\u00f3n, se describen los 5 tipos de linkage m\u00e1s empleados y sus definiciones. Complete or Maximum : se calcula la distancia entre todos los posibles pares formados por una observaci\u00f3n del cluster A y una del cluster B. La mayor de todas ellas se selecciona como la distancia entre los dos clusters. Se trata de la medida m\u00e1s conservadora (maximal intercluster dissimilarity). Single or Minimum : se calcula la distancia entre todos los posibles pares formados por una observaci\u00f3n del cluster A y una del cluster B. La menor de todas ellas se selecciona como la distancia entre los dos clusters. Se trata de la medida menos conservadora (minimal intercluster dissimilarity). Average : Se calcula la distancia entre todos los posibles pares formados por una observaci\u00f3n del cluster A y una del cluster B. El valor promedio de todas ellas se selecciona como la distancia entre los dos clusters (mean intercluster dissimilarity). Centroid : Se calcula el centroide de cada uno de los clusters y se selecciona la distancia entre ellos como la distancia entre los dos clusters. Ward : Se trata de un m\u00e9todo general. La selecci\u00f3n del par de clusters que se combinan en cada paso del agglomerative hierarchical clustering se basa en el valor \u00f3ptimo de una funci\u00f3n objetivo, pudiendo ser esta \u00faltima cualquier funci\u00f3n definida por el analista. El m\u00e9todo Ward's minimum variance es un caso particular en el que el objetivo es minimizar la suma total de varianza intra-cluster. En cada paso, se identifican aquellos 2 clusters cuya fusi\u00f3n conlleva menor incremento de la varianza total intra-cluster. Esta es la misma m\u00e9trica que se minimiza en K-means. Los m\u00e9todos de complete, average y Ward's minimum variance suelen ser los preferidos por los analistas debido a que generan dendrogramas m\u00e1s compensados. Sin embargo, no se puede determinar que uno sea mejor que otro, ya que depende del caso de estudio en cuesti\u00f3n. Por ejemplo, en gen\u00f3mica, se emplea con frecuencia el m\u00e9todo de centroides. Junto con los resultados de un proceso de hierarchical clustering siempre hay que indicar qu\u00e9 distancia se ha empleado, as\u00ed como el tipo de linkage, ya que, dependiendo de estos, los resultados pueden variar en gran medida. Divisivo El algoritmo m\u00e1s conocido de divisive hierarchical clustering es DIANA (DIvisive ANAlysis Clustering). Este algoritmo se inicia con un \u00fanico cluster que contiene todas las observaciones. A continuaci\u00f3n, se van sucediendo divisiones hasta que cada observaci\u00f3n forma un cluster independiente. En cada iteraci\u00f3n, se selecciona el cluster con mayor di\u00e1metro, entendiendo por di\u00e1metro de un cluster la mayor de las diferencias entre dos de sus observaciones. Una vez seleccionado el cluster, se identifica la observaci\u00f3n m\u00e1s dispar, que es aquella con mayor distancia promedio respecto al resto de observaciones que forman el cluster. Esta observaci\u00f3n inicia el nuevo cluster. Se reasignan las observaciones en funci\u00f3n de si est\u00e1n m\u00e1s pr\u00f3ximas al nuevo cluster o al resto de la partici\u00f3n, dividiendo as\u00ed el cluster seleccionado en dos nuevos clusters. Todas las \\(n\\) observaciones forman un \u00fanico cluster. Repetir hasta que haya \\(n\\) clusters: Calcular para cada cluster la mayor de las distancias entre pares de observaciones (di\u00e1metro del cluster). Seleccionar el cluster con mayor di\u00e1metro. Calcular la distancia media de cada observaci\u00f3n respecto a las dem\u00e1s. La observaci\u00f3n m\u00e1s distante inicia un nuevo cluster. Se reasignan las observaciones restantes al nuevo cluster o al viejo dependiendo de cu\u00e1l est\u00e1 m\u00e1s pr\u00f3ximo. A diferencia del clustering aglomerativo, en el que hay que elegir un tipo de distancia y un m\u00e9todo de linkage, en el clustering divisivo solo hay que elegir la distancia, no hay linkage. Dendograma Los resultados del hierarchical clustering pueden representarse como un \u00e1rbol en el que las ramas representan la jerarqu\u00eda con la que se van sucediendo las uniones de clusters. Sup\u00f3ngase que se dispone de 45 observaciones en un espacio de dos dimensiones, a los que se les aplica hierarchical clustering para intentar identificar grupos. El siguiente dendrograma representa los resultados obtenidos. En la base del dendrograma, cada observaci\u00f3n forma una terminaci\u00f3n individual conocida como hoja o leaf del \u00e1rbol. A medida que se asciende por la estructura, pares de hojas se fusionan formando las primeras ramas. Estas uniones se corresponden con los pares de observaciones m\u00e1s similares. Tambi\u00e9n ocurre que las ramas se fusionan con otras ramas o con hojas. Cuanto m\u00e1s temprana (m\u00e1s pr\u00f3xima a la base del dendrograma) ocurre una fusi\u00f3n, mayor es la similitud. Para cualquier par de observaciones, se puede identificar el punto del \u00e1rbol en el que las ramas que contienen dichas observaciones se fusionan. La altura a la que esto ocurre (eje vertical) indica c\u00f3mo de similares/diferentes son las dos observaciones. Los dendrogramas, por lo tanto, se deben interpretar \u00fanicamente en base al eje vertical y no por las posiciones que ocupan las observaciones en el eje horizontal, esto \u00faltimo es simplemente por est\u00e9tica y puede variar de un programa a otro. Por ejemplo, la observaci\u00f3n 8 es la m\u00e1s similar a la 10 ya que es la primera fusi\u00f3n que recibe la observaci\u00f3n 10 (y viceversa). Podr\u00eda resultar tentador decir que la observaci\u00f3n 14, situada inmediatamente a la derecha de la 10, es la siguiente m\u00e1s similar, sin embargo, las observaciones 28 y 44 son m\u00e1s similares a la 10 a pesar de que se encuentran m\u00e1s alejadas en el eje horizontal. Del mismo modo, no es correcto decir que la observaci\u00f3n 14 es m\u00e1s similar a la observaci\u00f3n 10 de lo que lo es la 36 por el hecho de que est\u00e1 m\u00e1s pr\u00f3xima en el eje horizontal. Prestando atenci\u00f3n a la altura en que las respectivas ramas se unen, la \u00fanica conclusi\u00f3n v\u00e1lida es que la similitud entre los pares 10-14 y 10-36 es la misma. Cortar el dendograma para generar los clusters Adem\u00e1s de representar en un dendrograma la similitud entre observaciones, se tiene que identificar el n\u00famero de clusters creados y qu\u00e9 observaciones forman parte de cada uno. Si se realiza un corte horizontal a una determinada altura del dendrograma, el n\u00famero de ramas que sobrepasan (en sentido ascendente) dicho corte se corresponde con el n\u00famero de clusters. La siguiente imagen muestra dos veces el mismo dendrograma. Si se realiza el corte a la altura de 5, se obtienen dos clusters, mientras que si se hace a la de 3.5 se obtienen 4. La altura de corte tiene por lo tanto la misma funci\u00f3n que el valor K en K-means-clustering: controla el n\u00famero de clusters obtenidos. Dos propiedades adicionales se derivan de la forma en que se generan los clusters en el m\u00e9todo de hierarchical clustering: Dada la longitud variable de las ramas, siempre existe un intervalo de altura para el que cualquier corte da lugar al mismo n\u00famero de clusters. En el ejemplo anterior, todos los cortes entre las alturas 5 y 6 tienen como resultado los mismos 2 clusters. Con un solo dendrograma se dispone de la flexibilidad para generar cualquier n\u00famero de clusters desde 1 a n. La selecci\u00f3n del n\u00famero \u00f3ptimo puede valorarse de forma visual, tratando de identificar las ramas principales en base a la altura a la que ocurren las uniones. En el ejemplo expuesto es razonable elegir entre 2 o 4 clusters. Aplicaci\u00f3n # Tratamiento de datos # ============================================================================== import numpy as np import pandas as pd from sklearn.datasets import make_blobs # Gr\u00e1ficos # ============================================================================== import matplotlib.pyplot as plt from matplotlib import style style . use ( 'ggplot' ) or plt . style . use ( 'ggplot' ) # Preprocesado y modelado # ============================================================================== from sklearn.cluster import AgglomerativeClustering from scipy.cluster.hierarchy import dendrogram from sklearn.preprocessing import scale from sklearn.metrics import silhouette_score # Configuraci\u00f3n warnings # ============================================================================== import warnings warnings . filterwarnings ( 'ignore' ) # generar datos X , y = make_blobs ( n_samples = 200 , n_features = 2 , centers = 4 , cluster_std = 0.60 , shuffle = True , random_state = 0 ) df = pd . DataFrame ({ 'x' : X [:, 0 ], 'y' : X [:, 1 ] }) # Escalado de datos scaler = StandardScaler () columns = [ 'x' , 'y' ] df [ columns ] = scaler . fit_transform ( df [ columns ]) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y 0 1.348818 -0.908114 1 -0.638621 -0.534950 2 0.653079 0.027910 3 -1.573023 1.276049 4 0.970706 -1.418431 # graficar sns . set ( rc = { 'figure.figsize' :( 11.7 , 8.27 )}) ax = sns . scatterplot ( data = df , x = \"x\" , y = \"y\" ) # Modelos X = np . array ( df [[ 'x' , 'y' ]]) # primer modelo modelo_hclust_complete = AgglomerativeClustering ( affinity = 'euclidean' , linkage = 'complete' , distance_threshold = 0 , n_clusters = None ) modelo_hclust_complete . fit ( X ) # segundo modelo modelo_hclust_average = AgglomerativeClustering ( affinity = 'euclidean' , linkage = 'average' , distance_threshold = 0 , n_clusters = None ) modelo_hclust_average . fit ( X ) # tercer modelo modelo_hclust_ward = AgglomerativeClustering ( affinity = 'euclidean' , linkage = 'ward' , distance_threshold = 0 , n_clusters = None ) modelo_hclust_ward . fit ( X ) AgglomerativeClustering(distance_threshold=0, n_clusters=None) def plot_dendrogram ( model , ** kwargs ): ''' Esta funci\u00f3n extrae la informaci\u00f3n de un modelo AgglomerativeClustering y representa su dendograma con la funci\u00f3n dendogram de scipy.cluster.hierarchy ''' counts = np . zeros ( model . children_ . shape [ 0 ]) n_samples = len ( model . labels_ ) for i , merge in enumerate ( model . children_ ): current_count = 0 for child_idx in merge : if child_idx < n_samples : current_count += 1 # leaf node else : current_count += counts [ child_idx - n_samples ] counts [ i ] = current_count linkage_matrix = np . column_stack ([ model . children_ , model . distances_ , counts ]) . astype ( float ) # Plot dendrogram ( linkage_matrix , ** kwargs ) # Dendrogramas plt . figure ( figsize = ( 20 , 10 )) plot_dendrogram ( modelo_hclust_average , color_threshold = 0 ) plt . title ( \"Distancia eucl\u00eddea, Linkage average\" ) plt . show () Text(0.5, 1.0, 'Distancia eucl\u00eddea, Linkage average') # Dendrogramas plt . figure ( figsize = ( 20 , 10 )) plot_dendrogram ( modelo_hclust_complete , color_threshold = 0 ) plt . title ( \"Distancia eucl\u00eddea, Linkage complete\" ) plt . show () # Dendrogramas plt . figure ( figsize = ( 20 , 10 )) plot_dendrogram ( modelo_hclust_ward , color_threshold = 0 ) plt . title ( \"Distancia eucl\u00eddea, Linkage ward\" ) plt . show () En este caso, los tres tipos de linkage identifican claramente 4 clusters, si bien esto no significa que en los 3 dendrogramas los clusters est\u00e9n formados por exactamente las mismas observaciones. N\u00famero de clusters Una forma de identificar el n\u00famero de clusters, es inspeccionar visualmente el dendograma y decidir a qu\u00e9 altura se corta para generar los clusters. Por ejemplo, para los resultados generados mediante distancia eucl\u00eddea y linkage ward, parece sensato cortar el dendograma a una altura de entre 5 y 10, de forma que se creen 4 clusters. plt . figure ( figsize = ( 20 , 10 )) altura_corte = 6 plot_dendrogram ( modelo_hclust_ward , color_threshold = altura_corte ) plt . title ( \"Distancia eucl\u00eddea, Linkage ward\" ) plt . axhline ( y = altura_corte , c = 'black' , linestyle = '--' , label = 'altura corte' ) plt . legend () plt . show () Una vez identificado el n\u00famero \u00f3ptimo de clusters, se reentrena el modelo indicando este valor. Density based clustering (DBSCAN) Teor\u00eda Density-based spatial clustering of applications with noise (DBSCAN) fue presentado en 1996 por Ester et al. como una forma de identificar clusters siguiendo el modo intuitivo en el que lo hace el cerebro humano, identificando regiones con alta densidad de observaciones separadas por regiones de baja densidad. El cerebro humano identifica f\u00e1cilmente 5 agrupaciones y algunas observaciones aisladas (ruido). V\u00e9anse ahora los clusters que se obtienen si se aplica, por ejemplo, K-means clustering. Los clusters generados distan mucho de representar las verdaderas agrupaciones. Esto es as\u00ed porque los m\u00e9todos de partitioning clustering como k-means, hierarchical, k-medoids, ... son buenos encontrando agrupaciones con forma esf\u00e9rica o convexa que no contengan un exceso de outliers o ruido, pero fallan al tratar de identificar formas arbitrarias. De ah\u00ed que el \u00fanico cluster que se corresponde con un grupo real sea el amarillo. DBSCAN evita este problema siguiendo la idea de que, para que una observaci\u00f3n forme parte de un cluster, tiene que haber un m\u00ednimo de observaciones vecinas dentro de un radio de proximidad y de que los clusters est\u00e1n separados por regiones vac\u00edas o con pocas observaciones. El algoritmo DBSCAN necesita dos par\u00e1metros: Epsilon ( \\(\\epsilon\\) ) : radio que define la regi\u00f3n vecina a una observaci\u00f3n, tambi\u00e9n llamada \ud835\udf16 -neighborhood. Minimum points ( \\(min_samples\\) ): n\u00famero m\u00ednimo de observaciones dentro de la regi\u00f3n epsilon. Empleando estos dos par\u00e1metros, cada observaci\u00f3n del set de datos se puede clasificar en una de las siguientes tres categor\u00edas: Core point : observaci\u00f3n que tiene en su \ud835\udf16 -neighborhood un n\u00famero de observaciones vecinas igual o mayor a min_samples. Border point : observaci\u00f3n no satisface el m\u00ednimo de observaciones vecinas para ser core point pero que pertenece al \\(\\epsilon\\) -neighborhood de otra observaci\u00f3n que s\u00ed es core point. Noise-outlier : observaci\u00f3n que no es core point ni border point. Por \u00faltimo, empleando las tres categor\u00edas anteriores se pueden definir tres niveles de conectividad entre observaciones: Directamente alcanzable (direct density reachable): una observaci\u00f3n \\(A\\) es directamente alcanzable desde otra observaci\u00f3n \\(B\\) si \\(A\\) forma parte del \\(\\epsilon\\) -neighborhood de \\(B\\) y \\(B\\) es un core point. Por definici\u00f3n, las observaciones solo pueden ser directamente alcanzables desde un core point. Alcanzable (density reachable): una observaci\u00f3n \\(A\\) es alcanzable desde otra observaci\u00f3n \\(\ud835\udc35\\) si existe una secuencia de core points que van desde \\(B\\) a \\(A\\) . Densamente conectadas (density conected): dos observaciones \\(A\\) y \\(B\\) est\u00e1n densamente conectadas si existe una observaci\u00f3n core point \\(C\\) tal que \\(A\\) y \\(B\\) son alcanzables desde \\(C\\) . La siguiente imagen muestra las conexiones existentes entre un conjunto de observaciones si se emplea \\(min_samples = 4\\) . La observaci\u00f3n \\(A\\) y el resto de observaciones marcadas en rojo son core points, ya que todas ellas contienen al menos 4 observaciones vecinas (incluy\u00e9ndose a ellas mismas) en su \\(\\epsilon\\) -neighborhood. Como todas son alcanzables entre ellas, forman un cluster. Las observaciones \\(B\\) y \\(C\\) no son core points pero son alcanzables desde \\(A\\) a trav\u00e9s de otros core points, por lo tanto, pertenecen al mismo cluster que \\(A\\) . La observaci\u00f3n \\(N\\) no es ni un core point ni es directamente alcanzable, por lo que se considera como ruido. Algoritmo Para cada observaci\u00f3n \\(x_i\\) calcular la distancia entre ella y el resto de observaciones. Si en su \\(epsilon\\) -neighborhood hay un n\u00famero de observaciones \\(\\geq min_samples\\) marcar la observaci\u00f3n como core point, de lo contrario marcarla como visitada. Para cada observaci\u00f3n \\(x_i\\) marcada como core point, si todav\u00eda no ha sido asignada a ning\u00fan cluster, crear uno nuevo y asignarla a \u00e9l. Encontrar recursivamente todas las observaciones densamente conectadas a ella y asignarlas al mismo cluster. Iterar el mismo proceso para todas las observaciones que no hayan sido visitadas. Aquellas observaciones que tras haber sido visitadas no pertenecen a ning\u00fan cluster se marcan como outliers. Como resultado, todo cluster cumple dos propiedades: todos los puntos que forman parte de un mismo cluster est\u00e1n densamente conectados entre ellos y, si una observaci\u00f3n \\(A\\) es densamente alcanzable desde cualquier otra observaci\u00f3n de un cluster, entonces \\(A\\) tambi\u00e9n pertenece al cluster. Hiperpar\u00e1metros Como ocurre en muchas otras t\u00e9cnicas estad\u00edsticas, en DBSCAN no existe una forma \u00fanica y exacta de encontrar el valor adecuado de epsilon ( \\(\\epsilon\\) )) y \\(min_samples\\) ) . A modo orientativo se pueden seguir las siguientes premisas: \\(min_samples\\) : cuanto mayor sea el tama\u00f1o del set de datos, mayor debe ser el valor m\u00ednimo de observaciones vecinas. En el libro Practical Guide to Cluster Analysis in R recomiendan no bajar nunca de 3. Si los datos contienen niveles altos de ruido, aumentar \\(min_samples\\) favorecer\u00e1 la creaci\u00f3n de clusters significativos menos influenciados por outliers. epsilon ( \\(\\epsilon\\) ): una buena forma de escoger el valor de \\(\\epsilon\\) es estudiar las distancias promedio entre las \\(k = minsamples\ud835\udc60\\) observaciones m\u00e1s pr\u00f3ximas. Al representar estas distancias en funci\u00f3n de \\(\\epsilon\\) , el punto de inflexi\u00f3n de la curva suele ser un valor \u00f3ptimo. Si el valor de \\(\\epsilon\\) escogido es muy peque\u00f1o, una proporci\u00f3n alta de las observaciones no se asignar\u00e1n a ning\u00fan cluster, por el contrario, si el valor es demasiado grande, la mayor\u00eda de observaciones se agrupar\u00e1n en un \u00fanico cluster. Ventajas y desventajas Ventajas No requiere que el usuario especifique el n\u00famero de clusters. Es independiente de la forma que tengan los clusters. Puede identificar outliers, por lo que los clusters generados no se ven influenciados por ellos. Desventajas Es un m\u00e9todo determin\u00edstico siempre y cuando el orden de los datos sea el mismo. Los border points que son alcanzables desde m\u00e1s de un cluster pueden asignarse a uno u otro dependiendo del orden en el que se procesen los datos. No genera buenos resultados cuando la densidad de los grupos es muy distinta, ya que no es posible encontrar los par\u00e1metros \ud835\udf16 y min_samples que sirvan para todos a la vez. Aplicaci\u00f3n Veamos un ejemplo de an\u00e1lisis no supervisado ocupando el algoritmo DBSCAN . # Tratamiento de datos # ============================================================================== import numpy as np import pandas as pd from sklearn.datasets import make_blobs # Gr\u00e1ficos # ============================================================================== import matplotlib.pyplot as plt from matplotlib import style style . use ( 'ggplot' ) or plt . style . use ( 'ggplot' ) # Preprocesado y modelado # ============================================================================== from sklearn.cluster import DBSCAN from sklearn.preprocessing import scale from sklearn.metrics import silhouette_score # Configuraci\u00f3n warnings # ============================================================================== import warnings warnings . filterwarnings ( 'ignore' ) df = pd . read_csv ( \"data/multishape.csv\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y shape 0 -0.803739 -0.853053 1 1 0.852851 0.367618 1 2 0.927180 -0.274902 1 3 -0.752626 -0.511565 1 4 0.706846 0.810679 1 # graficar sns . set ( rc = { 'figure.figsize' :( 11.7 , 8.27 )}) ax = sns . scatterplot ( data = df , x = \"x\" , y = \"y\" ) # Escalado de datos scaler = StandardScaler () columns = [ 'x' , 'y' ] df [ columns ] = scaler . fit_transform ( df [ columns ]) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y shape 0 -1.120749 -0.193616 1 1 1.448907 0.844692 1 2 1.564203 0.298161 1 3 -1.041463 0.096855 1 4 1.222429 1.221561 1 # Modelo X = np . array ( df [[ 'x' , 'y' ]]) modelo_dbscan = DBSCAN ( eps = 0.2 , min_samples = 5 , metric = 'euclidean' , ) modelo_dbscan . fit ( X ) DBSCAN(eps=0.2) # agregar labels df [ 'labels' ] = modelo_dbscan . labels_ # graficar sns . set ( rc = { 'figure.figsize' :( 11.7 , 8.27 )}) ax = sns . scatterplot ( data = df , x = \"x\" , y = \"y\" , hue = \"labels\" ) # N\u00famero de clusters y observaciones \"outliers\" n_clusters = len ( set ( df [ 'labels' ])) - ( 1 if - 1 in df [ 'labels' ] else 0 ) n_noise = list ( df [ 'labels' ]) . count ( - 1 ) print ( f 'N\u00famero de clusters encontrados: { n_clusters } ' ) print ( f 'N\u00famero de outliers encontrados: { n_noise } ' ) N\u00famero de clusters encontrados: 6 N\u00famero de outliers encontrados: 25 Gaussian mixture models (GMMs) Teor\u00eda Un Gaussian Mixture model es un modelo probabil\u00edstico en el que se considera que las observaciones siguen una distribuci\u00f3n probabil\u00edstica formada por la combinaci\u00f3n de m\u00faltiples distribuciones normales (componentes). En su aplicaci\u00f3n al clustering, puede entenderse como una generalizaci\u00f3n de K-means con la que, en lugar de asignar cada observaci\u00f3n a un \u00fanico cluster, se obtiene una probabilidad de pertenencia a cada uno. Para estimar los par\u00e1metros que definen la funci\u00f3n de distribuci\u00f3n de cada cluster (media y matriz de covarianza) se recurre al algoritmo de Expectation-Maximization (EM). Una vez aprendidos los par\u00e1metros, se puede calcular la probabilidad que tiene cada observaci\u00f3n de pertenecer a cada cluster y asignarla a aquel con mayor probabilidad. Algoritmo Junto con el n\u00famero de clusters (componentes), hay que determinar el tipo de matriz de covarianza que pueden tener los clusters. Dependiendo del tipo de matriz, la forma de los clusters puede ser: tied: todos los clusters comparten la misma matriz de covarianza. diagonal: las dimensiones de cada cluster a lo largo de cada dimensi\u00f3n puede ser distinto, pero las elipses generadas siempre quedan alineadas con los ejes, es decir, su orientaciones son limitadas. spherical: las dimensiones de cada cluster son las mismas en todas las dimensiones. Esto permite generar clusters de distinto tama\u00f1o pero todos esf\u00e9ricos. full: cada cluster puede puede ser modelado como una elipse cualquier orientaci\u00f3n y dimensiones. Aplicaci\u00f3n # Tratamiento de datos # ============================================================================== import numpy as np import pandas as pd from sklearn.datasets import make_blobs # Gr\u00e1ficos # ============================================================================== import matplotlib.pyplot as plt import matplotlib as mpl from matplotlib.patches import Ellipse from matplotlib import style style . use ( 'ggplot' ) or plt . style . use ( 'ggplot' ) # Preprocesado y modelado # ============================================================================== from sklearn.mixture import GaussianMixture from sklearn.preprocessing import scale from sklearn.metrics import silhouette_score # Configuraci\u00f3n warnings # ============================================================================== import warnings warnings . filterwarnings ( 'ignore' ) # generar datos X , y = make_blobs ( n_samples = 300 , n_features = 2 , centers = 4 , cluster_std = 0.60 , shuffle = True , random_state = 0 ) df = pd . DataFrame ({ 'x' : X [:, 0 ], 'y' : X [:, 1 ] }) # Escalado de datos scaler = StandardScaler () columns = [ 'x' , 'y' ] df [ columns ] = scaler . fit_transform ( df [ columns ]) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y 0 0.516255 -0.707227 1 -0.861664 1.329068 2 0.711174 0.437049 3 -0.619792 1.485573 4 0.782282 -0.801378 # graficar sns . set ( rc = { 'figure.figsize' :( 11.7 , 8.27 )}) ax = sns . scatterplot ( data = df , x = \"x\" , y = \"y\" ) # Modelo X = np . array ( df [[ 'x' , 'y' ]]) modelo_gmm = GaussianMixture ( n_components = 4 , covariance_type = 'full' , random_state = 123 ) modelo_gmm . fit ( X = X ) GaussianMixture(n_components=4, random_state=123) # Media de cada componente modelo_gmm . means_ array([[ 0.57844185, 0.17292982], [ 1.2180002 , -1.19725866], [-0.96910551, -0.44143927], [-0.83710796, 1.46219241]]) # Matriz de covarianza de cada componente modelo_gmm . covariances_ array([[[ 0.14277634, -0.00527707], [-0.00527707, 0.05201453]], [[ 0.12745218, -0.00619666], [-0.00619666, 0.05157763]], [[ 0.12131004, 0.00243031], [ 0.00243031, 0.04602115]], [[ 0.15451151, 0.0068188 ], [ 0.0068188 , 0.05660383]]]) Predicci\u00f3n y clasificaci\u00f3n Una vez entrenado el modelo GMMs, se puede predecir la probabilidad que tiene cada observaci\u00f3n de pertenecer a cada una de las componentes (clusters). Para obtener la clasificaci\u00f3n final, se asigna a la componente con mayor probabilidad # Probabilidades # ============================================================================== # Cada fila es una observaci\u00f3n y cada columna la probabilidad de pertenecer a # cada una de las componentes. probabilidades = modelo_gmm . predict_proba ( X ) probabilidades array([[2.59319058e-02, 9.71686641e-01, 2.38145325e-03, 8.05199375e-21], [7.16006205e-09, 7.13831446e-33, 2.34989165e-15, 9.99999993e-01], [9.99999970e-01, 8.78380305e-12, 9.13663813e-09, 2.04805600e-08], ..., [9.99965889e-01, 4.92493619e-10, 3.41016281e-05, 8.75675460e-09], [3.01319652e-06, 6.45628361e-30, 1.52897049e-18, 9.99996987e-01], [4.39337172e-07, 1.99785604e-11, 9.99999561e-01, 4.05381245e-15]]) # Clasificaci\u00f3n (asignaci\u00f3n a la componente de mayor probabilidad) # ============================================================================== # Cada fila es una observaci\u00f3n y cada columna la probabilidad de pertenecer a # cada una de las componentes. clasificacion = modelo_gmm . predict ( X ) clasificacion array([1, 3, 0, 3, 1, 1, 2, 0, 3, 3, 2, 3, 0, 3, 1, 0, 0, 1, 2, 2, 1, 1, 0, 2, 2, 0, 1, 0, 2, 0, 3, 3, 0, 3, 3, 3, 3, 3, 2, 1, 0, 2, 0, 0, 2, 2, 3, 2, 3, 1, 2, 1, 3, 1, 1, 2, 3, 2, 3, 1, 3, 0, 3, 2, 2, 2, 3, 1, 3, 2, 0, 2, 3, 2, 2, 3, 2, 0, 1, 3, 1, 0, 1, 1, 3, 0, 1, 0, 3, 3, 0, 1, 3, 2, 2, 0, 1, 1, 0, 2, 3, 1, 3, 1, 0, 1, 1, 0, 3, 0, 2, 2, 1, 3, 1, 0, 3, 1, 1, 0, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 3, 2, 2, 1, 3, 2, 2, 3, 0, 3, 3, 2, 0, 2, 0, 2, 3, 0, 3, 3, 3, 0, 3, 0, 1, 2, 3, 2, 1, 0, 3, 0, 0, 1, 0, 2, 2, 0, 1, 0, 0, 3, 1, 0, 2, 3, 1, 1, 0, 2, 1, 0, 2, 2, 0, 0, 0, 0, 1, 3, 0, 2, 0, 0, 2, 2, 2, 0, 2, 3, 0, 2, 1, 2, 0, 3, 2, 3, 0, 3, 0, 2, 0, 0, 3, 2, 2, 1, 1, 0, 3, 1, 1, 2, 1, 2, 0, 3, 3, 0, 0, 3, 0, 1, 2, 0, 1, 2, 3, 2, 1, 0, 1, 3, 3, 3, 3, 2, 2, 3, 0, 2, 1, 0, 2, 2, 2, 1, 1, 3, 0, 0, 2, 1, 3, 2, 0, 3, 0, 1, 1, 2, 2, 0, 1, 1, 1, 0, 3, 3, 1, 1, 0, 1, 1, 1, 3, 2, 3, 0, 1, 1, 3, 3, 3, 1, 1, 0, 3, 2]) # Representaci\u00f3n gr\u00e1fica # ============================================================================== # Codigo obtenido de: # https://github.com/amueller/COMS4995-s20/tree/master/slides/aml-14-clustering-mixture-models def make_ellipses ( gmm , ax ): for n in range ( gmm . n_components ): if gmm . covariance_type == 'full' : covariances = gmm . covariances_ [ n ] elif gmm . covariance_type == 'tied' : covariances = gmm . covariances_ elif gmm . covariance_type == 'diag' : covariances = np . diag ( gmm . covariances_ [ n ]) elif gmm . covariance_type == 'spherical' : covariances = np . eye ( gmm . means_ . shape [ 1 ]) * gmm . covariances_ [ n ] v , w = np . linalg . eigh ( covariances ) u = w [ 0 ] / np . linalg . norm ( w [ 0 ]) angle = np . arctan2 ( u [ 1 ], u [ 0 ]) angle = 180 * angle / np . pi # convert to degrees v = 2. * np . sqrt ( 2. ) * np . sqrt ( v ) for i in range ( 1 , 3 ): ell = mpl . patches . Ellipse ( gmm . means_ [ n ], i * v [ 0 ], i * v [ 1 ], 180 + angle , color = \"blue\" ) ell . set_clip_box ( ax . bbox ) ell . set_alpha ( 0.1 ) ax . add_artist ( ell ) fig , axs = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) # Distribuci\u00f3n de probabilidad de cada componente for i in np . unique ( clasificacion ): axs [ 0 ] . scatter ( x = X [ clasificacion == i , 0 ], y = X [ clasificacion == i , 1 ], c = plt . rcParams [ 'axes.prop_cycle' ] . by_key ()[ 'color' ][ i ], marker = 'o' , edgecolor = 'black' , label = f \"Componente { i } \" ) make_ellipses ( modelo_gmm , ax = axs [ 0 ]) axs [ 0 ] . set_title ( 'Distribuci\u00f3n de prob. de cada componente' ) axs [ 0 ] . legend () # Distribuci\u00f3n de probabilidad del modelo completo xs = np . linspace ( min ( X [:, 0 ]), max ( X [:, 0 ]), 1000 ) ys = np . linspace ( min ( X [:, 1 ]), max ( X [:, 1 ]), 1000 ) xx , yy = np . meshgrid ( xs , ys ) scores = modelo_gmm . score_samples ( np . c_ [ xx . ravel (), yy . ravel ()], ) axs [ 1 ] . scatter ( X [:, 0 ], X [:, 1 ], s = 5 , alpha = .6 , c = plt . cm . tab10 ( clasificacion )) scores = np . exp ( scores ) # Las probabilidades est\u00e1n en log axs [ 1 ] . contour ( xx , yy , scores . reshape ( xx . shape ), levels = np . percentile ( scores , np . linspace ( 0 , 100 , 10 ))[ 1 : - 1 ] ) axs [ 1 ] . set_title ( 'Distribuci\u00f3n de prob. del modelo completo' ); N\u00famero de clusters Dado que los modelos GMM son modelos probabil\u00edsticos, se puede recurrir a m\u00e9tricas como el Akaike information criterion (AIC) o Bayesian information criterion (BIC) para identificar c\u00f3mo de bien se ajustan los datos observados a modelo creado. n_components = range ( 1 , 21 ) valores_bic = [] valores_aic = [] for i in n_components : modelo = GaussianMixture ( n_components = i , covariance_type = \"full\" ) modelo = modelo . fit ( X ) valores_bic . append ( modelo . bic ( X )) valores_aic . append ( modelo . aic ( X )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 12 , 6 )) ax . plot ( n_components , valores_bic , label = 'BIC' ) ax . plot ( n_components , valores_aic , label = 'AIC' ) ax . set_title ( \"Valores BIC y AIC\" ) ax . set_xlabel ( \"N\u00famero componentes\" ) ax . legend (); print ( f \"N\u00famero \u00f3ptimo acorde al BIC: { range ( 1 , 21 )[ np . argmin ( valores_bic )] } \" ) print ( f \"N\u00famero \u00f3ptimo acorde al AIC: { range ( 1 , 21 )[ np . argmin ( valores_aic )] } \" ) N\u00famero \u00f3ptimo acorde al BIC: 4 N\u00famero \u00f3ptimo acorde al AIC: 4 Ambas m\u00e9tricas identifican el 4 como n\u00famero \u00f3ptimo de clusters (componentes). Limitaciones del clustering El clustering puede ser una herramienta muy \u00fatil para encontrar agrupaciones en los datos, sobre todo a medida que el volumen de los mismos aumenta. Sin embargo, es importante recordar sus limitaciones o problemas que pueden surgir al aplicarlo. Algunas de ellas son: Peque\u00f1as decisiones pueden tener grandes consecuencias:a la hora de utilizar los m\u00e9todos de clustering se tienen que tomar decisiones que influyen en gran medida en los resultados obtenidos. No existe una \u00fanica respuesta correcta, por lo que en la pr\u00e1ctica se prueban diferentes opciones. Escalado y centrado de las variables Qu\u00e9 medida de distancia/similitud emplear N\u00famero de clusters Tipo de linkage empleado en hierarchical clustering A que altura establecer el corte de un dendrograma Validaci\u00f3n de los clusters obtenidos: no es f\u00e1cil comprobar la validez de los resultados ya que en la mayor\u00eda de escenarios se desconoce la verdadera agrupaci\u00f3n. Falta de robustez: los m\u00e9todos de K-means-clustering e hierarchical clustering asignan obligatoriamente cada observaci\u00f3n a un grupo. Si existe en la muestra alg\u00fan outlier, a pesar de que realmente no pertenezca a ning\u00fan grupo, el algoritmo lo asignar\u00e1 a uno de ellos provocando una distorsi\u00f3n significativa del cluster en cuesti\u00f3n. Algunas alternativas son k-medoids y DBSCAN. La naturaleza del algoritmo de hierarchical clustering conlleva que, si se realiza una mala divisi\u00f3n en los pasos iniciales, no se pueda corregir en los pasos siguientes. Referencias Unsupervised learning Clustering con Python (Joaqu\u00edn Amat Rodrigo)","title":"Clustering"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#clustering","text":"El Clustering es la tarea de agrupar objetos por similitud, en grupos o conjuntos de manera que los miembros del mismo grupo tengan caracter\u00edsticas similares. Es la tarea principal de la miner\u00eda de datos exploratoria y es una t\u00e9cnica com\u00fan en el an\u00e1lisis de datos estad\u00edsticos.","title":"Clustering"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#k-means","text":"","title":"K-means"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#teoria","text":"El algoritmo K-means (MacQueen, 1967) agrupa las observaciones en un n\u00famero predefinido de \\(k\\) clusters de forma que, la suma de las varianzas internas de los clusters, sea lo menor posible. Existen varias implementaciones de este algoritmo, la m\u00e1s com\u00fan de ellas se conoce como Lloyd\u2019s. En la bibliograf\u00eda es com\u00fan encontrar los t\u00e9rminos inertia, within-cluster sum-of-squares o varianza intra-cluster para referirse a la varianza interna de los clusters. Consid\u00e9rense \\(\ud835\udc36_1 ,..., \ud835\udc36_k\\) como los sets formados por los \u00edndices de las observaciones de cada uno de los clusters. Por ejemplo, el set \\(\ud835\udc36_1\\) contiene los \u00edndices de las observaciones agrupadas en el cluster 1. La nomenclatura empleada para indicar que la observaci\u00f3n \\(i\\) pertenece al cluster \\(k\\) es: \\(i \\in C_k\\) . Todos los sets satisfacen dos propiedades: $C_1 \\cup C_2 \\cup ... \\cup C_k = {1,...,n} $ . Significa que toda observaci\u00f3n pertenece a uno de los \\(k\\) clusters. $C_i \\cap C_{j} = \\emptyset $ para todo \\(i \\neq j\\) . Implica que los clusters no solapan, ninguna observaci\u00f3n pertenece a m\u00e1s de un cluster a la vez. El algoritmo consiste en reducir al m\u00ednimo la suma de las distancias cuadradas desde la media dentro del agrupamiento. Matem\u00e1ticamente: \\begin{align } (P) \\ \\textrm{Minimizar } f(C_l,\\mu_l) = \\sum_{l=1}^k \\sum_{x_n \\in C_l} ||x_n - \\mu_l ||^2 \\textrm{, respecto a } C_l, \\mu_l, \\end{align } donde \\(C_l\\) es el cluster l-\u00e9simo y \\(\\mu_l\\) es el centroide l-\u00e9simo.","title":"Teor\u00eda"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#algoritmo","text":"Especificar el n\u00famero \\(k\\) de clusters que se quieren crear. Seleccionar de forma aleatoria \\(k\\) observaciones del set de datos como centroides iniciales. Asignar cada una de las observaciones al centroide m\u00e1s cercano. Para cada uno de los \\(k\\) clusters generados en el paso 3, recalcular su centroide. Repetir los pasos 3 y 4 hasta que las asignaciones no cambien o se alcance el n\u00famero m\u00e1ximo de iteraciones establecido. El problema anterior es NP-hard (imposible de resolver en tiempo polinomial, del tipo m\u00e1s dif\u00edcil de los probleams NP).","title":"Algoritmo"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#ventajas-y-desventajas","text":"K-means es uno de los m\u00e9todos de clustering m\u00e1s utilizados. Destaca por la sencillez y velocidad de su algoritmo, sin embargo, presenta una serie de limitaciones que se deben tener en cuenta. Requiere que se indique de antemano el n\u00famero de clusters que se van a crear. Esto puede ser complicado si no se dispone de informaci\u00f3n adicional sobre los datos con los que se trabaja. Se han desarrollado varias estrategias para ayudar a identificar potenciales valores \u00f3ptimos de \\(k\\) (elbow, shilouette), pero todas ellas son orientativas. Dificultad para detectar clusters alargados o con formas irregulares. Las agrupaciones resultantes pueden variar dependiendo de la asignaci\u00f3n aleatoria inicial de los centroides. Para minimizar este problema, se recomienda repetir el proceso de clustering entre 25-50 veces y seleccionar como resultado definitivo el que tenga menor suma total de varianza interna. Aun as\u00ed, solo se puede garantizar la reproducibilidad de los resultados si se emplean semillas. Presenta problemas de robustez frente a outliers.","title":"Ventajas y desventajas"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#aplicacion","text":"Veamos un ejemplo de an\u00e1lisis no supervisado ocupando el algoritmo k-means . # librerias import os import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.datasets import make_blobs pd . set_option ( 'display.max_columns' , 500 ) # Ver m\u00e1s columnas de los dataframes # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab % matplotlib inline def init_blobs ( N , k , seed = 42 ): X , y = make_blobs ( n_samples = N , centers = k , random_state = seed , cluster_std = 0.60 ) return X # generar datos data = init_blobs ( 10000 , 6 , seed = 43 ) df = pd . DataFrame ( data , columns = [ \"x\" , \"y\" ]) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y 0 -6.953617 -4.989933 1 -2.681117 7.583914 2 -1.510161 4.933676 3 -9.748491 5.479457 4 -7.438017 -4.597754 Debido a que trabajamos con el concepto de distancia, muchas veces las columnas del dataframe pueden estar en distintas escalas, lo cual puede complicar a los algoritmos ocupados (al menos con sklearn ). En estos casos, se suele normalizar los atributos, es decir, dejar los valores en una escala acotada y/o con estimadores fijos. Por ejemplo, en *sklearn podemos encontrar las siguientes formas de normalizar: StandardScaler : se normaliza restando la media y escalando por su desviaci\u00f3n estanda. \\( \\(x_{prep} = \\dfrac{x-u}{s}\\) \\) La ventaja es que la media del nuevo conjunto de datos cumple con la propiedad que su media \\(\\mu\\) es igual a cero y su desviaci\u00f3n estandar \\(s\\) es igual a 1. * MinMaxScaler : se normaliza ocupando los valores de los m\u00ednimos y m\u00e1ximo del conjunto de datos. \\( \\(x_{prep} = \\dfrac{x-x_{min}}{x_{min}-x_{max}}\\) \\) Esta forma de normalizar resulta \u00fatil cuando la desviaci\u00f3n estandar \\(s\\) es muy peque\u00f1a (cercana) a cero, por lo que lo convierte en un estimador m\u00e1s roubusto que el StandardScaler . from sklearn.preprocessing import StandardScaler scaler = StandardScaler () columns = [ 'x' , 'y' ] df [ columns ] = scaler . fit_transform ( df [ columns ]) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y 0 -0.579033 -1.831435 1 0.408821 1.194578 2 0.679560 0.556774 3 -1.225241 0.688121 4 -0.691032 -1.737053 # comprobar resultados del estimador df . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y count 1.000000e+04 1.000000e+04 mean 2.060574e-16 -2.285105e-15 std 1.000050e+00 1.000050e+00 min -1.638247e+00 -2.410317e+00 25% -8.015576e-01 -4.418042e-01 50% -2.089351e-01 1.863259e-01 75% 5.480066e-01 8.159808e-01 max 2.243358e+00 1.639547e+00 Con esta parametrizaci\u00f3n procedemos a graficar nuestros resultados: # graficar sns . set ( rc = { 'figure.figsize' :( 11.7 , 8.27 )}) ax = sns . scatterplot ( data = df , x = \"x\" , y = \"y\" ) Ahora ajustamos el algoritmo KMeans de sklearn . Primero, comprendamos los hiperpar\u00e1metros m\u00e1s importantes: n_clusters : El n\u00famero de clusters a crear, o sea K . Por defecto es 8 init : M\u00e9todo de inicializaci\u00f3n. Un problema que tiene el algoritmo K-Medias es que la solucci\u00f3n alcanzada varia seg\u00fan la inicializaci\u00f3n de los centroides. sklearn empieza usando el m\u00e9todo kmeans++ que es una versi\u00f3n m\u00e1s moderna y que proporciona mejores resultados que la inicializaci\u00f3n aleatoria (random) n_init : El n\u00famero de inicializaciones a probar. B\u00e1sicamente KMeans aplica el algoritmo n_init veces y elige los clusters que minimizan la inercia. max_iter : M\u00e1ximo n\u00famero de iteraciones para llegar al criterio de parada. random_state : semilla para garantizar la reproducibilidad de los resultados. tol : Tolerancia para declarar criterio de parada (cuanto m\u00e1s grande, antes parar\u00e1 el algoritmo). # ajustar modelo: k-means from sklearn.cluster import KMeans X = np . array ( df ) kmeans = KMeans ( n_clusters = 6 , n_init = 25 , random_state = 123 ) kmeans . fit ( X ) centroids = kmeans . cluster_centers_ # centros clusters = kmeans . labels_ # clusters # etiquetar los datos con los clusters encontrados df [ \"cluster\" ] = clusters df [ \"cluster\" ] = df [ \"cluster\" ] . astype ( 'category' ) centroids_df = pd . DataFrame ( centroids , columns = [ \"x\" , \"y\" ]) centroids_df [ \"cluster\" ] = [ 1 , 2 , 3 , 4 , 5 , 6 ] # graficar los datos etiquetados con k-means fig , ax = plt . subplots ( figsize = ( 11 , 8.5 )) sns . scatterplot ( data = df , x = \"x\" , y = \"y\" , hue = \"cluster\" , legend = 'full' , palette = \"Set2\" ) sns . scatterplot ( x = \"x\" , y = \"y\" , s = 100 , color = \"black\" , marker = \"x\" , data = centroids_df ) plt . show () Ahora la pregunta que surge de manera natural es ... \u00bf c\u00f3mo escoger el mejor n\u00famero de clusters?. No existe un criterio objetivo ni ampliamente v\u00e1lido para la elecci\u00f3n de un n\u00famero \u00f3ptimo de clusters. Aunque no exista un criterio objetivo para la selecci\u00f3n del n\u00famero de clusters, si que se han implementado diferentes m\u00e9todos que nos ayudan a elegir un n\u00famero apropiado de clusters para agrupar los datos; como son, m\u00e9todo del codo (elbow method) criterio de Calinsky Affinity Propagation (AP) Gap (tambi\u00e9n con su versi\u00f3n estad\u00edstica) Dendrogramas etc.","title":"Aplicaci\u00f3n"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#regla-del-codo","text":"Este m\u00e9todo utiliza los valores de la funci\u00f3n de perdida, \\(f(C_l,\\mu_l)\\) , obtenidos tras aplicar el \\(K\\) -means a diferente n\u00famero de Clusters (desde 1 a \\(N\\) clusters). Una vez obtenidos los valores de la funci\u00f3n de p\u00e9rdida tras aplicar el K-means de 1 a \\(N\\) clusters, representamos en una gr\u00e1fica lineal la funci\u00f3n de p\u00e9rdida respecto del n\u00famero de clusters. En esta gr\u00e1fica se deber\u00eda de apreciar un cambio brusco en la evoluci\u00f3n de la funci\u00f3n de p\u00e9rdida, teniendo la l\u00ednea representada una forma similar a la de un brazo y su codo. El punto en el que se observa ese cambio brusco en la funci\u00f3n de p\u00e9rdida nos dir\u00e1 el n\u00famero \u00f3ptimo de clusters a seleccionar para ese data set; o dicho de otra manera: el punto que representar\u00eda al codo del brazo ser\u00e1 el n\u00famero \u00f3ptimo de clusters para ese data set . # implementaci\u00f3n de la regla del codo Nc = range ( 1 , 15 ) kmeans = [ KMeans ( n_clusters = i ) for i in Nc ] score = [ kmeans [ i ] . fit ( df ) . inertia_ for i in range ( len ( kmeans ))] df_Elbow = pd . DataFrame ({ 'Number of Clusters' : Nc , 'Score' : score }) df_Elbow . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Number of Clusters Score 0 1 49337.951600 1 2 20004.858535 2 3 12733.014667 3 4 6760.679396 4 5 3139.657771 # graficar los datos etiquetados con k-means fig , ax = plt . subplots ( figsize = ( 11 , 8.5 )) plt . title ( 'Elbow Curve' ) sns . lineplot ( x = \"Number of Clusters\" , y = \"Score\" , data = df_Elbow ) sns . scatterplot ( x = \"Number of Clusters\" , y = \"Score\" , data = df_Elbow ) plt . show () A partir de 4 clusters la reducci\u00f3n en la suma total de cuadrados internos parece estabilizarse, indicando que \\(k\\) = 4 es una buena opci\u00f3n.","title":"Regla del codo"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#hierarchical-clustering","text":"","title":"Hierarchical clustering"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#teoria_1","text":"Hierarchical clustering es una alternativa a los m\u00e9todos de partitioning clustering que no requiere que se pre-especifique el n\u00famero de clusters. Los m\u00e9todos que engloba el hierarchical clustering se subdividen en dos tipos dependiendo de la estrategia seguida para crear los grupos: Aglomerativo (agglomerative clustering o bottom-up): el agrupamiento se inicia con todas las observaciones separadas, cada una formando un cluster individual. Los clusters se van combinado a medida que la estructura crece hasta converger en uno solo. Divisivo (divisive clustering o top-down): es la estrategia opuesta al aglomerativo. Se inicia con todas las observaciones contenidas en un mismo cluster y se suceden divisiones hasta que cada observaci\u00f3n forma un cluster* individual. En ambos casos, los resultados pueden representarse de forma muy intuitiva en una estructura de \u00e1rbol llamada dendrograma.","title":"Teor\u00eda"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#algoritmo_1","text":"","title":"Algoritmo"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#aglomerativo","text":"El algoritmo seguido para por el clustering aglomerativo es: Considerar cada una de las n observaciones como un cluster individual, formando as\u00ed la base del dendrograma (hojas). Proceso iterativo hasta que todas las observaciones pertenecen a un \u00fanico cluster: Calcular la distancia entre cada posible par de los n clusters. El investigador debe determinar el tipo de medida empleada para cuantificar la similitud entre observaciones o grupos (distancia y linkage). Los dos clusters m\u00e1s similares se fusionan, de forma que quedan n-1 clusters. Cortar la estructura de \u00e1rbol generada (dendrograma) a una determinada altura para crear los clusters finales. Para que el proceso de agrupamiento pueda llevarse a cabo tal como indica el algoritmo anterior, es necesario definir c\u00f3mo se cuantifica la similitud entre dos clusters. Es decir, se tiene que extender el concepto de distancia entre pares de observaciones para que sea aplicable a pares de grupos, cada uno formado por varias observaciones. A este proceso se le conoce como linkage. A continuaci\u00f3n, se describen los 5 tipos de linkage m\u00e1s empleados y sus definiciones. Complete or Maximum : se calcula la distancia entre todos los posibles pares formados por una observaci\u00f3n del cluster A y una del cluster B. La mayor de todas ellas se selecciona como la distancia entre los dos clusters. Se trata de la medida m\u00e1s conservadora (maximal intercluster dissimilarity). Single or Minimum : se calcula la distancia entre todos los posibles pares formados por una observaci\u00f3n del cluster A y una del cluster B. La menor de todas ellas se selecciona como la distancia entre los dos clusters. Se trata de la medida menos conservadora (minimal intercluster dissimilarity). Average : Se calcula la distancia entre todos los posibles pares formados por una observaci\u00f3n del cluster A y una del cluster B. El valor promedio de todas ellas se selecciona como la distancia entre los dos clusters (mean intercluster dissimilarity). Centroid : Se calcula el centroide de cada uno de los clusters y se selecciona la distancia entre ellos como la distancia entre los dos clusters. Ward : Se trata de un m\u00e9todo general. La selecci\u00f3n del par de clusters que se combinan en cada paso del agglomerative hierarchical clustering se basa en el valor \u00f3ptimo de una funci\u00f3n objetivo, pudiendo ser esta \u00faltima cualquier funci\u00f3n definida por el analista. El m\u00e9todo Ward's minimum variance es un caso particular en el que el objetivo es minimizar la suma total de varianza intra-cluster. En cada paso, se identifican aquellos 2 clusters cuya fusi\u00f3n conlleva menor incremento de la varianza total intra-cluster. Esta es la misma m\u00e9trica que se minimiza en K-means. Los m\u00e9todos de complete, average y Ward's minimum variance suelen ser los preferidos por los analistas debido a que generan dendrogramas m\u00e1s compensados. Sin embargo, no se puede determinar que uno sea mejor que otro, ya que depende del caso de estudio en cuesti\u00f3n. Por ejemplo, en gen\u00f3mica, se emplea con frecuencia el m\u00e9todo de centroides. Junto con los resultados de un proceso de hierarchical clustering siempre hay que indicar qu\u00e9 distancia se ha empleado, as\u00ed como el tipo de linkage, ya que, dependiendo de estos, los resultados pueden variar en gran medida.","title":"Aglomerativo"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#divisivo","text":"El algoritmo m\u00e1s conocido de divisive hierarchical clustering es DIANA (DIvisive ANAlysis Clustering). Este algoritmo se inicia con un \u00fanico cluster que contiene todas las observaciones. A continuaci\u00f3n, se van sucediendo divisiones hasta que cada observaci\u00f3n forma un cluster independiente. En cada iteraci\u00f3n, se selecciona el cluster con mayor di\u00e1metro, entendiendo por di\u00e1metro de un cluster la mayor de las diferencias entre dos de sus observaciones. Una vez seleccionado el cluster, se identifica la observaci\u00f3n m\u00e1s dispar, que es aquella con mayor distancia promedio respecto al resto de observaciones que forman el cluster. Esta observaci\u00f3n inicia el nuevo cluster. Se reasignan las observaciones en funci\u00f3n de si est\u00e1n m\u00e1s pr\u00f3ximas al nuevo cluster o al resto de la partici\u00f3n, dividiendo as\u00ed el cluster seleccionado en dos nuevos clusters. Todas las \\(n\\) observaciones forman un \u00fanico cluster. Repetir hasta que haya \\(n\\) clusters: Calcular para cada cluster la mayor de las distancias entre pares de observaciones (di\u00e1metro del cluster). Seleccionar el cluster con mayor di\u00e1metro. Calcular la distancia media de cada observaci\u00f3n respecto a las dem\u00e1s. La observaci\u00f3n m\u00e1s distante inicia un nuevo cluster. Se reasignan las observaciones restantes al nuevo cluster o al viejo dependiendo de cu\u00e1l est\u00e1 m\u00e1s pr\u00f3ximo. A diferencia del clustering aglomerativo, en el que hay que elegir un tipo de distancia y un m\u00e9todo de linkage, en el clustering divisivo solo hay que elegir la distancia, no hay linkage.","title":"Divisivo"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#dendograma","text":"Los resultados del hierarchical clustering pueden representarse como un \u00e1rbol en el que las ramas representan la jerarqu\u00eda con la que se van sucediendo las uniones de clusters. Sup\u00f3ngase que se dispone de 45 observaciones en un espacio de dos dimensiones, a los que se les aplica hierarchical clustering para intentar identificar grupos. El siguiente dendrograma representa los resultados obtenidos. En la base del dendrograma, cada observaci\u00f3n forma una terminaci\u00f3n individual conocida como hoja o leaf del \u00e1rbol. A medida que se asciende por la estructura, pares de hojas se fusionan formando las primeras ramas. Estas uniones se corresponden con los pares de observaciones m\u00e1s similares. Tambi\u00e9n ocurre que las ramas se fusionan con otras ramas o con hojas. Cuanto m\u00e1s temprana (m\u00e1s pr\u00f3xima a la base del dendrograma) ocurre una fusi\u00f3n, mayor es la similitud. Para cualquier par de observaciones, se puede identificar el punto del \u00e1rbol en el que las ramas que contienen dichas observaciones se fusionan. La altura a la que esto ocurre (eje vertical) indica c\u00f3mo de similares/diferentes son las dos observaciones. Los dendrogramas, por lo tanto, se deben interpretar \u00fanicamente en base al eje vertical y no por las posiciones que ocupan las observaciones en el eje horizontal, esto \u00faltimo es simplemente por est\u00e9tica y puede variar de un programa a otro. Por ejemplo, la observaci\u00f3n 8 es la m\u00e1s similar a la 10 ya que es la primera fusi\u00f3n que recibe la observaci\u00f3n 10 (y viceversa). Podr\u00eda resultar tentador decir que la observaci\u00f3n 14, situada inmediatamente a la derecha de la 10, es la siguiente m\u00e1s similar, sin embargo, las observaciones 28 y 44 son m\u00e1s similares a la 10 a pesar de que se encuentran m\u00e1s alejadas en el eje horizontal. Del mismo modo, no es correcto decir que la observaci\u00f3n 14 es m\u00e1s similar a la observaci\u00f3n 10 de lo que lo es la 36 por el hecho de que est\u00e1 m\u00e1s pr\u00f3xima en el eje horizontal. Prestando atenci\u00f3n a la altura en que las respectivas ramas se unen, la \u00fanica conclusi\u00f3n v\u00e1lida es que la similitud entre los pares 10-14 y 10-36 es la misma. Cortar el dendograma para generar los clusters Adem\u00e1s de representar en un dendrograma la similitud entre observaciones, se tiene que identificar el n\u00famero de clusters creados y qu\u00e9 observaciones forman parte de cada uno. Si se realiza un corte horizontal a una determinada altura del dendrograma, el n\u00famero de ramas que sobrepasan (en sentido ascendente) dicho corte se corresponde con el n\u00famero de clusters. La siguiente imagen muestra dos veces el mismo dendrograma. Si se realiza el corte a la altura de 5, se obtienen dos clusters, mientras que si se hace a la de 3.5 se obtienen 4. La altura de corte tiene por lo tanto la misma funci\u00f3n que el valor K en K-means-clustering: controla el n\u00famero de clusters obtenidos. Dos propiedades adicionales se derivan de la forma en que se generan los clusters en el m\u00e9todo de hierarchical clustering: Dada la longitud variable de las ramas, siempre existe un intervalo de altura para el que cualquier corte da lugar al mismo n\u00famero de clusters. En el ejemplo anterior, todos los cortes entre las alturas 5 y 6 tienen como resultado los mismos 2 clusters. Con un solo dendrograma se dispone de la flexibilidad para generar cualquier n\u00famero de clusters desde 1 a n. La selecci\u00f3n del n\u00famero \u00f3ptimo puede valorarse de forma visual, tratando de identificar las ramas principales en base a la altura a la que ocurren las uniones. En el ejemplo expuesto es razonable elegir entre 2 o 4 clusters.","title":"Dendograma"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#aplicacion_1","text":"# Tratamiento de datos # ============================================================================== import numpy as np import pandas as pd from sklearn.datasets import make_blobs # Gr\u00e1ficos # ============================================================================== import matplotlib.pyplot as plt from matplotlib import style style . use ( 'ggplot' ) or plt . style . use ( 'ggplot' ) # Preprocesado y modelado # ============================================================================== from sklearn.cluster import AgglomerativeClustering from scipy.cluster.hierarchy import dendrogram from sklearn.preprocessing import scale from sklearn.metrics import silhouette_score # Configuraci\u00f3n warnings # ============================================================================== import warnings warnings . filterwarnings ( 'ignore' ) # generar datos X , y = make_blobs ( n_samples = 200 , n_features = 2 , centers = 4 , cluster_std = 0.60 , shuffle = True , random_state = 0 ) df = pd . DataFrame ({ 'x' : X [:, 0 ], 'y' : X [:, 1 ] }) # Escalado de datos scaler = StandardScaler () columns = [ 'x' , 'y' ] df [ columns ] = scaler . fit_transform ( df [ columns ]) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y 0 1.348818 -0.908114 1 -0.638621 -0.534950 2 0.653079 0.027910 3 -1.573023 1.276049 4 0.970706 -1.418431 # graficar sns . set ( rc = { 'figure.figsize' :( 11.7 , 8.27 )}) ax = sns . scatterplot ( data = df , x = \"x\" , y = \"y\" ) # Modelos X = np . array ( df [[ 'x' , 'y' ]]) # primer modelo modelo_hclust_complete = AgglomerativeClustering ( affinity = 'euclidean' , linkage = 'complete' , distance_threshold = 0 , n_clusters = None ) modelo_hclust_complete . fit ( X ) # segundo modelo modelo_hclust_average = AgglomerativeClustering ( affinity = 'euclidean' , linkage = 'average' , distance_threshold = 0 , n_clusters = None ) modelo_hclust_average . fit ( X ) # tercer modelo modelo_hclust_ward = AgglomerativeClustering ( affinity = 'euclidean' , linkage = 'ward' , distance_threshold = 0 , n_clusters = None ) modelo_hclust_ward . fit ( X ) AgglomerativeClustering(distance_threshold=0, n_clusters=None) def plot_dendrogram ( model , ** kwargs ): ''' Esta funci\u00f3n extrae la informaci\u00f3n de un modelo AgglomerativeClustering y representa su dendograma con la funci\u00f3n dendogram de scipy.cluster.hierarchy ''' counts = np . zeros ( model . children_ . shape [ 0 ]) n_samples = len ( model . labels_ ) for i , merge in enumerate ( model . children_ ): current_count = 0 for child_idx in merge : if child_idx < n_samples : current_count += 1 # leaf node else : current_count += counts [ child_idx - n_samples ] counts [ i ] = current_count linkage_matrix = np . column_stack ([ model . children_ , model . distances_ , counts ]) . astype ( float ) # Plot dendrogram ( linkage_matrix , ** kwargs ) # Dendrogramas plt . figure ( figsize = ( 20 , 10 )) plot_dendrogram ( modelo_hclust_average , color_threshold = 0 ) plt . title ( \"Distancia eucl\u00eddea, Linkage average\" ) plt . show () Text(0.5, 1.0, 'Distancia eucl\u00eddea, Linkage average') # Dendrogramas plt . figure ( figsize = ( 20 , 10 )) plot_dendrogram ( modelo_hclust_complete , color_threshold = 0 ) plt . title ( \"Distancia eucl\u00eddea, Linkage complete\" ) plt . show () # Dendrogramas plt . figure ( figsize = ( 20 , 10 )) plot_dendrogram ( modelo_hclust_ward , color_threshold = 0 ) plt . title ( \"Distancia eucl\u00eddea, Linkage ward\" ) plt . show () En este caso, los tres tipos de linkage identifican claramente 4 clusters, si bien esto no significa que en los 3 dendrogramas los clusters est\u00e9n formados por exactamente las mismas observaciones. N\u00famero de clusters Una forma de identificar el n\u00famero de clusters, es inspeccionar visualmente el dendograma y decidir a qu\u00e9 altura se corta para generar los clusters. Por ejemplo, para los resultados generados mediante distancia eucl\u00eddea y linkage ward, parece sensato cortar el dendograma a una altura de entre 5 y 10, de forma que se creen 4 clusters. plt . figure ( figsize = ( 20 , 10 )) altura_corte = 6 plot_dendrogram ( modelo_hclust_ward , color_threshold = altura_corte ) plt . title ( \"Distancia eucl\u00eddea, Linkage ward\" ) plt . axhline ( y = altura_corte , c = 'black' , linestyle = '--' , label = 'altura corte' ) plt . legend () plt . show () Una vez identificado el n\u00famero \u00f3ptimo de clusters, se reentrena el modelo indicando este valor.","title":"Aplicaci\u00f3n"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#density-based-clustering-dbscan","text":"","title":"Density based clustering (DBSCAN)"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#teoria_2","text":"Density-based spatial clustering of applications with noise (DBSCAN) fue presentado en 1996 por Ester et al. como una forma de identificar clusters siguiendo el modo intuitivo en el que lo hace el cerebro humano, identificando regiones con alta densidad de observaciones separadas por regiones de baja densidad. El cerebro humano identifica f\u00e1cilmente 5 agrupaciones y algunas observaciones aisladas (ruido). V\u00e9anse ahora los clusters que se obtienen si se aplica, por ejemplo, K-means clustering. Los clusters generados distan mucho de representar las verdaderas agrupaciones. Esto es as\u00ed porque los m\u00e9todos de partitioning clustering como k-means, hierarchical, k-medoids, ... son buenos encontrando agrupaciones con forma esf\u00e9rica o convexa que no contengan un exceso de outliers o ruido, pero fallan al tratar de identificar formas arbitrarias. De ah\u00ed que el \u00fanico cluster que se corresponde con un grupo real sea el amarillo. DBSCAN evita este problema siguiendo la idea de que, para que una observaci\u00f3n forme parte de un cluster, tiene que haber un m\u00ednimo de observaciones vecinas dentro de un radio de proximidad y de que los clusters est\u00e1n separados por regiones vac\u00edas o con pocas observaciones. El algoritmo DBSCAN necesita dos par\u00e1metros: Epsilon ( \\(\\epsilon\\) ) : radio que define la regi\u00f3n vecina a una observaci\u00f3n, tambi\u00e9n llamada \ud835\udf16 -neighborhood. Minimum points ( \\(min_samples\\) ): n\u00famero m\u00ednimo de observaciones dentro de la regi\u00f3n epsilon. Empleando estos dos par\u00e1metros, cada observaci\u00f3n del set de datos se puede clasificar en una de las siguientes tres categor\u00edas: Core point : observaci\u00f3n que tiene en su \ud835\udf16 -neighborhood un n\u00famero de observaciones vecinas igual o mayor a min_samples. Border point : observaci\u00f3n no satisface el m\u00ednimo de observaciones vecinas para ser core point pero que pertenece al \\(\\epsilon\\) -neighborhood de otra observaci\u00f3n que s\u00ed es core point. Noise-outlier : observaci\u00f3n que no es core point ni border point. Por \u00faltimo, empleando las tres categor\u00edas anteriores se pueden definir tres niveles de conectividad entre observaciones: Directamente alcanzable (direct density reachable): una observaci\u00f3n \\(A\\) es directamente alcanzable desde otra observaci\u00f3n \\(B\\) si \\(A\\) forma parte del \\(\\epsilon\\) -neighborhood de \\(B\\) y \\(B\\) es un core point. Por definici\u00f3n, las observaciones solo pueden ser directamente alcanzables desde un core point. Alcanzable (density reachable): una observaci\u00f3n \\(A\\) es alcanzable desde otra observaci\u00f3n \\(\ud835\udc35\\) si existe una secuencia de core points que van desde \\(B\\) a \\(A\\) . Densamente conectadas (density conected): dos observaciones \\(A\\) y \\(B\\) est\u00e1n densamente conectadas si existe una observaci\u00f3n core point \\(C\\) tal que \\(A\\) y \\(B\\) son alcanzables desde \\(C\\) . La siguiente imagen muestra las conexiones existentes entre un conjunto de observaciones si se emplea \\(min_samples = 4\\) . La observaci\u00f3n \\(A\\) y el resto de observaciones marcadas en rojo son core points, ya que todas ellas contienen al menos 4 observaciones vecinas (incluy\u00e9ndose a ellas mismas) en su \\(\\epsilon\\) -neighborhood. Como todas son alcanzables entre ellas, forman un cluster. Las observaciones \\(B\\) y \\(C\\) no son core points pero son alcanzables desde \\(A\\) a trav\u00e9s de otros core points, por lo tanto, pertenecen al mismo cluster que \\(A\\) . La observaci\u00f3n \\(N\\) no es ni un core point ni es directamente alcanzable, por lo que se considera como ruido.","title":"Teor\u00eda"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#algoritmo_2","text":"Para cada observaci\u00f3n \\(x_i\\) calcular la distancia entre ella y el resto de observaciones. Si en su \\(epsilon\\) -neighborhood hay un n\u00famero de observaciones \\(\\geq min_samples\\) marcar la observaci\u00f3n como core point, de lo contrario marcarla como visitada. Para cada observaci\u00f3n \\(x_i\\) marcada como core point, si todav\u00eda no ha sido asignada a ning\u00fan cluster, crear uno nuevo y asignarla a \u00e9l. Encontrar recursivamente todas las observaciones densamente conectadas a ella y asignarlas al mismo cluster. Iterar el mismo proceso para todas las observaciones que no hayan sido visitadas. Aquellas observaciones que tras haber sido visitadas no pertenecen a ning\u00fan cluster se marcan como outliers. Como resultado, todo cluster cumple dos propiedades: todos los puntos que forman parte de un mismo cluster est\u00e1n densamente conectados entre ellos y, si una observaci\u00f3n \\(A\\) es densamente alcanzable desde cualquier otra observaci\u00f3n de un cluster, entonces \\(A\\) tambi\u00e9n pertenece al cluster.","title":"Algoritmo"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#hiperparametros","text":"Como ocurre en muchas otras t\u00e9cnicas estad\u00edsticas, en DBSCAN no existe una forma \u00fanica y exacta de encontrar el valor adecuado de epsilon ( \\(\\epsilon\\) )) y \\(min_samples\\) ) . A modo orientativo se pueden seguir las siguientes premisas: \\(min_samples\\) : cuanto mayor sea el tama\u00f1o del set de datos, mayor debe ser el valor m\u00ednimo de observaciones vecinas. En el libro Practical Guide to Cluster Analysis in R recomiendan no bajar nunca de 3. Si los datos contienen niveles altos de ruido, aumentar \\(min_samples\\) favorecer\u00e1 la creaci\u00f3n de clusters significativos menos influenciados por outliers. epsilon ( \\(\\epsilon\\) ): una buena forma de escoger el valor de \\(\\epsilon\\) es estudiar las distancias promedio entre las \\(k = minsamples\ud835\udc60\\) observaciones m\u00e1s pr\u00f3ximas. Al representar estas distancias en funci\u00f3n de \\(\\epsilon\\) , el punto de inflexi\u00f3n de la curva suele ser un valor \u00f3ptimo. Si el valor de \\(\\epsilon\\) escogido es muy peque\u00f1o, una proporci\u00f3n alta de las observaciones no se asignar\u00e1n a ning\u00fan cluster, por el contrario, si el valor es demasiado grande, la mayor\u00eda de observaciones se agrupar\u00e1n en un \u00fanico cluster.","title":"Hiperpar\u00e1metros"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#ventajas-y-desventajas_1","text":"Ventajas No requiere que el usuario especifique el n\u00famero de clusters. Es independiente de la forma que tengan los clusters. Puede identificar outliers, por lo que los clusters generados no se ven influenciados por ellos. Desventajas Es un m\u00e9todo determin\u00edstico siempre y cuando el orden de los datos sea el mismo. Los border points que son alcanzables desde m\u00e1s de un cluster pueden asignarse a uno u otro dependiendo del orden en el que se procesen los datos. No genera buenos resultados cuando la densidad de los grupos es muy distinta, ya que no es posible encontrar los par\u00e1metros \ud835\udf16 y min_samples que sirvan para todos a la vez.","title":"Ventajas y desventajas"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#aplicacion_2","text":"Veamos un ejemplo de an\u00e1lisis no supervisado ocupando el algoritmo DBSCAN . # Tratamiento de datos # ============================================================================== import numpy as np import pandas as pd from sklearn.datasets import make_blobs # Gr\u00e1ficos # ============================================================================== import matplotlib.pyplot as plt from matplotlib import style style . use ( 'ggplot' ) or plt . style . use ( 'ggplot' ) # Preprocesado y modelado # ============================================================================== from sklearn.cluster import DBSCAN from sklearn.preprocessing import scale from sklearn.metrics import silhouette_score # Configuraci\u00f3n warnings # ============================================================================== import warnings warnings . filterwarnings ( 'ignore' ) df = pd . read_csv ( \"data/multishape.csv\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y shape 0 -0.803739 -0.853053 1 1 0.852851 0.367618 1 2 0.927180 -0.274902 1 3 -0.752626 -0.511565 1 4 0.706846 0.810679 1 # graficar sns . set ( rc = { 'figure.figsize' :( 11.7 , 8.27 )}) ax = sns . scatterplot ( data = df , x = \"x\" , y = \"y\" ) # Escalado de datos scaler = StandardScaler () columns = [ 'x' , 'y' ] df [ columns ] = scaler . fit_transform ( df [ columns ]) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y shape 0 -1.120749 -0.193616 1 1 1.448907 0.844692 1 2 1.564203 0.298161 1 3 -1.041463 0.096855 1 4 1.222429 1.221561 1 # Modelo X = np . array ( df [[ 'x' , 'y' ]]) modelo_dbscan = DBSCAN ( eps = 0.2 , min_samples = 5 , metric = 'euclidean' , ) modelo_dbscan . fit ( X ) DBSCAN(eps=0.2) # agregar labels df [ 'labels' ] = modelo_dbscan . labels_ # graficar sns . set ( rc = { 'figure.figsize' :( 11.7 , 8.27 )}) ax = sns . scatterplot ( data = df , x = \"x\" , y = \"y\" , hue = \"labels\" ) # N\u00famero de clusters y observaciones \"outliers\" n_clusters = len ( set ( df [ 'labels' ])) - ( 1 if - 1 in df [ 'labels' ] else 0 ) n_noise = list ( df [ 'labels' ]) . count ( - 1 ) print ( f 'N\u00famero de clusters encontrados: { n_clusters } ' ) print ( f 'N\u00famero de outliers encontrados: { n_noise } ' ) N\u00famero de clusters encontrados: 6 N\u00famero de outliers encontrados: 25","title":"Aplicaci\u00f3n"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#gaussian-mixture-models-gmms","text":"","title":"Gaussian mixture models (GMMs)"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#teoria_3","text":"Un Gaussian Mixture model es un modelo probabil\u00edstico en el que se considera que las observaciones siguen una distribuci\u00f3n probabil\u00edstica formada por la combinaci\u00f3n de m\u00faltiples distribuciones normales (componentes). En su aplicaci\u00f3n al clustering, puede entenderse como una generalizaci\u00f3n de K-means con la que, en lugar de asignar cada observaci\u00f3n a un \u00fanico cluster, se obtiene una probabilidad de pertenencia a cada uno. Para estimar los par\u00e1metros que definen la funci\u00f3n de distribuci\u00f3n de cada cluster (media y matriz de covarianza) se recurre al algoritmo de Expectation-Maximization (EM). Una vez aprendidos los par\u00e1metros, se puede calcular la probabilidad que tiene cada observaci\u00f3n de pertenecer a cada cluster y asignarla a aquel con mayor probabilidad.","title":"Teor\u00eda"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#algoritmo_3","text":"Junto con el n\u00famero de clusters (componentes), hay que determinar el tipo de matriz de covarianza que pueden tener los clusters. Dependiendo del tipo de matriz, la forma de los clusters puede ser: tied: todos los clusters comparten la misma matriz de covarianza. diagonal: las dimensiones de cada cluster a lo largo de cada dimensi\u00f3n puede ser distinto, pero las elipses generadas siempre quedan alineadas con los ejes, es decir, su orientaciones son limitadas. spherical: las dimensiones de cada cluster son las mismas en todas las dimensiones. Esto permite generar clusters de distinto tama\u00f1o pero todos esf\u00e9ricos. full: cada cluster puede puede ser modelado como una elipse cualquier orientaci\u00f3n y dimensiones.","title":"Algoritmo"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#aplicacion_3","text":"# Tratamiento de datos # ============================================================================== import numpy as np import pandas as pd from sklearn.datasets import make_blobs # Gr\u00e1ficos # ============================================================================== import matplotlib.pyplot as plt import matplotlib as mpl from matplotlib.patches import Ellipse from matplotlib import style style . use ( 'ggplot' ) or plt . style . use ( 'ggplot' ) # Preprocesado y modelado # ============================================================================== from sklearn.mixture import GaussianMixture from sklearn.preprocessing import scale from sklearn.metrics import silhouette_score # Configuraci\u00f3n warnings # ============================================================================== import warnings warnings . filterwarnings ( 'ignore' ) # generar datos X , y = make_blobs ( n_samples = 300 , n_features = 2 , centers = 4 , cluster_std = 0.60 , shuffle = True , random_state = 0 ) df = pd . DataFrame ({ 'x' : X [:, 0 ], 'y' : X [:, 1 ] }) # Escalado de datos scaler = StandardScaler () columns = [ 'x' , 'y' ] df [ columns ] = scaler . fit_transform ( df [ columns ]) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y 0 0.516255 -0.707227 1 -0.861664 1.329068 2 0.711174 0.437049 3 -0.619792 1.485573 4 0.782282 -0.801378 # graficar sns . set ( rc = { 'figure.figsize' :( 11.7 , 8.27 )}) ax = sns . scatterplot ( data = df , x = \"x\" , y = \"y\" ) # Modelo X = np . array ( df [[ 'x' , 'y' ]]) modelo_gmm = GaussianMixture ( n_components = 4 , covariance_type = 'full' , random_state = 123 ) modelo_gmm . fit ( X = X ) GaussianMixture(n_components=4, random_state=123) # Media de cada componente modelo_gmm . means_ array([[ 0.57844185, 0.17292982], [ 1.2180002 , -1.19725866], [-0.96910551, -0.44143927], [-0.83710796, 1.46219241]]) # Matriz de covarianza de cada componente modelo_gmm . covariances_ array([[[ 0.14277634, -0.00527707], [-0.00527707, 0.05201453]], [[ 0.12745218, -0.00619666], [-0.00619666, 0.05157763]], [[ 0.12131004, 0.00243031], [ 0.00243031, 0.04602115]], [[ 0.15451151, 0.0068188 ], [ 0.0068188 , 0.05660383]]]) Predicci\u00f3n y clasificaci\u00f3n Una vez entrenado el modelo GMMs, se puede predecir la probabilidad que tiene cada observaci\u00f3n de pertenecer a cada una de las componentes (clusters). Para obtener la clasificaci\u00f3n final, se asigna a la componente con mayor probabilidad # Probabilidades # ============================================================================== # Cada fila es una observaci\u00f3n y cada columna la probabilidad de pertenecer a # cada una de las componentes. probabilidades = modelo_gmm . predict_proba ( X ) probabilidades array([[2.59319058e-02, 9.71686641e-01, 2.38145325e-03, 8.05199375e-21], [7.16006205e-09, 7.13831446e-33, 2.34989165e-15, 9.99999993e-01], [9.99999970e-01, 8.78380305e-12, 9.13663813e-09, 2.04805600e-08], ..., [9.99965889e-01, 4.92493619e-10, 3.41016281e-05, 8.75675460e-09], [3.01319652e-06, 6.45628361e-30, 1.52897049e-18, 9.99996987e-01], [4.39337172e-07, 1.99785604e-11, 9.99999561e-01, 4.05381245e-15]]) # Clasificaci\u00f3n (asignaci\u00f3n a la componente de mayor probabilidad) # ============================================================================== # Cada fila es una observaci\u00f3n y cada columna la probabilidad de pertenecer a # cada una de las componentes. clasificacion = modelo_gmm . predict ( X ) clasificacion array([1, 3, 0, 3, 1, 1, 2, 0, 3, 3, 2, 3, 0, 3, 1, 0, 0, 1, 2, 2, 1, 1, 0, 2, 2, 0, 1, 0, 2, 0, 3, 3, 0, 3, 3, 3, 3, 3, 2, 1, 0, 2, 0, 0, 2, 2, 3, 2, 3, 1, 2, 1, 3, 1, 1, 2, 3, 2, 3, 1, 3, 0, 3, 2, 2, 2, 3, 1, 3, 2, 0, 2, 3, 2, 2, 3, 2, 0, 1, 3, 1, 0, 1, 1, 3, 0, 1, 0, 3, 3, 0, 1, 3, 2, 2, 0, 1, 1, 0, 2, 3, 1, 3, 1, 0, 1, 1, 0, 3, 0, 2, 2, 1, 3, 1, 0, 3, 1, 1, 0, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 3, 2, 2, 1, 3, 2, 2, 3, 0, 3, 3, 2, 0, 2, 0, 2, 3, 0, 3, 3, 3, 0, 3, 0, 1, 2, 3, 2, 1, 0, 3, 0, 0, 1, 0, 2, 2, 0, 1, 0, 0, 3, 1, 0, 2, 3, 1, 1, 0, 2, 1, 0, 2, 2, 0, 0, 0, 0, 1, 3, 0, 2, 0, 0, 2, 2, 2, 0, 2, 3, 0, 2, 1, 2, 0, 3, 2, 3, 0, 3, 0, 2, 0, 0, 3, 2, 2, 1, 1, 0, 3, 1, 1, 2, 1, 2, 0, 3, 3, 0, 0, 3, 0, 1, 2, 0, 1, 2, 3, 2, 1, 0, 1, 3, 3, 3, 3, 2, 2, 3, 0, 2, 1, 0, 2, 2, 2, 1, 1, 3, 0, 0, 2, 1, 3, 2, 0, 3, 0, 1, 1, 2, 2, 0, 1, 1, 1, 0, 3, 3, 1, 1, 0, 1, 1, 1, 3, 2, 3, 0, 1, 1, 3, 3, 3, 1, 1, 0, 3, 2]) # Representaci\u00f3n gr\u00e1fica # ============================================================================== # Codigo obtenido de: # https://github.com/amueller/COMS4995-s20/tree/master/slides/aml-14-clustering-mixture-models def make_ellipses ( gmm , ax ): for n in range ( gmm . n_components ): if gmm . covariance_type == 'full' : covariances = gmm . covariances_ [ n ] elif gmm . covariance_type == 'tied' : covariances = gmm . covariances_ elif gmm . covariance_type == 'diag' : covariances = np . diag ( gmm . covariances_ [ n ]) elif gmm . covariance_type == 'spherical' : covariances = np . eye ( gmm . means_ . shape [ 1 ]) * gmm . covariances_ [ n ] v , w = np . linalg . eigh ( covariances ) u = w [ 0 ] / np . linalg . norm ( w [ 0 ]) angle = np . arctan2 ( u [ 1 ], u [ 0 ]) angle = 180 * angle / np . pi # convert to degrees v = 2. * np . sqrt ( 2. ) * np . sqrt ( v ) for i in range ( 1 , 3 ): ell = mpl . patches . Ellipse ( gmm . means_ [ n ], i * v [ 0 ], i * v [ 1 ], 180 + angle , color = \"blue\" ) ell . set_clip_box ( ax . bbox ) ell . set_alpha ( 0.1 ) ax . add_artist ( ell ) fig , axs = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) # Distribuci\u00f3n de probabilidad de cada componente for i in np . unique ( clasificacion ): axs [ 0 ] . scatter ( x = X [ clasificacion == i , 0 ], y = X [ clasificacion == i , 1 ], c = plt . rcParams [ 'axes.prop_cycle' ] . by_key ()[ 'color' ][ i ], marker = 'o' , edgecolor = 'black' , label = f \"Componente { i } \" ) make_ellipses ( modelo_gmm , ax = axs [ 0 ]) axs [ 0 ] . set_title ( 'Distribuci\u00f3n de prob. de cada componente' ) axs [ 0 ] . legend () # Distribuci\u00f3n de probabilidad del modelo completo xs = np . linspace ( min ( X [:, 0 ]), max ( X [:, 0 ]), 1000 ) ys = np . linspace ( min ( X [:, 1 ]), max ( X [:, 1 ]), 1000 ) xx , yy = np . meshgrid ( xs , ys ) scores = modelo_gmm . score_samples ( np . c_ [ xx . ravel (), yy . ravel ()], ) axs [ 1 ] . scatter ( X [:, 0 ], X [:, 1 ], s = 5 , alpha = .6 , c = plt . cm . tab10 ( clasificacion )) scores = np . exp ( scores ) # Las probabilidades est\u00e1n en log axs [ 1 ] . contour ( xx , yy , scores . reshape ( xx . shape ), levels = np . percentile ( scores , np . linspace ( 0 , 100 , 10 ))[ 1 : - 1 ] ) axs [ 1 ] . set_title ( 'Distribuci\u00f3n de prob. del modelo completo' ); N\u00famero de clusters Dado que los modelos GMM son modelos probabil\u00edsticos, se puede recurrir a m\u00e9tricas como el Akaike information criterion (AIC) o Bayesian information criterion (BIC) para identificar c\u00f3mo de bien se ajustan los datos observados a modelo creado. n_components = range ( 1 , 21 ) valores_bic = [] valores_aic = [] for i in n_components : modelo = GaussianMixture ( n_components = i , covariance_type = \"full\" ) modelo = modelo . fit ( X ) valores_bic . append ( modelo . bic ( X )) valores_aic . append ( modelo . aic ( X )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 12 , 6 )) ax . plot ( n_components , valores_bic , label = 'BIC' ) ax . plot ( n_components , valores_aic , label = 'AIC' ) ax . set_title ( \"Valores BIC y AIC\" ) ax . set_xlabel ( \"N\u00famero componentes\" ) ax . legend (); print ( f \"N\u00famero \u00f3ptimo acorde al BIC: { range ( 1 , 21 )[ np . argmin ( valores_bic )] } \" ) print ( f \"N\u00famero \u00f3ptimo acorde al AIC: { range ( 1 , 21 )[ np . argmin ( valores_aic )] } \" ) N\u00famero \u00f3ptimo acorde al BIC: 4 N\u00famero \u00f3ptimo acorde al AIC: 4 Ambas m\u00e9tricas identifican el 4 como n\u00famero \u00f3ptimo de clusters (componentes).","title":"Aplicaci\u00f3n"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#limitaciones-del-clustering","text":"El clustering puede ser una herramienta muy \u00fatil para encontrar agrupaciones en los datos, sobre todo a medida que el volumen de los mismos aumenta. Sin embargo, es importante recordar sus limitaciones o problemas que pueden surgir al aplicarlo. Algunas de ellas son: Peque\u00f1as decisiones pueden tener grandes consecuencias:a la hora de utilizar los m\u00e9todos de clustering se tienen que tomar decisiones que influyen en gran medida en los resultados obtenidos. No existe una \u00fanica respuesta correcta, por lo que en la pr\u00e1ctica se prueban diferentes opciones. Escalado y centrado de las variables Qu\u00e9 medida de distancia/similitud emplear N\u00famero de clusters Tipo de linkage empleado en hierarchical clustering A que altura establecer el corte de un dendrograma Validaci\u00f3n de los clusters obtenidos: no es f\u00e1cil comprobar la validez de los resultados ya que en la mayor\u00eda de escenarios se desconoce la verdadera agrupaci\u00f3n. Falta de robustez: los m\u00e9todos de K-means-clustering e hierarchical clustering asignan obligatoriamente cada observaci\u00f3n a un grupo. Si existe en la muestra alg\u00fan outlier, a pesar de que realmente no pertenezca a ning\u00fan grupo, el algoritmo lo asignar\u00e1 a uno de ellos provocando una distorsi\u00f3n significativa del cluster en cuesti\u00f3n. Algunas alternativas son k-medoids y DBSCAN. La naturaleza del algoritmo de hierarchical clustering conlleva que, si se realiza una mala divisi\u00f3n en los pasos iniciales, no se pueda corregir en los pasos siguientes.","title":"Limitaciones del clustering"},{"location":"lectures/ml/analisis_no_supervisado/03_clustering/#referencias","text":"Unsupervised learning Clustering con Python (Joaqu\u00edn Amat Rodrigo)","title":"Referencias"},{"location":"lectures/ml/analisis_no_supervisado/03_reduccion_dimensionalidad/","text":"Reducci\u00f3n de dimensionalidad Introducci\u00f3n En aprendizaje autom\u00e1tico y estad\u00edsticas reducci\u00f3n de dimensionalidad es el proceso de reducci\u00f3n del n\u00famero de variables aleatorias que se trate, y se puede dividir en selecci\u00f3n de funci\u00f3n y extracci\u00f3n de funci\u00f3n Sin embargo, se puede utilizar como un paso de preprocesamiento de transformaci\u00f3n de datos para algoritmos de aprendizaje autom\u00e1tico en conjuntos de datos de modelado predictivo de clasificaci\u00f3n y regresi\u00f3n con algoritmos de aprendizaje supervisado. Hay muchos algoritmos de reducci\u00f3n de dimensionalidad entre los que elegir y no existe el mejor algoritmo para todos los casos. En cambio, es una buena idea explorar una variedad de algoritmos de reducci\u00f3n de dimensionalidad y diferentes configuraciones para cada algoritmo. Algoritmos de reducci\u00f3n de la dimensionalidad Hay muchos algoritmos que pueden ser usados para la reducci\u00f3n de la dimensionalidad. Dos clases principales de m\u00e9todos son los que se extraen del \u00e1lgebra lineal y los que se extraen del aprendizaje m\u00faltiple. M\u00e9todos de \u00e1lgebra lineal Los m\u00e9todos de factorizaci\u00f3n matricial extra\u00eddos del campo del \u00e1lgebra lineal pueden utilizarse para la dimensionalidad. Algunos de los m\u00e9todos m\u00e1s populares incluyen: An\u00e1lisis de los componentes principales Descomposici\u00f3n del valor singular Factorizaci\u00f3n de matriz no negativa M\u00faltiples m\u00e9todos de aprendizaje Los m\u00faltiples m\u00e9todos de aprendizaje buscan una proyecci\u00f3n de dimensiones inferiores de alta entrada dimensional que capte las propiedades salientes de los datos de entrada. Algunos de los m\u00e9todos m\u00e1s populares incluyen: Isomap Embedding Locally Linear Embedding Multidimensional Scaling Spectral Embedding t-distributed Stochastic Neighbor Embedding (t-sne) Cada algoritmo ofrece un enfoque diferente para el desaf\u00edo de descubrir las relaciones naturales en los datos de dimensiones inferiores. No hay un mejor algoritmo de reducci\u00f3n de la dimensionalidad, y no hay una manera f\u00e1cil de encontrar el mejor algoritmo para sus datos sin usar experimentos controlados. Debido a la importancia que se tiene en el mundo del machine lerning, se dar\u00e1 un explicaci\u00f3n formal del m\u00e9todo de PCA y luego se dar\u00e1 una breve rese\u00f1a de los dem\u00e1s m\u00e9todos. PCA El an\u00e1lisis de componentes principales (Principal Component Analysis PCA) es un m\u00e9todo de reducci\u00f3n de dimensionalidad que permite simplificar la complejidad de espacios con m\u00faltiples dimensiones a la vez que conserva su informaci\u00f3n. Sup\u00f3ngase que existe una muestra con \\(n\\) individuos cada uno con \\(p\\) variables ( \\(X_1\\) ,..., \\(X_p\\) ), es decir, el espacio muestral tiene \\(p\\) dimensiones. PCA permite encontrar un n\u00famero de factores subyacentes ( \\(z<p\\) ) que explican aproximadamente lo mismo que las \\(p\\) variables originales. Donde antes se necesitaban \\(p\\) valores para caracterizar a cada individuo, ahora bastan \\(z\\) valores. Cada una de estas \\(z\\) nuevas variables recibe el nombre de componente principal. El m\u00e9todo de PCA permite por lo tanto \"condensar\" la informaci\u00f3n aportada por m\u00faltiples variables en solo unas pocas componentes. Aun as\u00ed, no hay que olvidar que sigue siendo necesario disponer del valor de las variables originales para calcular las componentes. Dos de las principales aplicaciones del PCA son la visualizaci\u00f3n y el preprocesado de predictores previo ajuste de modelos supervisados. Interpretaci\u00f3n geom\u00e9trica de las componentes principales Una forma intuitiva de entender el proceso de PCA es interpretar las componentes principales desde un punto de vista geom\u00e9trico. Sup\u00f3ngase un conjunto de observaciones para las que se dispone de dos variables ( \\(X_1\\) , \\(X_2\\) ). El vector que define la primera componente principal ( \\(Z_1\\) ) sigue la direcci\u00f3n en la que las observaciones tienen m\u00e1s varianza (l\u00ednea roja). La proyecci\u00f3n de cada observaci\u00f3n sobre esa direcci\u00f3n equivale al valor de la primera componente para dicha observaci\u00f3n (principal component score, \\(z_{i1}\\) ). La segunda componente ( \\(Z_2\\) ) sigue la segunda direcci\u00f3n en la que los datos muestran mayor varianza y que no est\u00e1 correlacionada con la primera componente. La condici\u00f3n de no correlaci\u00f3n entre componentes principales equivale a decir que sus direcciones son perpendiculares/ortogonales. C\u00e1lculo de las componentes principales Cada componente principal ( \\(Z_i\\) ) se obtiene por combinaci\u00f3n lineal de las variables originales. Se pueden entender como nuevas variables obtenidas al combinar de una determinada forma las variables originales. La primera componente principal de un grupo de variables ( \\(X_1,...,X_p\\) ) es la combinaci\u00f3n lineal normalizada de dichas variables que tiene mayor varianza: \\[ Z_1 = \\phi_{11}X_1 + ... + \\phi_{p1}X_p\\] Que la combinaci\u00f3n lineal sea normalizada implica que: \\[\\sum_{j=1}^p \\phi^2_{j1} = 1\\] Los t\u00e9rminos \\(\\phi_{11},...,\\phi_{p1}\\) reciben en el nombre de loadings y son los que definen las componentes. Por ejemplo, \\(\\phi_{11}\\) es el loading de la variable \\(X_1\\) de la primera componente principal. Los loadings pueden interpretarse como el peso/importancia que tiene cada variable en cada componente y, por lo tanto, ayudan a conocer que tipo de informaci\u00f3n recoge cada una de las componentes. Dado un set de datos \\(X\\) con \\(n\\) observaciones y \\(p\\) variables, el proceso a seguir para calcular la primera componente principal es: Centrar las variables: se resta a cada valor la media de la variable a la que pertenece. Con esto se consigue que todas las variables tengan media cero. Se resuelve un problema de optimizaci\u00f3n para encontrar el valor de los loadings con los que se maximiza la varianza. Una forma de resolver esta optimizaci\u00f3n es mediante el c\u00e1lculo de eigenvector-eigenvalue de la matriz de covarianzas. Una vez calculada la primera componente ( \\(Z_1\\) ), se calcula la segunda ( \\(Z_2\\) ) repitiendo el mismo proceso pero a\u00f1adiendo la condici\u00f3n de que la combinaci\u00f3n lineal no pude estar correlacionada con la primera componente. Esto equivale a decir que \\(Z_1\\) y \\(Z_2\\) tienen que ser perpendiculares. EL proceso se repite de forma iterativa hasta calcular todas las posibles componentes ( min( \\(n-1, p\\) ) ) o hasta que se decida detener el proceso. El orden de importancia de las componentes viene dado por la magnitud del eigenvalue asociado a cada eigenvector. Caracter\u00edsticas del PCA Escalado de las variables : El proceso de PCA identifica las direcciones con mayor varianza. Reproducibilidad de las componentes : El proceso de PCA est\u00e1ndar es determinista, genera siempre las mismas componentes principales, es decir, el valor de los loadings resultantes es el mismo. Influencia de outliers : Al trabajar con varianzas, el m\u00e9todo PCA es muy sensible a outliers, por lo que es recomendable estudiar si los hay. La detecci\u00f3n de valores at\u00edpicos con respecto a una determinada dimensi\u00f3n es algo relativamente sencillo de hacer mediante comprobaciones gr\u00e1ficas. Proporci\u00f3n de varianza explicada Una de las preguntas m\u00e1s frecuentes que surge tras realizar un PCA es: \u00bfCu\u00e1nta informaci\u00f3n presente en el set de datos original se pierde al proyectar las observaciones en un espacio de menor dimensi\u00f3n? o lo que es lo mismo \u00bfCuanta informaci\u00f3n es capaz de capturar cada una de las componentes principales obtenidas? Para contestar a estas preguntas se recurre a la proporci\u00f3n de varianza explicada por cada componente principal. Asumiendo que las variables se han normalizado para tener media cero, la varianza total presente en el set de datos se define como \\[\\sum_{j=1}^p Var(X_j) = \\dfrac{1}{n}\\sum_{j=1}^p\\sum_{i=1}^nx_{ij}^2\\] y la varianza explicada por la componente m es \\[\\dfrac{1}{n}\\sum_{i=1}^n z_{im}^2 = \\dfrac{1}{n}\\sum_{i=1}^n (\\sum_{j=1}^p \\phi_{jm}x_{ij})^2\\] Por lo tanto, la proporci\u00f3n de varianza explicada por la componente m viene dada por el ratio \\[ \\dfrac{\\sum_{i=1}^n (\\sum_{j=1}^p \\phi_{jm}x_{ij})^2}{\\sum_{j=1}^p\\sum_{i=1}^nx_{ij}^2}\\] Tanto la proporci\u00f3n de varianza explicada, como la proporci\u00f3n de varianza explicada acumulada, son dos valores de gran utilidad a la hora de decidir el n\u00famero de componentes principales a utilizar en los an\u00e1lisis posteriores. Si se calculan todas las componentes principales de un set de datos, entonces, aunque transformada, se est\u00e1 almacenando toda la informaci\u00f3n presente en los datos originales. El sumatorio de la proporci\u00f3n de varianza explicada acumulada de todas las componentes es siempre 1. N\u00famero \u00f3ptimo de componentes principales Por lo general, dada una matriz de datos de dimensiones \\(n \\times p\\) , el n\u00famero de componentes principales que se pueden calcular es como m\u00e1ximo de \\(n-1\\) o \\(p\\) (el menor de los dos valores es el limitante). Sin embargo, siendo el objetivo del PCA reducir la dimensionalidad, suelen ser de inter\u00e9s utilizar el n\u00famero m\u00ednimo de componentes que resultan suficientes para explicar los datos. No existe una respuesta o m\u00e9todo \u00fanico que permita identificar cual es el n\u00famero \u00f3ptimo de componentes principales a utilizar. Una forma de proceder muy extendida consiste en evaluar la proporci\u00f3n de varianza explicada acumulada y seleccionar el n\u00famero de componentes m\u00ednimo a partir del cual el incremento deja de ser sustancial. Aplicaci\u00f3n El m\u00e9todo Principal Components Regression PCR consiste en ajustar un modelo de regresi\u00f3n lineal por m\u00ednimos cuadrados empleando como predictores las componentes generadas a partir de un Principal Component Analysis (PCA). De esta forma, con un n\u00famero reducido de componentes se puede explicar la mayor parte de la varianza de los datos. En los estudios observacionales, es frecuente disponer de un n\u00famero elevado de variables que se pueden emplear como predictores, sin embargo, esto no implica necesariamente que se disponga de mucha informaci\u00f3n. Si las variables est\u00e1n correlacionadas entre ellas, la informaci\u00f3n que aportan es redundante y adem\u00e1s, se incumple la condici\u00f3n de no colinealidad necesaria en la regresi\u00f3n por m\u00ednimos cuadrados. Dado que el PCA es \u00fatil eliminando informaci\u00f3n redundante, si se emplean como predictores las componentes principales, se puede mejorar el modelo de regresi\u00f3n. Es importante tener en cuenta que, si bien el Principal Components Regression reduce el n\u00famero de predictores del modelo, no se puede considerar como un m\u00e9todo de selecci\u00f3n de variables ya que todas ellas se necesitan para el c\u00e1lculo de las componentes. La identificaci\u00f3n del n\u00famero \u00f3ptimo de componentes principales que se emplean como predictores en PCR puede identificarse por validaci\u00f3n cruzada. Datos : El set de datos USArrests contiene el porcentaje de asaltos (Assault), asesinatos (Murder) y secuestros (Rape) por cada 100,000 habitantes para cada uno de los 50 estados de USA (1973). Adem\u00e1s, tambi\u00e9n incluye el porcentaje de la poblaci\u00f3n de cada estado que vive en zonas rurales (UrbanPoP). # librerias import os import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns pd . set_option ( 'display.max_columns' , 500 ) # Ver m\u00e1s columnas de los dataframes # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab % matplotlib inline from sklearn.model_selection import train_test_split from sklearn.decomposition import PCA from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import scale # Datos datos = pd . read_csv ( 'data/USArrests.csv' ) datos = datos . rename ( columns = { datos . columns [ 0 ]: 'index' }) . set_index ( 'index' ) datos . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Murder Assault UrbanPop Rape index Alabama 13.2 236 58 21.2 Alaska 10.0 263 48 44.5 Arizona 8.1 294 80 31.0 Arkansas 8.8 190 50 19.5 California 9.0 276 91 40.6 Veamos una exploraci\u00f3n inicial de los datos: print ( '----------------------' ) print ( 'Media de cada variable' ) print ( '----------------------' ) datos . mean ( axis = 0 ) ---------------------- Media de cada variable ---------------------- Murder 7.788 Assault 170.760 UrbanPop 65.540 Rape 21.232 dtype: float64 La media de las variables muestra que hay tres veces m\u00e1s secuestros que asesinatos y 8 veces m\u00e1s asaltos que secuestros. print ( '-------------------------' ) print ( 'Varianza de cada variable' ) print ( '-------------------------' ) datos . var ( axis = 0 ) ------------------------- Varianza de cada variable ------------------------- Murder 18.970465 Assault 6945.165714 UrbanPop 209.518776 Rape 87.729159 dtype: float64 La varianza es muy distinta entre las variables, en el caso de Assault, la varianza es varios \u00f3rdenes de magnitud superior al resto. Si no se estandarizan las variables para que tengan media cero y desviaci\u00f3n est\u00e1ndar de uno antes de realizar el estudio PCA, la variable Assault, que tiene una media y dispersi\u00f3n muy superior al resto, dominar\u00e1 la mayor\u00eda de las componentes principales. Modelo PCA La clase sklearn.decomposition.PCA incorpora las principales funcionalidades que se necesitan a la hora de trabajar con modelos PCA. El argumento n_components determina el n\u00famero de componentes calculados. Si se indica None, se calculan todas las posibles (min(filas, columnas) - 1). Por defecto, PCA() centra los valores pero no los escala. Esto es importante ya que, si las variables tienen distinta dispersi\u00f3n, como en este caso, es necesario escalarlas. Una forma de hacerlo es combinar un StandardScaler() y un PCA() dentro de un pipeline . # Entrenamiento modelo PCA con escalado de los datos # ============================================================================== pca_pipe = make_pipeline ( StandardScaler (), PCA ()) pca_pipe . fit ( datos ) # Se extrae el modelo entrenado del pipeline modelo_pca = pca_pipe . named_steps [ 'pca' ] Una vez entrenado el objeto PCA , pude accederse a toda la informaci\u00f3n de las componentes creadas. components_ contiene el valor de los loadings \ud835\udf19 que definen cada componente (eigenvector). Las filas se corresponden con las componentes principals (ordenadas de mayor a menor varianza explicada). Las filas se corresponden con las variables de entrada. # Se combierte el array a dataframe para a\u00f1adir nombres a los ejes. pd . DataFrame ( data = modelo_pca . components_ , columns = datos . columns , index = [ 'PC1' , 'PC2' , 'PC3' , 'PC4' ] ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Murder Assault UrbanPop Rape PC1 0.535899 0.583184 0.278191 0.543432 PC2 0.418181 0.187986 -0.872806 -0.167319 PC3 -0.341233 -0.268148 -0.378016 0.817778 PC4 0.649228 -0.743407 0.133878 0.089024 Analizar con detalle el vector de loadings que forma cada componente puede ayudar a interpretar qu\u00e9 tipo de informaci\u00f3n recoge cada una de ellas. Por ejemplo, la primera componente es el resultado de la siguiente combinaci\u00f3n lineal de las variables originales: \\[PC1=0.535899 Murder+0.583184 Assault+0.278191 UrbanPop+0.543432 Rape\\] Los pesos asignados en la primera componente a las variables Assault, Murder y Rape son aproximadamente iguales entre ellos y superiores al asignado a UrbanPoP. Esto significa que la primera componente recoge mayoritariamente la informaci\u00f3n correspondiente a los delitos. En la segunda componente, es la variable UrbanPoP es la que tiene con diferencia mayor peso, por lo que se corresponde principalmente con el nivel de urbanizaci\u00f3n del estado. Si bien en este ejemplo la interpretaci\u00f3n de las componentes es bastante clara, no en todos los casos ocurre lo mismo, sobre todo a medida que aumenta el n\u00famero de variables. La influencia de las variables en cada componente analizarse visualmente con un gr\u00e1fico de tipo heatmap. # Heatmap componentes # ============================================================================== plt . figure ( figsize = ( 12 , 4 )) componentes = modelo_pca . components_ plt . imshow ( componentes . T , cmap = 'viridis' , aspect = 'auto' ) plt . yticks ( range ( len ( datos . columns )), datos . columns ) plt . xticks ( range ( len ( datos . columns )), np . arange ( modelo_pca . n_components_ ) + 1 ) plt . grid ( False ) plt . colorbar (); Una vez calculadas las componentes principales, se puede conocer la varianza explicada por cada una de ellas, la proporci\u00f3n respecto al total y la proporci\u00f3n de varianza acumulada. Esta informaci\u00f3n est\u00e1 almacenada en los atributos explained_variance_ y explained_variance_ratio_ del modelo. # graficar varianza por componente percent_variance = np . round ( modelo_pca . explained_variance_ratio_ * 100 , decimals = 2 ) columns = [ 'PC1' , 'PC2' , 'PC3' , 'PC4' ] plt . figure ( figsize = ( 12 , 4 )) plt . bar ( x = range ( 1 , 5 ), height = percent_variance , tick_label = columns ) plt . xticks ( np . arange ( modelo_pca . n_components_ ) + 1 ) plt . ylabel ( 'Componente principal' ) plt . xlabel ( 'Por. varianza explicada' ) plt . title ( 'Porcentaje de varianza explicada por cada componente' ) plt . show () Ahora realizamos el gr\u00e1fico pero respecto a la suma acumulada. # graficar varianza por la suma acumulada de los componente percent_variance_cum = np . cumsum ( percent_variance ) columns = [ 'PC1' , 'PC1+PC2' , 'PC1+PC2+PC3' , 'PC1+PC2+PC3+PC4' ] plt . figure ( figsize = ( 12 , 4 )) plt . bar ( x = range ( 1 , 5 ), height = percent_variance_cum , tick_label = columns ) plt . ylabel ( 'Percentate of Variance Explained' ) plt . xlabel ( 'Principal Component Cumsum' ) plt . title ( 'PCA Scree Plot' ) plt . show () Si se empleasen \u00fanicamente las dos primeras componentes se conseguir\u00eda explicar el 87% de la varianza observada. Transformaci\u00f3n Una vez entrenado el modelo, con el m\u00e9todo transform() se puede reducir la dimensionalidad de nuevas observaciones proyect\u00e1ndolas en el espacio definido por las componentes. # Proyecci\u00f3n de las observaciones de entrenamiento # ============================================================================== proyecciones = pca_pipe . transform ( X = datos ) proyecciones = pd . DataFrame ( proyecciones , columns = [ 'PC1' , 'PC2' , 'PC3' , 'PC4' ], index = datos . index ) proyecciones . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PC1 PC2 PC3 PC4 index Alabama 0.985566 1.133392 -0.444269 0.156267 Alaska 1.950138 1.073213 2.040003 -0.438583 Arizona 1.763164 -0.745957 0.054781 -0.834653 Arkansas -0.141420 1.119797 0.114574 -0.182811 California 2.523980 -1.542934 0.598557 -0.341996 La transformaci\u00f3n es el resultado de multiplicar los vectores que definen cada componente con el valor de las variables. Puede calcularse de forma manual: proyecciones = np . dot ( modelo_pca . components_ , scale ( datos ) . T ) proyecciones = pd . DataFrame ( proyecciones , index = [ 'PC1' , 'PC2' , 'PC3' , 'PC4' ]) proyecciones = proyecciones . transpose () . set_index ( datos . index ) proyecciones . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PC1 PC2 PC3 PC4 index Alabama 0.985566 1.133392 -0.444269 0.156267 Alaska 1.950138 1.073213 2.040003 -0.438583 Arizona 1.763164 -0.745957 0.054781 -0.834653 Arkansas -0.141420 1.119797 0.114574 -0.182811 California 2.523980 -1.542934 0.598557 -0.341996 Reconstrucci\u00f3n Puede revertirse la transformaci\u00f3n y reconstruir el valor inicial con el m\u00e9todo inverse_transform(). Es importante tener en cuenta que, la reconstrucci\u00f3n, solo ser\u00e1 completa si se han incluido todas las componentes. # Recostruccion de las proyecciones # ============================================================================== recostruccion = pca_pipe . inverse_transform ( X = proyecciones ) recostruccion = pd . DataFrame ( recostruccion , columns = datos . columns , index = datos . index ) print ( '------------------' ) print ( 'Valores originales' ) print ( '------------------' ) display ( recostruccion . head ()) print ( '---------------------' ) print ( 'Valores reconstruidos' ) print ( '---------------------' ) display ( datos . head ()) ------------------ Valores originales ------------------ .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Murder Assault UrbanPop Rape index Alabama 13.2 236.0 58.0 21.2 Alaska 10.0 263.0 48.0 44.5 Arizona 8.1 294.0 80.0 31.0 Arkansas 8.8 190.0 50.0 19.5 California 9.0 276.0 91.0 40.6 --------------------- Valores reconstruidos --------------------- .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Murder Assault UrbanPop Rape index Alabama 13.2 236 58 21.2 Alaska 10.0 263 48 44.5 Arizona 8.1 294 80 31.0 Arkansas 8.8 190 50 19.5 California 9.0 276 91 40.6 t-Distributed Stochastic Neighbor Embedding (t-SNE) t-Distributed Stochastic Neighbor Embedding (t-SNE) es una t\u00e9cnica no lineal no supervisada utilizada principalmente para la exploraci\u00f3n de datos y la visualizaci\u00f3n de datos de alta dimensi\u00f3n. En t\u00e9rminos m\u00e1s simples, tSNE le da una sensaci\u00f3n o intuici\u00f3n de c\u00f3mo se organizan los datos en un espacio de alta dimensi\u00f3n. Fue desarrollado por Laurens van der Maatens y Geoffrey Hinton en 2008. Comparando con PCA Si est\u00e1 familiarizado con An\u00e1lisis de componentes principales (PCA), entonces como yo , probablemente se est\u00e9 preguntando la diferencia entre PCA y tSNE. Lo primero a tener en cuenta es que PCA se desarroll\u00f3 en 1933, mientras que tSNE se desarroll\u00f3 en 2008. Mucho ha cambiado en el mundo de la ciencia de datos desde 1933, principalmente en el \u00e1mbito del c\u00e1lculo y el tama\u00f1o de los datos. En segundo lugar, PCA es una t\u00e9cnica de reducci\u00f3n de dimensi\u00f3n lineal que busca maximizar la varianza y preserva las distancias pares grandes. En otras palabras, las cosas que son diferentes terminan muy separadas. Esto puede conducir a una visualizaci\u00f3n deficiente, especialmente cuando se trata de estructuras distribuidoras no lineales. Piense en una estructura m\u00faltiple como cualquier forma geom\u00e9trica como: cilindro, bola, curva, etc. tSNE difiere de PCA al preservar solo peque\u00f1as distancias por pares o similitudes locales, mientras que PCA se preocupa por preservar distancias pares grandes para maximizar la varianza. Laurens ilustra bastante bien el enfoque PCA y tSNE utilizando el conjunto de datos Swiss Roll en la Figura 1 [1]. Puede ver que debido a la no linealidad de este conjunto de datos de juguete (m\u00faltiple) y la preservaci\u00f3n de grandes distancias, PCA conservar\u00eda incorrectamente la estructura de los datos. Figura 1 \u2013 Dataset de rollo suizo. Conservar la distancia peque\u00f1a con tSNE (l\u00ednea continua) frente a la maximizaci\u00f3n de la variaci\u00f3n PCA [1] Explicaci\u00f3n Ahora que sabemos por qu\u00e9 podr\u00edamos usar tSNE sobre PCA, analicemos c\u00f3mo funciona tSNE. El algoritmo tSNE calcula una medida de similitud entre pares de instancias en el espacio de alta dimensi\u00f3n y en el espacio de baja dimensi\u00f3n. Luego trata de optimizar estas dos medidas de similitud usando una funci\u00f3n de costo. Vamos a dividirlo en 3 pasos b\u00e1sicos. Paso 1, mide similitudes entre puntos en el espacio de alta dimensi\u00f3n. Piense en un conjunto de puntos de datos dispersos en un espacio 2D (Figura 2). Para cada punto de datos (xi) centraremos una distribuci\u00f3n Gaussiana sobre ese punto. Luego medimos la densidad de todos los puntos (xj) bajo esa distribuci\u00f3n Gaussiana. Luego renormalize para todos los puntos. Esto nos da un conjunto de probabilidades (Pij) para todos los puntos. Esas probabilidades son proporcionales a las similitudes. Todo lo que eso significa es que si los puntos de datos x1 y x2 tienen valores iguales bajo este c\u00edrculo gaussiano, entonces sus proporciones y similitudes son iguales y, por lo tanto, tienes similitudes locales en la estructura de este espacio de alta dimensi\u00f3n. La distribuci\u00f3n gaussiana o el c\u00edrculo se pueden manipular usando lo que se llama perplejidad, que influye en la varianza de la distribuci\u00f3n (tama\u00f1o del c\u00edrculo) y esencialmente en el n\u00famero de vecinos m\u00e1s cercanos. El rango normal para la perplejidad est\u00e1 entre 5 y 50 [2]. Figura 2 \u2013 Medici\u00f3n de similitudes por pares en el espacio de alta dimensi\u00f3n Figura 2 \u2013 Medici\u00f3n de similitudes por pares en el espacio de alta dimensi\u00f3n 2. El paso 2 es similar al paso 1, pero en lugar de usar una distribuci\u00f3n gaussiana se usa una distribuci\u00f3n t de Student con un grado de libertad, que tambi\u00e9n se conoce como la distribuci\u00f3n de Cauchy (Figura 3). Esto nos da un segundo conjunto de probabilidades ( \\(Q_{ij}\\) ) en el espacio de baja dimensi\u00f3n. Como puede ver, la distribuci\u00f3n t de Student tiene colas m\u00e1s pesadas que la distribuci\u00f3n normal. Las colas pesadas permiten un mejor modelado de distancias muy separadas. Figura 3 \u2013 Distribuci\u00f3n noraml vs t-student El \u00faltimo paso es que queremos que este conjunto de probabilidades del espacio de baja dimensi\u00f3n ( \\(Q_{ij}\\) ) refleje las del espacio de alta dimensi\u00f3n ( \\(P_{ij}\\) ) de la mejor manera posible. Queremos que las dos estructuras de mapa sean similares. Medimos la diferencia entre las distribuciones de probabilidad de los espacios bidimensionales utilizando la divergencia de Kullback-Liebler (KL). No incluir\u00e9 mucho en KL, excepto que es un enfoque asim\u00e9trico que compara de manera eficiente los grandes valores \\(P_{ij}\\) y \\(Q_{ij}\\) . Finalmente, utilizamos el descenso de gradiente para minimizar nuestra funci\u00f3n de costo KL. Aplicaci\u00f3n Laurens van der Maaten menciona el uso de tSNE en \u00e1reas como investigaci\u00f3n del clima, seguridad inform\u00e1tica, bioinform\u00e1tica, investigaci\u00f3n del c\u00e1ncer, etc. tSNE podr\u00eda usarse en datos de alta dimensi\u00f3n y luego el resultado de esas dimensiones se convierte en insumos para alg\u00fan otro modelo de clasificaci\u00f3n . Adem\u00e1s, tSNE podr\u00eda usarse para investigar, aprender o evaluar la segmentaci\u00f3n. Muchas veces seleccionamos la cantidad de segmentos antes del modelado o iteramos despu\u00e9s de los resultados. tSNE a menudo puede mostrar una separaci\u00f3n clara en los datos. Esto se puede usar antes de usar su modelo de segmentaci\u00f3n para seleccionar un n\u00famero de cl\u00faster o despu\u00e9s para evaluar si sus segmentos realmente se mantienen. tSNE, sin embargo, no es un enfoque de agrupamiento, ya que no conserva las entradas como PCA y los valores a menudo pueden cambiar entre ejecuciones, por lo que es pura exploraci\u00f3n. A continuaci\u00f3n se procede a comparar de manera visual los algoritmos de PCA y tSNE en el conjunto de datos Digits . Datos : El conjunto de datos contiene im\u00e1genes de d\u00edgitos escritos a mano: 10 clases donde cada clase se refiere a un d\u00edgito. Los programas de preprocesamiento puestos a disposici\u00f3n por NIST se utilizaron para extraer mapas de bits normalizados de d\u00edgitos escritos a mano de un formulario preimpreso. De un total de 43 personas, 30 contribuyeron al conjunto de entrenamiento y diferentes 13 al conjunto de prueba. Los mapas de bits de 32x32 se dividen en bloques no superpuestos de 4x4 y se cuenta el n\u00famero de p\u00edxeles en cada bloque. Esto genera una matriz de entrada de 8x8 donde cada elemento es un n\u00famero entero en el rango 0..16. Esto reduce la dimensionalidad y da invariancia a peque\u00f1as distorsiones. # Load Python Libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from time import time % matplotlib inline from sklearn.datasets import load_digits from sklearn.manifold import TSNE from sklearn.decomposition import PCA digits = load_digits () df = pd . DataFrame ( digits [ 'data' ]) df [ 'label' ] = digits [ 'target' ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 label 0 0.0 0.0 5.0 13.0 9.0 1.0 0.0 0.0 0.0 0.0 13.0 15.0 10.0 15.0 5.0 0.0 0.0 3.0 15.0 2.0 0.0 11.0 8.0 0.0 0.0 4.0 12.0 0.0 0.0 8.0 8.0 0.0 0.0 5.0 8.0 0.0 0.0 9.0 8.0 0.0 0.0 4.0 11.0 0.0 1.0 12.0 7.0 0.0 0.0 2.0 14.0 5.0 10.0 12.0 0.0 0.0 0.0 0.0 6.0 13.0 10.0 0.0 0.0 0.0 0 1 0.0 0.0 0.0 12.0 13.0 5.0 0.0 0.0 0.0 0.0 0.0 11.0 16.0 9.0 0.0 0.0 0.0 0.0 3.0 15.0 16.0 6.0 0.0 0.0 0.0 7.0 15.0 16.0 16.0 2.0 0.0 0.0 0.0 0.0 1.0 16.0 16.0 3.0 0.0 0.0 0.0 0.0 1.0 16.0 16.0 6.0 0.0 0.0 0.0 0.0 1.0 16.0 16.0 6.0 0.0 0.0 0.0 0.0 0.0 11.0 16.0 10.0 0.0 0.0 1 2 0.0 0.0 0.0 4.0 15.0 12.0 0.0 0.0 0.0 0.0 3.0 16.0 15.0 14.0 0.0 0.0 0.0 0.0 8.0 13.0 8.0 16.0 0.0 0.0 0.0 0.0 1.0 6.0 15.0 11.0 0.0 0.0 0.0 1.0 8.0 13.0 15.0 1.0 0.0 0.0 0.0 9.0 16.0 16.0 5.0 0.0 0.0 0.0 0.0 3.0 13.0 16.0 16.0 11.0 5.0 0.0 0.0 0.0 0.0 3.0 11.0 16.0 9.0 0.0 2 3 0.0 0.0 7.0 15.0 13.0 1.0 0.0 0.0 0.0 8.0 13.0 6.0 15.0 4.0 0.0 0.0 0.0 2.0 1.0 13.0 13.0 0.0 0.0 0.0 0.0 0.0 2.0 15.0 11.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 12.0 12.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 10.0 8.0 0.0 0.0 0.0 8.0 4.0 5.0 14.0 9.0 0.0 0.0 0.0 7.0 13.0 13.0 9.0 0.0 0.0 3 4 0.0 0.0 0.0 1.0 11.0 0.0 0.0 0.0 0.0 0.0 0.0 7.0 8.0 0.0 0.0 0.0 0.0 0.0 1.0 13.0 6.0 2.0 2.0 0.0 0.0 0.0 7.0 15.0 0.0 9.0 8.0 0.0 0.0 5.0 16.0 10.0 0.0 16.0 6.0 0.0 0.0 4.0 15.0 16.0 13.0 16.0 1.0 0.0 0.0 0.0 0.0 3.0 15.0 10.0 0.0 0.0 0.0 0.0 0.0 2.0 16.0 4.0 0.0 0.0 4 # PCA scaler = StandardScaler () X = df . drop ( columns = 'label' ) y = df [ 'label' ] embedding = PCA ( n_components = 2 ) X_transform = embedding . fit_transform ( X ) df_pca = pd . DataFrame ( X_transform , columns = [ 'Score1' , 'Score2' ]) df_pca [ 'label' ] = y # Plot Digits PCA # Set style of scatterplot sns . set_context ( \"notebook\" , font_scale = 1.1 ) sns . set_style ( \"ticks\" ) # Create scatterplot of dataframe sns . lmplot ( x = 'Score1' , y = 'Score2' , data = df_pca , fit_reg = False , legend = True , height = 9 , hue = 'label' , scatter_kws = { \"s\" : 200 , \"alpha\" : 0.3 }) plt . title ( 'PCA Results: Digits' , weight = 'bold' ) . set_fontsize ( '14' ) plt . xlabel ( 'Prin Comp 1' , weight = 'bold' ) . set_fontsize ( '10' ) plt . ylabel ( 'Prin Comp 2' , weight = 'bold' ) . set_fontsize ( '10' ) Al graficar las dos componentes principales del m\u00e9todo PCA, se observa que no existe una clara distinci\u00f3n entre la distintas clases (solo se ve un gran c\u00famulo de puntos mezclados). # tsne scaler = StandardScaler () X = df . drop ( columns = 'label' ) y = df [ 'label' ] embedding = TSNE ( n_components = 2 ) X_transform = embedding . fit_transform ( X ) df_tsne = pd . DataFrame ( X_transform , columns = [ '_DIM_1_' , '_DIM_2_' ]) df_tsne [ 'label' ] = y # Plot Digits t-SNE sns . set_context ( \"notebook\" , font_scale = 1.1 ) sns . set_style ( \"ticks\" ) sns . lmplot ( x = '_DIM_1_' , y = '_DIM_2_' , data = df_tsne , fit_reg = False , legend = True , height = 9 , hue = 'label' , scatter_kws = { \"s\" : 200 , \"alpha\" : 0.3 }) plt . title ( 't-SNE Results: Digits' , weight = 'bold' ) . set_fontsize ( '14' ) plt . xlabel ( 'Dimension 1' , weight = 'bold' ) . set_fontsize ( '10' ) plt . ylabel ( 'Dimension 2' , weight = 'bold' ) . set_fontsize ( '10' ) Para el caso del m\u00e9todo TSNE, se observa una diferenciaci\u00f3n entre los grupos de estudios (aspecto que fue muy distinto al momento de analizar el m\u00e9todo del PCA). Observaci\u00f3n : Si bien se muestra donde el m\u00e9todo TSNE logra ser superior en aspecto de reducci\u00f3n de dimensionalidad que el m\u00e9todo PCA, no significa que para distintos experimientos se tengan los mismo resultados. Otros m\u00e9todos de reducci\u00f3n de dimensionalidad Existen otro m\u00e9todos de reducci\u00f3n de dimencionalidad, a continuaci\u00f3n se deja una referencia con la descripci\u00f3n de cada uno de estos algoritmos. Descomposici\u00f3n del valor singular Non-Negative Matrix Factorization Isomap Embedding Locally Linear Embedding Multidimensional Scaling Spectral Embedding Aplicaci\u00f3n En este ejemplo se quiere aprovechar las bondades de aplicar la reducci\u00f3n de dimensionalidad para ocupar un modelo de clasificaci\u00f3n (en este caso, el modelo de regresi\u00f3n log\u00edstica). Para ello se ocupar\u00e1 el conjunto de datos meatspec.csv Datos : El departamento de calidad de una empresa de alimentaci\u00f3n se encarga de medir el contenido en grasa de la carne que comercializa. Este estudio se realiza mediante t\u00e9cnicas de anal\u00edtica qu\u00edmica, un proceso relativamente costoso en tiempo y recursos. Una alternativa que permitir\u00eda reducir costes y optimizar tiempo es emplear un espectrofot\u00f3metro (instrumento capaz de detectar la absorbancia que tiene un material a diferentes tipos de luz en funci\u00f3n de sus caracter\u00edsticas) e inferir el contenido en grasa a partir de sus medidas. Antes de dar por v\u00e1lida esta nueva t\u00e9cnica, la empresa necesita comprobar qu\u00e9 margen de error tiene respecto al an\u00e1lisis qu\u00edmico. Para ello, se mide el espectro de absorbancia a 100 longitudes de onda en 215 muestras de carne, cuyo contenido en grasa se obtiene tambi\u00e9n por an\u00e1lisis qu\u00edmico, y se entrena un modelo con el objetivo de predecir el contenido en grasa a partir de los valores dados por el espectrofot\u00f3metro. # Librerias from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.decomposition import PCA , TruncatedSVD , NMF from sklearn.manifold import Isomap , LocallyLinearEmbedding , MDS , SpectralEmbedding , TSNE from metrics_regression import summary_metrics # Datos datos = pd . read_csv ( 'data/meatspec.csv' ) datos = datos . drop ( columns = datos . columns [ 0 ]) datos [ 'fat' ] = datos [ 'fat' ] . astype ( float ) datos . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 V29 V30 V31 V32 V33 V34 V35 V36 V37 V38 V39 V40 V41 V42 V43 V44 V45 V46 V47 V48 V49 V50 V51 V52 V53 V54 V55 V56 V57 V58 V59 V60 V61 V62 V63 V64 V65 V66 V67 V68 V69 V70 V71 V72 V73 V74 V75 V76 V77 V78 V79 V80 V81 V82 V83 V84 V85 V86 V87 V88 V89 V90 V91 V92 V93 V94 V95 V96 V97 V98 V99 V100 fat 0 2.61776 2.61814 2.61859 2.61912 2.61981 2.62071 2.62186 2.62334 2.62511 2.62722 2.62964 2.63245 2.63565 2.63933 2.64353 2.64825 2.65350 2.65937 2.66585 2.67281 2.68008 2.68733 2.69427 2.70073 2.70684 2.71281 2.71914 2.72628 2.73462 2.74416 2.75466 2.76568 2.77679 2.78790 2.79949 2.81225 2.82706 2.84356 2.86106 2.87857 2.89497 2.90924 2.92085 2.93015 2.93846 2.94771 2.96019 2.97831 3.00306 3.03506 3.07428 3.11963 3.16868 3.21771 3.26254 3.29988 3.32847 3.34899 3.36342 3.37379 3.38152 3.38741 3.39164 3.39418 3.39490 3.39366 3.39045 3.38541 3.37869 3.37041 3.36073 3.34979 3.33769 3.32443 3.31013 3.29487 3.27891 3.26232 3.24542 3.22828 3.21080 3.19287 3.17433 3.15503 3.13475 3.11339 3.09116 3.06850 3.04596 3.02393 3.00247 2.98145 2.96072 2.94013 2.91978 2.89966 2.87964 2.85960 2.83940 2.81920 22.5 1 2.83454 2.83871 2.84283 2.84705 2.85138 2.85587 2.86060 2.86566 2.87093 2.87661 2.88264 2.88898 2.89577 2.90308 2.91097 2.91953 2.92873 2.93863 2.94929 2.96072 2.97272 2.98493 2.99690 3.00833 3.01920 3.02990 3.04101 3.05345 3.06777 3.08416 3.10221 3.12106 3.13983 3.15810 3.17623 3.19519 3.21584 3.23747 3.25889 3.27835 3.29384 3.30362 3.30681 3.30393 3.29700 3.28925 3.28409 3.28505 3.29326 3.30923 3.33267 3.36251 3.39661 3.43188 3.46492 3.49295 3.51458 3.53004 3.54067 3.54797 3.55306 3.55675 3.55921 3.56045 3.56034 3.55876 3.55571 3.55132 3.54585 3.53950 3.53235 3.52442 3.51583 3.50668 3.49700 3.48683 3.47626 3.46552 3.45501 3.44481 3.43477 3.42465 3.41419 3.40303 3.39082 3.37731 3.36265 3.34745 3.33245 3.31818 3.30473 3.29186 3.27921 3.26655 3.25369 3.24045 3.22659 3.21181 3.19600 3.17942 40.1 2 2.58284 2.58458 2.58629 2.58808 2.58996 2.59192 2.59401 2.59627 2.59873 2.60131 2.60414 2.60714 2.61029 2.61361 2.61714 2.62089 2.62486 2.62909 2.63361 2.63835 2.64330 2.64838 2.65354 2.65870 2.66375 2.66880 2.67383 2.67892 2.68411 2.68937 2.69470 2.70012 2.70563 2.71141 2.71775 2.72490 2.73344 2.74327 2.75433 2.76642 2.77931 2.79272 2.80649 2.82064 2.83541 2.85121 2.86872 2.88905 2.91289 2.94088 2.97325 3.00946 3.04780 3.08554 3.11947 3.14696 3.16677 3.17938 3.18631 3.18924 3.18950 3.18801 3.18498 3.18039 3.17411 3.16611 3.15641 3.14512 3.13241 3.11843 3.10329 3.08714 3.07014 3.05237 3.03393 3.01504 2.99569 2.97612 2.95642 2.93660 2.91667 2.89655 2.87622 2.85563 2.83474 2.81361 2.79235 2.77113 2.75015 2.72956 2.70934 2.68951 2.67009 2.65112 2.63262 2.61461 2.59718 2.58034 2.56404 2.54816 8.4 3 2.82286 2.82460 2.82630 2.82814 2.83001 2.83192 2.83392 2.83606 2.83842 2.84097 2.84374 2.84664 2.84975 2.85307 2.85661 2.86038 2.86437 2.86860 2.87308 2.87789 2.88301 2.88832 2.89374 2.89917 2.90457 2.90991 2.91521 2.92043 2.92565 2.93082 2.93604 2.94128 2.94658 2.95202 2.95777 2.96419 2.97159 2.98045 2.99090 3.00284 3.01611 3.03048 3.04579 3.06194 3.07889 3.09686 3.11629 3.13775 3.16217 3.19068 3.22376 3.26172 3.30379 3.34793 3.39093 3.42920 3.45998 3.48227 3.49687 3.50558 3.51026 3.51221 3.51215 3.51036 3.50682 3.50140 3.49398 3.48457 3.47333 3.46041 3.44595 3.43005 3.41285 3.39450 3.37511 3.35482 3.33376 3.31204 3.28986 3.26730 3.24442 3.22117 3.19757 3.17357 3.14915 3.12429 3.09908 3.07366 3.04825 3.02308 2.99820 2.97367 2.94951 2.92576 2.90251 2.87988 2.85794 2.83672 2.81617 2.79622 5.9 4 2.78813 2.78989 2.79167 2.79350 2.79538 2.79746 2.79984 2.80254 2.80553 2.80890 2.81272 2.81704 2.82184 2.82710 2.83294 2.83945 2.84664 2.85458 2.86331 2.87280 2.88291 2.89335 2.90374 2.91371 2.92305 2.93187 2.94060 2.94986 2.96035 2.97241 2.98606 3.00097 3.01652 3.03220 3.04793 3.06413 3.08153 3.10078 3.12185 3.14371 3.16510 3.18470 3.20140 3.21477 3.22544 3.23505 3.24586 3.26027 3.28063 3.30889 3.34543 3.39019 3.44198 3.49800 3.55407 3.60534 3.64789 3.68011 3.70272 3.71815 3.72863 3.73574 3.74059 3.74357 3.74453 3.74336 3.73991 3.73418 3.72638 3.71676 3.70553 3.69289 3.67900 3.66396 3.64785 3.63085 3.61305 3.59463 3.57582 3.55695 3.53796 3.51880 3.49936 3.47938 3.45869 3.43711 3.41458 3.39129 3.36772 3.34450 3.32201 3.30025 3.27907 3.25831 3.23784 3.21765 3.19766 3.17770 3.15770 3.13753 25.5 El set de datos contiene 101 columnas. Las 100 primeras, nombradas como \\(V_1,...,V_{100}\\) recogen el valor de absorbancia para cada una de las 100 longitudes de onda analizadas (predictores), y la columna fat el contenido en grasa medido por t\u00e9cnicas qu\u00edmicas (variable respuesta). Muchas de las variables est\u00e1n altamente correlacionadas (correlaci\u00f3n absoluta > 0.8), lo que supone un problema a la hora de emplear modelos de regresi\u00f3n lineal. # Correlaci\u00f3n entre columnas num\u00e9ricas def tidy_corr_matrix ( corr_mat ): ''' Funci\u00f3n para convertir una matriz de correlaci\u00f3n de pandas en formato tidy ''' corr_mat = corr_mat . stack () . reset_index () corr_mat . columns = [ 'variable_1' , 'variable_2' , 'r' ] corr_mat = corr_mat . loc [ corr_mat [ 'variable_1' ] != corr_mat [ 'variable_2' ], :] corr_mat [ 'abs_r' ] = np . abs ( corr_mat [ 'r' ]) corr_mat = corr_mat . sort_values ( 'abs_r' , ascending = False ) return ( corr_mat ) corr_matrix = datos . select_dtypes ( include = [ 'float64' , 'int' ]) \\ . corr ( method = 'pearson' ) display ( tidy_corr_matrix ( corr_matrix ) . head ( 5 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } variable_1 variable_2 r abs_r 1019 V11 V10 0.999996 0.999996 919 V10 V11 0.999996 0.999996 1021 V11 V12 0.999996 0.999996 1121 V12 V11 0.999996 0.999996 917 V10 V9 0.999996 0.999996 Se procede aplicar el modelo de regresi\u00f3n lineal . # Divisi\u00f3n de los datos en train y test X = datos . drop ( columns = 'fat' ) y = datos [ 'fat' ] X_train , X_test , y_train , y_test = train_test_split ( X , y . values , train_size = 0.7 , random_state = 1234 , shuffle = True ) # Creaci\u00f3n y entrenamiento del modelo modelo = LinearRegression ( normalize = True ) modelo . fit ( X = X_train , y = y_train ) LinearRegression(normalize=True) # Predicciones test predicciones = modelo . predict ( X = X_test ) predicciones = predicciones . flatten () # Error de test del modelo df_pred = pd . DataFrame ({ 'y' : y_test , 'yhat' : predicciones }) df_summary = summary_metrics ( df_pred ) df_summary .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse mape maape wmape mmape smape 0 2.0904 14.743 3.8397 0.1616 0.1484 0.1088 0.1362 0.1665 Ahora se ocuparan los modelos de reducci\u00f3n de dimensionalidad para entrenar el modelo de regresi\u00f3n lineal. Para ello se ocuparan los distintos algoritmos mencionados. Lo primero es crear una funci\u00f3n que pueda realizar esta tarea de manera autom\u00e1tica. # funcion de reduccion de dimensionalidad y aplicacion de la regresion lineal def dr_pipeline ( df , model_dr ): # datos X = df . drop ( columns = 'fat' ) y = df [ 'fat' ] # reduccion de la dimensionalidad embedding = model_dr X = embedding . fit_transform ( X ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.7 , random_state = 1234 , shuffle = True ) # Creaci\u00f3n y entrenamiento del modelo modelo = LinearRegression ( normalize = True ) modelo . fit ( X = X_train , y = y_train ) # Predicciones test predicciones = modelo . predict ( X = X_test ) predicciones = predicciones . flatten () # Error de test del modelo df_pred = pd . DataFrame ({ 'y' : y_test , 'yhat' : predicciones }) df_summary = summary_metrics ( df_pred ) return df_summary Enfoque: Algebra lineal modelos_algebra_lineal = [ ( 'PCA' , PCA ( n_components = 5 )), ( 'SVD' , TruncatedSVD ( n_components = 5 )), ( 'NMF' , NMF ( n_components = 5 )) ] names = [ x [ 0 ] for x in modelos_algebra_lineal ] results = [ dr_pipeline ( datos , x [ 1 ]) for x in modelos_algebra_lineal ] /home/fralfaro/.cache/pypoetry/virtualenvs/mat281-2021-kaeOORRv-py3.8/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26). warnings.warn((\"The 'init' value, when 'init=None' and \" /home/fralfaro/.cache/pypoetry/virtualenvs/mat281-2021-kaeOORRv-py3.8/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence. warnings.warn(\"Maximum number of iterations %d reached. Increase it to\" df_algebra_lineal = pd . concat ( results ) . reset_index ( drop = True ) df_algebra_lineal [ 'metodo' ] = names df_algebra_lineal .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse mape maape wmape mmape smape metodo 0 2.8050 12.2999 3.5071 0.2869 0.2169 0.1460 0.2183 0.2140 PCA 1 2.6344 11.3195 3.3645 0.2605 0.2023 0.1372 0.1987 0.1979 SVD 2 3.5327 16.4079 4.0507 0.4956 0.2859 0.1839 0.3594 0.2930 NMF Enfoque: Manifold Learning modelos_manifold = [ ( 'Isomap' , Isomap ( n_components = 5 )), ( 'LocallyLinearEmbedding' , LocallyLinearEmbedding ( n_components = 5 )), ( 'MDS' , MDS ( n_components = 5 )), ( 'SpectralEmbedding' , SpectralEmbedding ( n_components = 5 )), ( 'TSNE' , TSNE ( n_components = 2 )), ] names = [ x [ 0 ] for x in modelos_manifold ] results = [ dr_pipeline ( datos , x [ 1 ]) for x in modelos_manifold ] df_manifold = pd . concat ( results ) . reset_index ( drop = True ) df_manifold [ 'metodo' ] = names df_manifold .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse mape maape wmape mmape smape metodo 0 9.4021 133.7433 11.5647 0.9914 0.5142 0.4895 0.7646 0.5491 Isomap 1 9.1329 130.6913 11.4320 0.8865 0.5235 0.4755 0.6995 0.5447 LocallyLinearEmbedding 2 7.6519 109.5706 10.4676 0.7519 0.4474 0.3984 0.5787 0.4857 MDS 3 8.9529 130.9545 11.4435 0.8330 0.5061 0.4661 0.6646 0.5261 SpectralEmbedding 4 9.0884 132.0296 11.4904 0.8893 0.5175 0.4732 0.6946 0.5405 TSNE En este caso en particular, funciona de mejor forma aplicar los m\u00e9todos de descomposici\u00f3n del Algebra Lineal en relaci\u00f3n de los m\u00e9todos de Manifold Learning . La ense\u00f1anza que se lleva de esto que, dependiendo del volumen de datos que se trabaje, la capidad de c\u00f3mputo y las habilidades de programaci\u00f3n suficiente, se pueden probar y automatizar varios de estos m\u00e9todos. Por supuesto, quedar\u00e1 como responsabilidad del programador buscar el criterio para poder seleccionar el mejor m\u00e9todo (dependiendo del caso en estudio). Referencia In Depth: Principal Component Analysis Unsupervised dimensionality reduction","title":"Reducci\u00f3n Dimensionalidad"},{"location":"lectures/ml/analisis_no_supervisado/03_reduccion_dimensionalidad/#reduccion-de-dimensionalidad","text":"","title":"Reducci\u00f3n de dimensionalidad"},{"location":"lectures/ml/analisis_no_supervisado/03_reduccion_dimensionalidad/#introduccion","text":"En aprendizaje autom\u00e1tico y estad\u00edsticas reducci\u00f3n de dimensionalidad es el proceso de reducci\u00f3n del n\u00famero de variables aleatorias que se trate, y se puede dividir en selecci\u00f3n de funci\u00f3n y extracci\u00f3n de funci\u00f3n Sin embargo, se puede utilizar como un paso de preprocesamiento de transformaci\u00f3n de datos para algoritmos de aprendizaje autom\u00e1tico en conjuntos de datos de modelado predictivo de clasificaci\u00f3n y regresi\u00f3n con algoritmos de aprendizaje supervisado. Hay muchos algoritmos de reducci\u00f3n de dimensionalidad entre los que elegir y no existe el mejor algoritmo para todos los casos. En cambio, es una buena idea explorar una variedad de algoritmos de reducci\u00f3n de dimensionalidad y diferentes configuraciones para cada algoritmo.","title":"Introducci\u00f3n"},{"location":"lectures/ml/analisis_no_supervisado/03_reduccion_dimensionalidad/#algoritmos-de-reduccion-de-la-dimensionalidad","text":"Hay muchos algoritmos que pueden ser usados para la reducci\u00f3n de la dimensionalidad. Dos clases principales de m\u00e9todos son los que se extraen del \u00e1lgebra lineal y los que se extraen del aprendizaje m\u00faltiple.","title":"Algoritmos de reducci\u00f3n de la dimensionalidad"},{"location":"lectures/ml/analisis_no_supervisado/03_reduccion_dimensionalidad/#metodos-de-algebra-lineal","text":"Los m\u00e9todos de factorizaci\u00f3n matricial extra\u00eddos del campo del \u00e1lgebra lineal pueden utilizarse para la dimensionalidad. Algunos de los m\u00e9todos m\u00e1s populares incluyen: An\u00e1lisis de los componentes principales Descomposici\u00f3n del valor singular Factorizaci\u00f3n de matriz no negativa","title":"M\u00e9todos de \u00e1lgebra lineal"},{"location":"lectures/ml/analisis_no_supervisado/03_reduccion_dimensionalidad/#multiples-metodos-de-aprendizaje","text":"Los m\u00faltiples m\u00e9todos de aprendizaje buscan una proyecci\u00f3n de dimensiones inferiores de alta entrada dimensional que capte las propiedades salientes de los datos de entrada. Algunos de los m\u00e9todos m\u00e1s populares incluyen: Isomap Embedding Locally Linear Embedding Multidimensional Scaling Spectral Embedding t-distributed Stochastic Neighbor Embedding (t-sne) Cada algoritmo ofrece un enfoque diferente para el desaf\u00edo de descubrir las relaciones naturales en los datos de dimensiones inferiores. No hay un mejor algoritmo de reducci\u00f3n de la dimensionalidad, y no hay una manera f\u00e1cil de encontrar el mejor algoritmo para sus datos sin usar experimentos controlados. Debido a la importancia que se tiene en el mundo del machine lerning, se dar\u00e1 un explicaci\u00f3n formal del m\u00e9todo de PCA y luego se dar\u00e1 una breve rese\u00f1a de los dem\u00e1s m\u00e9todos.","title":"M\u00faltiples m\u00e9todos de aprendizaje"},{"location":"lectures/ml/analisis_no_supervisado/03_reduccion_dimensionalidad/#pca","text":"El an\u00e1lisis de componentes principales (Principal Component Analysis PCA) es un m\u00e9todo de reducci\u00f3n de dimensionalidad que permite simplificar la complejidad de espacios con m\u00faltiples dimensiones a la vez que conserva su informaci\u00f3n. Sup\u00f3ngase que existe una muestra con \\(n\\) individuos cada uno con \\(p\\) variables ( \\(X_1\\) ,..., \\(X_p\\) ), es decir, el espacio muestral tiene \\(p\\) dimensiones. PCA permite encontrar un n\u00famero de factores subyacentes ( \\(z<p\\) ) que explican aproximadamente lo mismo que las \\(p\\) variables originales. Donde antes se necesitaban \\(p\\) valores para caracterizar a cada individuo, ahora bastan \\(z\\) valores. Cada una de estas \\(z\\) nuevas variables recibe el nombre de componente principal. El m\u00e9todo de PCA permite por lo tanto \"condensar\" la informaci\u00f3n aportada por m\u00faltiples variables en solo unas pocas componentes. Aun as\u00ed, no hay que olvidar que sigue siendo necesario disponer del valor de las variables originales para calcular las componentes. Dos de las principales aplicaciones del PCA son la visualizaci\u00f3n y el preprocesado de predictores previo ajuste de modelos supervisados.","title":"PCA"},{"location":"lectures/ml/analisis_no_supervisado/03_reduccion_dimensionalidad/#interpretacion-geometrica-de-las-componentes-principales","text":"Una forma intuitiva de entender el proceso de PCA es interpretar las componentes principales desde un punto de vista geom\u00e9trico. Sup\u00f3ngase un conjunto de observaciones para las que se dispone de dos variables ( \\(X_1\\) , \\(X_2\\) ). El vector que define la primera componente principal ( \\(Z_1\\) ) sigue la direcci\u00f3n en la que las observaciones tienen m\u00e1s varianza (l\u00ednea roja). La proyecci\u00f3n de cada observaci\u00f3n sobre esa direcci\u00f3n equivale al valor de la primera componente para dicha observaci\u00f3n (principal component score, \\(z_{i1}\\) ). La segunda componente ( \\(Z_2\\) ) sigue la segunda direcci\u00f3n en la que los datos muestran mayor varianza y que no est\u00e1 correlacionada con la primera componente. La condici\u00f3n de no correlaci\u00f3n entre componentes principales equivale a decir que sus direcciones son perpendiculares/ortogonales.","title":"Interpretaci\u00f3n geom\u00e9trica de las componentes principales"},{"location":"lectures/ml/analisis_no_supervisado/03_reduccion_dimensionalidad/#calculo-de-las-componentes-principales","text":"Cada componente principal ( \\(Z_i\\) ) se obtiene por combinaci\u00f3n lineal de las variables originales. Se pueden entender como nuevas variables obtenidas al combinar de una determinada forma las variables originales. La primera componente principal de un grupo de variables ( \\(X_1,...,X_p\\) ) es la combinaci\u00f3n lineal normalizada de dichas variables que tiene mayor varianza: \\[ Z_1 = \\phi_{11}X_1 + ... + \\phi_{p1}X_p\\] Que la combinaci\u00f3n lineal sea normalizada implica que: \\[\\sum_{j=1}^p \\phi^2_{j1} = 1\\] Los t\u00e9rminos \\(\\phi_{11},...,\\phi_{p1}\\) reciben en el nombre de loadings y son los que definen las componentes. Por ejemplo, \\(\\phi_{11}\\) es el loading de la variable \\(X_1\\) de la primera componente principal. Los loadings pueden interpretarse como el peso/importancia que tiene cada variable en cada componente y, por lo tanto, ayudan a conocer que tipo de informaci\u00f3n recoge cada una de las componentes. Dado un set de datos \\(X\\) con \\(n\\) observaciones y \\(p\\) variables, el proceso a seguir para calcular la primera componente principal es: Centrar las variables: se resta a cada valor la media de la variable a la que pertenece. Con esto se consigue que todas las variables tengan media cero. Se resuelve un problema de optimizaci\u00f3n para encontrar el valor de los loadings con los que se maximiza la varianza. Una forma de resolver esta optimizaci\u00f3n es mediante el c\u00e1lculo de eigenvector-eigenvalue de la matriz de covarianzas. Una vez calculada la primera componente ( \\(Z_1\\) ), se calcula la segunda ( \\(Z_2\\) ) repitiendo el mismo proceso pero a\u00f1adiendo la condici\u00f3n de que la combinaci\u00f3n lineal no pude estar correlacionada con la primera componente. Esto equivale a decir que \\(Z_1\\) y \\(Z_2\\) tienen que ser perpendiculares. EL proceso se repite de forma iterativa hasta calcular todas las posibles componentes ( min( \\(n-1, p\\) ) ) o hasta que se decida detener el proceso. El orden de importancia de las componentes viene dado por la magnitud del eigenvalue asociado a cada eigenvector.","title":"C\u00e1lculo de las componentes principales"},{"location":"lectures/ml/analisis_no_supervisado/03_reduccion_dimensionalidad/#caracteristicas-del-pca","text":"Escalado de las variables : El proceso de PCA identifica las direcciones con mayor varianza. Reproducibilidad de las componentes : El proceso de PCA est\u00e1ndar es determinista, genera siempre las mismas componentes principales, es decir, el valor de los loadings resultantes es el mismo. Influencia de outliers : Al trabajar con varianzas, el m\u00e9todo PCA es muy sensible a outliers, por lo que es recomendable estudiar si los hay. La detecci\u00f3n de valores at\u00edpicos con respecto a una determinada dimensi\u00f3n es algo relativamente sencillo de hacer mediante comprobaciones gr\u00e1ficas.","title":"Caracter\u00edsticas del PCA"},{"location":"lectures/ml/analisis_no_supervisado/03_reduccion_dimensionalidad/#proporcion-de-varianza-explicada","text":"Una de las preguntas m\u00e1s frecuentes que surge tras realizar un PCA es: \u00bfCu\u00e1nta informaci\u00f3n presente en el set de datos original se pierde al proyectar las observaciones en un espacio de menor dimensi\u00f3n? o lo que es lo mismo \u00bfCuanta informaci\u00f3n es capaz de capturar cada una de las componentes principales obtenidas? Para contestar a estas preguntas se recurre a la proporci\u00f3n de varianza explicada por cada componente principal. Asumiendo que las variables se han normalizado para tener media cero, la varianza total presente en el set de datos se define como \\[\\sum_{j=1}^p Var(X_j) = \\dfrac{1}{n}\\sum_{j=1}^p\\sum_{i=1}^nx_{ij}^2\\] y la varianza explicada por la componente m es \\[\\dfrac{1}{n}\\sum_{i=1}^n z_{im}^2 = \\dfrac{1}{n}\\sum_{i=1}^n (\\sum_{j=1}^p \\phi_{jm}x_{ij})^2\\] Por lo tanto, la proporci\u00f3n de varianza explicada por la componente m viene dada por el ratio \\[ \\dfrac{\\sum_{i=1}^n (\\sum_{j=1}^p \\phi_{jm}x_{ij})^2}{\\sum_{j=1}^p\\sum_{i=1}^nx_{ij}^2}\\] Tanto la proporci\u00f3n de varianza explicada, como la proporci\u00f3n de varianza explicada acumulada, son dos valores de gran utilidad a la hora de decidir el n\u00famero de componentes principales a utilizar en los an\u00e1lisis posteriores. Si se calculan todas las componentes principales de un set de datos, entonces, aunque transformada, se est\u00e1 almacenando toda la informaci\u00f3n presente en los datos originales. El sumatorio de la proporci\u00f3n de varianza explicada acumulada de todas las componentes es siempre 1.","title":"Proporci\u00f3n de varianza explicada"},{"location":"lectures/ml/analisis_no_supervisado/03_reduccion_dimensionalidad/#numero-optimo-de-componentes-principales","text":"Por lo general, dada una matriz de datos de dimensiones \\(n \\times p\\) , el n\u00famero de componentes principales que se pueden calcular es como m\u00e1ximo de \\(n-1\\) o \\(p\\) (el menor de los dos valores es el limitante). Sin embargo, siendo el objetivo del PCA reducir la dimensionalidad, suelen ser de inter\u00e9s utilizar el n\u00famero m\u00ednimo de componentes que resultan suficientes para explicar los datos. No existe una respuesta o m\u00e9todo \u00fanico que permita identificar cual es el n\u00famero \u00f3ptimo de componentes principales a utilizar. Una forma de proceder muy extendida consiste en evaluar la proporci\u00f3n de varianza explicada acumulada y seleccionar el n\u00famero de componentes m\u00ednimo a partir del cual el incremento deja de ser sustancial.","title":"N\u00famero \u00f3ptimo de componentes principales"},{"location":"lectures/ml/analisis_no_supervisado/03_reduccion_dimensionalidad/#aplicacion","text":"El m\u00e9todo Principal Components Regression PCR consiste en ajustar un modelo de regresi\u00f3n lineal por m\u00ednimos cuadrados empleando como predictores las componentes generadas a partir de un Principal Component Analysis (PCA). De esta forma, con un n\u00famero reducido de componentes se puede explicar la mayor parte de la varianza de los datos. En los estudios observacionales, es frecuente disponer de un n\u00famero elevado de variables que se pueden emplear como predictores, sin embargo, esto no implica necesariamente que se disponga de mucha informaci\u00f3n. Si las variables est\u00e1n correlacionadas entre ellas, la informaci\u00f3n que aportan es redundante y adem\u00e1s, se incumple la condici\u00f3n de no colinealidad necesaria en la regresi\u00f3n por m\u00ednimos cuadrados. Dado que el PCA es \u00fatil eliminando informaci\u00f3n redundante, si se emplean como predictores las componentes principales, se puede mejorar el modelo de regresi\u00f3n. Es importante tener en cuenta que, si bien el Principal Components Regression reduce el n\u00famero de predictores del modelo, no se puede considerar como un m\u00e9todo de selecci\u00f3n de variables ya que todas ellas se necesitan para el c\u00e1lculo de las componentes. La identificaci\u00f3n del n\u00famero \u00f3ptimo de componentes principales que se emplean como predictores en PCR puede identificarse por validaci\u00f3n cruzada. Datos : El set de datos USArrests contiene el porcentaje de asaltos (Assault), asesinatos (Murder) y secuestros (Rape) por cada 100,000 habitantes para cada uno de los 50 estados de USA (1973). Adem\u00e1s, tambi\u00e9n incluye el porcentaje de la poblaci\u00f3n de cada estado que vive en zonas rurales (UrbanPoP). # librerias import os import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns pd . set_option ( 'display.max_columns' , 500 ) # Ver m\u00e1s columnas de los dataframes # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab % matplotlib inline from sklearn.model_selection import train_test_split from sklearn.decomposition import PCA from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import scale # Datos datos = pd . read_csv ( 'data/USArrests.csv' ) datos = datos . rename ( columns = { datos . columns [ 0 ]: 'index' }) . set_index ( 'index' ) datos . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Murder Assault UrbanPop Rape index Alabama 13.2 236 58 21.2 Alaska 10.0 263 48 44.5 Arizona 8.1 294 80 31.0 Arkansas 8.8 190 50 19.5 California 9.0 276 91 40.6 Veamos una exploraci\u00f3n inicial de los datos: print ( '----------------------' ) print ( 'Media de cada variable' ) print ( '----------------------' ) datos . mean ( axis = 0 ) ---------------------- Media de cada variable ---------------------- Murder 7.788 Assault 170.760 UrbanPop 65.540 Rape 21.232 dtype: float64 La media de las variables muestra que hay tres veces m\u00e1s secuestros que asesinatos y 8 veces m\u00e1s asaltos que secuestros. print ( '-------------------------' ) print ( 'Varianza de cada variable' ) print ( '-------------------------' ) datos . var ( axis = 0 ) ------------------------- Varianza de cada variable ------------------------- Murder 18.970465 Assault 6945.165714 UrbanPop 209.518776 Rape 87.729159 dtype: float64 La varianza es muy distinta entre las variables, en el caso de Assault, la varianza es varios \u00f3rdenes de magnitud superior al resto. Si no se estandarizan las variables para que tengan media cero y desviaci\u00f3n est\u00e1ndar de uno antes de realizar el estudio PCA, la variable Assault, que tiene una media y dispersi\u00f3n muy superior al resto, dominar\u00e1 la mayor\u00eda de las componentes principales. Modelo PCA La clase sklearn.decomposition.PCA incorpora las principales funcionalidades que se necesitan a la hora de trabajar con modelos PCA. El argumento n_components determina el n\u00famero de componentes calculados. Si se indica None, se calculan todas las posibles (min(filas, columnas) - 1). Por defecto, PCA() centra los valores pero no los escala. Esto es importante ya que, si las variables tienen distinta dispersi\u00f3n, como en este caso, es necesario escalarlas. Una forma de hacerlo es combinar un StandardScaler() y un PCA() dentro de un pipeline . # Entrenamiento modelo PCA con escalado de los datos # ============================================================================== pca_pipe = make_pipeline ( StandardScaler (), PCA ()) pca_pipe . fit ( datos ) # Se extrae el modelo entrenado del pipeline modelo_pca = pca_pipe . named_steps [ 'pca' ] Una vez entrenado el objeto PCA , pude accederse a toda la informaci\u00f3n de las componentes creadas. components_ contiene el valor de los loadings \ud835\udf19 que definen cada componente (eigenvector). Las filas se corresponden con las componentes principals (ordenadas de mayor a menor varianza explicada). Las filas se corresponden con las variables de entrada. # Se combierte el array a dataframe para a\u00f1adir nombres a los ejes. pd . DataFrame ( data = modelo_pca . components_ , columns = datos . columns , index = [ 'PC1' , 'PC2' , 'PC3' , 'PC4' ] ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Murder Assault UrbanPop Rape PC1 0.535899 0.583184 0.278191 0.543432 PC2 0.418181 0.187986 -0.872806 -0.167319 PC3 -0.341233 -0.268148 -0.378016 0.817778 PC4 0.649228 -0.743407 0.133878 0.089024 Analizar con detalle el vector de loadings que forma cada componente puede ayudar a interpretar qu\u00e9 tipo de informaci\u00f3n recoge cada una de ellas. Por ejemplo, la primera componente es el resultado de la siguiente combinaci\u00f3n lineal de las variables originales: \\[PC1=0.535899 Murder+0.583184 Assault+0.278191 UrbanPop+0.543432 Rape\\] Los pesos asignados en la primera componente a las variables Assault, Murder y Rape son aproximadamente iguales entre ellos y superiores al asignado a UrbanPoP. Esto significa que la primera componente recoge mayoritariamente la informaci\u00f3n correspondiente a los delitos. En la segunda componente, es la variable UrbanPoP es la que tiene con diferencia mayor peso, por lo que se corresponde principalmente con el nivel de urbanizaci\u00f3n del estado. Si bien en este ejemplo la interpretaci\u00f3n de las componentes es bastante clara, no en todos los casos ocurre lo mismo, sobre todo a medida que aumenta el n\u00famero de variables. La influencia de las variables en cada componente analizarse visualmente con un gr\u00e1fico de tipo heatmap. # Heatmap componentes # ============================================================================== plt . figure ( figsize = ( 12 , 4 )) componentes = modelo_pca . components_ plt . imshow ( componentes . T , cmap = 'viridis' , aspect = 'auto' ) plt . yticks ( range ( len ( datos . columns )), datos . columns ) plt . xticks ( range ( len ( datos . columns )), np . arange ( modelo_pca . n_components_ ) + 1 ) plt . grid ( False ) plt . colorbar (); Una vez calculadas las componentes principales, se puede conocer la varianza explicada por cada una de ellas, la proporci\u00f3n respecto al total y la proporci\u00f3n de varianza acumulada. Esta informaci\u00f3n est\u00e1 almacenada en los atributos explained_variance_ y explained_variance_ratio_ del modelo. # graficar varianza por componente percent_variance = np . round ( modelo_pca . explained_variance_ratio_ * 100 , decimals = 2 ) columns = [ 'PC1' , 'PC2' , 'PC3' , 'PC4' ] plt . figure ( figsize = ( 12 , 4 )) plt . bar ( x = range ( 1 , 5 ), height = percent_variance , tick_label = columns ) plt . xticks ( np . arange ( modelo_pca . n_components_ ) + 1 ) plt . ylabel ( 'Componente principal' ) plt . xlabel ( 'Por. varianza explicada' ) plt . title ( 'Porcentaje de varianza explicada por cada componente' ) plt . show () Ahora realizamos el gr\u00e1fico pero respecto a la suma acumulada. # graficar varianza por la suma acumulada de los componente percent_variance_cum = np . cumsum ( percent_variance ) columns = [ 'PC1' , 'PC1+PC2' , 'PC1+PC2+PC3' , 'PC1+PC2+PC3+PC4' ] plt . figure ( figsize = ( 12 , 4 )) plt . bar ( x = range ( 1 , 5 ), height = percent_variance_cum , tick_label = columns ) plt . ylabel ( 'Percentate of Variance Explained' ) plt . xlabel ( 'Principal Component Cumsum' ) plt . title ( 'PCA Scree Plot' ) plt . show () Si se empleasen \u00fanicamente las dos primeras componentes se conseguir\u00eda explicar el 87% de la varianza observada. Transformaci\u00f3n Una vez entrenado el modelo, con el m\u00e9todo transform() se puede reducir la dimensionalidad de nuevas observaciones proyect\u00e1ndolas en el espacio definido por las componentes. # Proyecci\u00f3n de las observaciones de entrenamiento # ============================================================================== proyecciones = pca_pipe . transform ( X = datos ) proyecciones = pd . DataFrame ( proyecciones , columns = [ 'PC1' , 'PC2' , 'PC3' , 'PC4' ], index = datos . index ) proyecciones . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PC1 PC2 PC3 PC4 index Alabama 0.985566 1.133392 -0.444269 0.156267 Alaska 1.950138 1.073213 2.040003 -0.438583 Arizona 1.763164 -0.745957 0.054781 -0.834653 Arkansas -0.141420 1.119797 0.114574 -0.182811 California 2.523980 -1.542934 0.598557 -0.341996 La transformaci\u00f3n es el resultado de multiplicar los vectores que definen cada componente con el valor de las variables. Puede calcularse de forma manual: proyecciones = np . dot ( modelo_pca . components_ , scale ( datos ) . T ) proyecciones = pd . DataFrame ( proyecciones , index = [ 'PC1' , 'PC2' , 'PC3' , 'PC4' ]) proyecciones = proyecciones . transpose () . set_index ( datos . index ) proyecciones . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PC1 PC2 PC3 PC4 index Alabama 0.985566 1.133392 -0.444269 0.156267 Alaska 1.950138 1.073213 2.040003 -0.438583 Arizona 1.763164 -0.745957 0.054781 -0.834653 Arkansas -0.141420 1.119797 0.114574 -0.182811 California 2.523980 -1.542934 0.598557 -0.341996 Reconstrucci\u00f3n Puede revertirse la transformaci\u00f3n y reconstruir el valor inicial con el m\u00e9todo inverse_transform(). Es importante tener en cuenta que, la reconstrucci\u00f3n, solo ser\u00e1 completa si se han incluido todas las componentes. # Recostruccion de las proyecciones # ============================================================================== recostruccion = pca_pipe . inverse_transform ( X = proyecciones ) recostruccion = pd . DataFrame ( recostruccion , columns = datos . columns , index = datos . index ) print ( '------------------' ) print ( 'Valores originales' ) print ( '------------------' ) display ( recostruccion . head ()) print ( '---------------------' ) print ( 'Valores reconstruidos' ) print ( '---------------------' ) display ( datos . head ()) ------------------ Valores originales ------------------ .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Murder Assault UrbanPop Rape index Alabama 13.2 236.0 58.0 21.2 Alaska 10.0 263.0 48.0 44.5 Arizona 8.1 294.0 80.0 31.0 Arkansas 8.8 190.0 50.0 19.5 California 9.0 276.0 91.0 40.6 --------------------- Valores reconstruidos --------------------- .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Murder Assault UrbanPop Rape index Alabama 13.2 236 58 21.2 Alaska 10.0 263 48 44.5 Arizona 8.1 294 80 31.0 Arkansas 8.8 190 50 19.5 California 9.0 276 91 40.6","title":"Aplicaci\u00f3n"},{"location":"lectures/ml/analisis_no_supervisado/03_reduccion_dimensionalidad/#t-distributed-stochastic-neighbor-embedding-t-sne","text":"t-Distributed Stochastic Neighbor Embedding (t-SNE) es una t\u00e9cnica no lineal no supervisada utilizada principalmente para la exploraci\u00f3n de datos y la visualizaci\u00f3n de datos de alta dimensi\u00f3n. En t\u00e9rminos m\u00e1s simples, tSNE le da una sensaci\u00f3n o intuici\u00f3n de c\u00f3mo se organizan los datos en un espacio de alta dimensi\u00f3n. Fue desarrollado por Laurens van der Maatens y Geoffrey Hinton en 2008.","title":"t-Distributed Stochastic Neighbor Embedding (t-SNE)"},{"location":"lectures/ml/analisis_no_supervisado/03_reduccion_dimensionalidad/#comparando-con-pca","text":"Si est\u00e1 familiarizado con An\u00e1lisis de componentes principales (PCA), entonces como yo , probablemente se est\u00e9 preguntando la diferencia entre PCA y tSNE. Lo primero a tener en cuenta es que PCA se desarroll\u00f3 en 1933, mientras que tSNE se desarroll\u00f3 en 2008. Mucho ha cambiado en el mundo de la ciencia de datos desde 1933, principalmente en el \u00e1mbito del c\u00e1lculo y el tama\u00f1o de los datos. En segundo lugar, PCA es una t\u00e9cnica de reducci\u00f3n de dimensi\u00f3n lineal que busca maximizar la varianza y preserva las distancias pares grandes. En otras palabras, las cosas que son diferentes terminan muy separadas. Esto puede conducir a una visualizaci\u00f3n deficiente, especialmente cuando se trata de estructuras distribuidoras no lineales. Piense en una estructura m\u00faltiple como cualquier forma geom\u00e9trica como: cilindro, bola, curva, etc. tSNE difiere de PCA al preservar solo peque\u00f1as distancias por pares o similitudes locales, mientras que PCA se preocupa por preservar distancias pares grandes para maximizar la varianza. Laurens ilustra bastante bien el enfoque PCA y tSNE utilizando el conjunto de datos Swiss Roll en la Figura 1 [1]. Puede ver que debido a la no linealidad de este conjunto de datos de juguete (m\u00faltiple) y la preservaci\u00f3n de grandes distancias, PCA conservar\u00eda incorrectamente la estructura de los datos. Figura 1 \u2013 Dataset de rollo suizo. Conservar la distancia peque\u00f1a con tSNE (l\u00ednea continua) frente a la maximizaci\u00f3n de la variaci\u00f3n PCA [1]","title":"Comparando con PCA"},{"location":"lectures/ml/analisis_no_supervisado/03_reduccion_dimensionalidad/#explicacion","text":"Ahora que sabemos por qu\u00e9 podr\u00edamos usar tSNE sobre PCA, analicemos c\u00f3mo funciona tSNE. El algoritmo tSNE calcula una medida de similitud entre pares de instancias en el espacio de alta dimensi\u00f3n y en el espacio de baja dimensi\u00f3n. Luego trata de optimizar estas dos medidas de similitud usando una funci\u00f3n de costo. Vamos a dividirlo en 3 pasos b\u00e1sicos. Paso 1, mide similitudes entre puntos en el espacio de alta dimensi\u00f3n. Piense en un conjunto de puntos de datos dispersos en un espacio 2D (Figura 2). Para cada punto de datos (xi) centraremos una distribuci\u00f3n Gaussiana sobre ese punto. Luego medimos la densidad de todos los puntos (xj) bajo esa distribuci\u00f3n Gaussiana. Luego renormalize para todos los puntos. Esto nos da un conjunto de probabilidades (Pij) para todos los puntos. Esas probabilidades son proporcionales a las similitudes. Todo lo que eso significa es que si los puntos de datos x1 y x2 tienen valores iguales bajo este c\u00edrculo gaussiano, entonces sus proporciones y similitudes son iguales y, por lo tanto, tienes similitudes locales en la estructura de este espacio de alta dimensi\u00f3n. La distribuci\u00f3n gaussiana o el c\u00edrculo se pueden manipular usando lo que se llama perplejidad, que influye en la varianza de la distribuci\u00f3n (tama\u00f1o del c\u00edrculo) y esencialmente en el n\u00famero de vecinos m\u00e1s cercanos. El rango normal para la perplejidad est\u00e1 entre 5 y 50 [2]. Figura 2 \u2013 Medici\u00f3n de similitudes por pares en el espacio de alta dimensi\u00f3n Figura 2 \u2013 Medici\u00f3n de similitudes por pares en el espacio de alta dimensi\u00f3n 2. El paso 2 es similar al paso 1, pero en lugar de usar una distribuci\u00f3n gaussiana se usa una distribuci\u00f3n t de Student con un grado de libertad, que tambi\u00e9n se conoce como la distribuci\u00f3n de Cauchy (Figura 3). Esto nos da un segundo conjunto de probabilidades ( \\(Q_{ij}\\) ) en el espacio de baja dimensi\u00f3n. Como puede ver, la distribuci\u00f3n t de Student tiene colas m\u00e1s pesadas que la distribuci\u00f3n normal. Las colas pesadas permiten un mejor modelado de distancias muy separadas. Figura 3 \u2013 Distribuci\u00f3n noraml vs t-student El \u00faltimo paso es que queremos que este conjunto de probabilidades del espacio de baja dimensi\u00f3n ( \\(Q_{ij}\\) ) refleje las del espacio de alta dimensi\u00f3n ( \\(P_{ij}\\) ) de la mejor manera posible. Queremos que las dos estructuras de mapa sean similares. Medimos la diferencia entre las distribuciones de probabilidad de los espacios bidimensionales utilizando la divergencia de Kullback-Liebler (KL). No incluir\u00e9 mucho en KL, excepto que es un enfoque asim\u00e9trico que compara de manera eficiente los grandes valores \\(P_{ij}\\) y \\(Q_{ij}\\) . Finalmente, utilizamos el descenso de gradiente para minimizar nuestra funci\u00f3n de costo KL.","title":"Explicaci\u00f3n"},{"location":"lectures/ml/analisis_no_supervisado/03_reduccion_dimensionalidad/#aplicacion_1","text":"Laurens van der Maaten menciona el uso de tSNE en \u00e1reas como investigaci\u00f3n del clima, seguridad inform\u00e1tica, bioinform\u00e1tica, investigaci\u00f3n del c\u00e1ncer, etc. tSNE podr\u00eda usarse en datos de alta dimensi\u00f3n y luego el resultado de esas dimensiones se convierte en insumos para alg\u00fan otro modelo de clasificaci\u00f3n . Adem\u00e1s, tSNE podr\u00eda usarse para investigar, aprender o evaluar la segmentaci\u00f3n. Muchas veces seleccionamos la cantidad de segmentos antes del modelado o iteramos despu\u00e9s de los resultados. tSNE a menudo puede mostrar una separaci\u00f3n clara en los datos. Esto se puede usar antes de usar su modelo de segmentaci\u00f3n para seleccionar un n\u00famero de cl\u00faster o despu\u00e9s para evaluar si sus segmentos realmente se mantienen. tSNE, sin embargo, no es un enfoque de agrupamiento, ya que no conserva las entradas como PCA y los valores a menudo pueden cambiar entre ejecuciones, por lo que es pura exploraci\u00f3n. A continuaci\u00f3n se procede a comparar de manera visual los algoritmos de PCA y tSNE en el conjunto de datos Digits . Datos : El conjunto de datos contiene im\u00e1genes de d\u00edgitos escritos a mano: 10 clases donde cada clase se refiere a un d\u00edgito. Los programas de preprocesamiento puestos a disposici\u00f3n por NIST se utilizaron para extraer mapas de bits normalizados de d\u00edgitos escritos a mano de un formulario preimpreso. De un total de 43 personas, 30 contribuyeron al conjunto de entrenamiento y diferentes 13 al conjunto de prueba. Los mapas de bits de 32x32 se dividen en bloques no superpuestos de 4x4 y se cuenta el n\u00famero de p\u00edxeles en cada bloque. Esto genera una matriz de entrada de 8x8 donde cada elemento es un n\u00famero entero en el rango 0..16. Esto reduce la dimensionalidad y da invariancia a peque\u00f1as distorsiones. # Load Python Libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from time import time % matplotlib inline from sklearn.datasets import load_digits from sklearn.manifold import TSNE from sklearn.decomposition import PCA digits = load_digits () df = pd . DataFrame ( digits [ 'data' ]) df [ 'label' ] = digits [ 'target' ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 label 0 0.0 0.0 5.0 13.0 9.0 1.0 0.0 0.0 0.0 0.0 13.0 15.0 10.0 15.0 5.0 0.0 0.0 3.0 15.0 2.0 0.0 11.0 8.0 0.0 0.0 4.0 12.0 0.0 0.0 8.0 8.0 0.0 0.0 5.0 8.0 0.0 0.0 9.0 8.0 0.0 0.0 4.0 11.0 0.0 1.0 12.0 7.0 0.0 0.0 2.0 14.0 5.0 10.0 12.0 0.0 0.0 0.0 0.0 6.0 13.0 10.0 0.0 0.0 0.0 0 1 0.0 0.0 0.0 12.0 13.0 5.0 0.0 0.0 0.0 0.0 0.0 11.0 16.0 9.0 0.0 0.0 0.0 0.0 3.0 15.0 16.0 6.0 0.0 0.0 0.0 7.0 15.0 16.0 16.0 2.0 0.0 0.0 0.0 0.0 1.0 16.0 16.0 3.0 0.0 0.0 0.0 0.0 1.0 16.0 16.0 6.0 0.0 0.0 0.0 0.0 1.0 16.0 16.0 6.0 0.0 0.0 0.0 0.0 0.0 11.0 16.0 10.0 0.0 0.0 1 2 0.0 0.0 0.0 4.0 15.0 12.0 0.0 0.0 0.0 0.0 3.0 16.0 15.0 14.0 0.0 0.0 0.0 0.0 8.0 13.0 8.0 16.0 0.0 0.0 0.0 0.0 1.0 6.0 15.0 11.0 0.0 0.0 0.0 1.0 8.0 13.0 15.0 1.0 0.0 0.0 0.0 9.0 16.0 16.0 5.0 0.0 0.0 0.0 0.0 3.0 13.0 16.0 16.0 11.0 5.0 0.0 0.0 0.0 0.0 3.0 11.0 16.0 9.0 0.0 2 3 0.0 0.0 7.0 15.0 13.0 1.0 0.0 0.0 0.0 8.0 13.0 6.0 15.0 4.0 0.0 0.0 0.0 2.0 1.0 13.0 13.0 0.0 0.0 0.0 0.0 0.0 2.0 15.0 11.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 12.0 12.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 10.0 8.0 0.0 0.0 0.0 8.0 4.0 5.0 14.0 9.0 0.0 0.0 0.0 7.0 13.0 13.0 9.0 0.0 0.0 3 4 0.0 0.0 0.0 1.0 11.0 0.0 0.0 0.0 0.0 0.0 0.0 7.0 8.0 0.0 0.0 0.0 0.0 0.0 1.0 13.0 6.0 2.0 2.0 0.0 0.0 0.0 7.0 15.0 0.0 9.0 8.0 0.0 0.0 5.0 16.0 10.0 0.0 16.0 6.0 0.0 0.0 4.0 15.0 16.0 13.0 16.0 1.0 0.0 0.0 0.0 0.0 3.0 15.0 10.0 0.0 0.0 0.0 0.0 0.0 2.0 16.0 4.0 0.0 0.0 4 # PCA scaler = StandardScaler () X = df . drop ( columns = 'label' ) y = df [ 'label' ] embedding = PCA ( n_components = 2 ) X_transform = embedding . fit_transform ( X ) df_pca = pd . DataFrame ( X_transform , columns = [ 'Score1' , 'Score2' ]) df_pca [ 'label' ] = y # Plot Digits PCA # Set style of scatterplot sns . set_context ( \"notebook\" , font_scale = 1.1 ) sns . set_style ( \"ticks\" ) # Create scatterplot of dataframe sns . lmplot ( x = 'Score1' , y = 'Score2' , data = df_pca , fit_reg = False , legend = True , height = 9 , hue = 'label' , scatter_kws = { \"s\" : 200 , \"alpha\" : 0.3 }) plt . title ( 'PCA Results: Digits' , weight = 'bold' ) . set_fontsize ( '14' ) plt . xlabel ( 'Prin Comp 1' , weight = 'bold' ) . set_fontsize ( '10' ) plt . ylabel ( 'Prin Comp 2' , weight = 'bold' ) . set_fontsize ( '10' ) Al graficar las dos componentes principales del m\u00e9todo PCA, se observa que no existe una clara distinci\u00f3n entre la distintas clases (solo se ve un gran c\u00famulo de puntos mezclados). # tsne scaler = StandardScaler () X = df . drop ( columns = 'label' ) y = df [ 'label' ] embedding = TSNE ( n_components = 2 ) X_transform = embedding . fit_transform ( X ) df_tsne = pd . DataFrame ( X_transform , columns = [ '_DIM_1_' , '_DIM_2_' ]) df_tsne [ 'label' ] = y # Plot Digits t-SNE sns . set_context ( \"notebook\" , font_scale = 1.1 ) sns . set_style ( \"ticks\" ) sns . lmplot ( x = '_DIM_1_' , y = '_DIM_2_' , data = df_tsne , fit_reg = False , legend = True , height = 9 , hue = 'label' , scatter_kws = { \"s\" : 200 , \"alpha\" : 0.3 }) plt . title ( 't-SNE Results: Digits' , weight = 'bold' ) . set_fontsize ( '14' ) plt . xlabel ( 'Dimension 1' , weight = 'bold' ) . set_fontsize ( '10' ) plt . ylabel ( 'Dimension 2' , weight = 'bold' ) . set_fontsize ( '10' ) Para el caso del m\u00e9todo TSNE, se observa una diferenciaci\u00f3n entre los grupos de estudios (aspecto que fue muy distinto al momento de analizar el m\u00e9todo del PCA). Observaci\u00f3n : Si bien se muestra donde el m\u00e9todo TSNE logra ser superior en aspecto de reducci\u00f3n de dimensionalidad que el m\u00e9todo PCA, no significa que para distintos experimientos se tengan los mismo resultados.","title":"Aplicaci\u00f3n"},{"location":"lectures/ml/analisis_no_supervisado/03_reduccion_dimensionalidad/#otros-metodos-de-reduccion-de-dimensionalidad","text":"Existen otro m\u00e9todos de reducci\u00f3n de dimencionalidad, a continuaci\u00f3n se deja una referencia con la descripci\u00f3n de cada uno de estos algoritmos. Descomposici\u00f3n del valor singular Non-Negative Matrix Factorization Isomap Embedding Locally Linear Embedding Multidimensional Scaling Spectral Embedding","title":"Otros m\u00e9todos de reducci\u00f3n de dimensionalidad"},{"location":"lectures/ml/analisis_no_supervisado/03_reduccion_dimensionalidad/#aplicacion_2","text":"En este ejemplo se quiere aprovechar las bondades de aplicar la reducci\u00f3n de dimensionalidad para ocupar un modelo de clasificaci\u00f3n (en este caso, el modelo de regresi\u00f3n log\u00edstica). Para ello se ocupar\u00e1 el conjunto de datos meatspec.csv Datos : El departamento de calidad de una empresa de alimentaci\u00f3n se encarga de medir el contenido en grasa de la carne que comercializa. Este estudio se realiza mediante t\u00e9cnicas de anal\u00edtica qu\u00edmica, un proceso relativamente costoso en tiempo y recursos. Una alternativa que permitir\u00eda reducir costes y optimizar tiempo es emplear un espectrofot\u00f3metro (instrumento capaz de detectar la absorbancia que tiene un material a diferentes tipos de luz en funci\u00f3n de sus caracter\u00edsticas) e inferir el contenido en grasa a partir de sus medidas. Antes de dar por v\u00e1lida esta nueva t\u00e9cnica, la empresa necesita comprobar qu\u00e9 margen de error tiene respecto al an\u00e1lisis qu\u00edmico. Para ello, se mide el espectro de absorbancia a 100 longitudes de onda en 215 muestras de carne, cuyo contenido en grasa se obtiene tambi\u00e9n por an\u00e1lisis qu\u00edmico, y se entrena un modelo con el objetivo de predecir el contenido en grasa a partir de los valores dados por el espectrofot\u00f3metro. # Librerias from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.decomposition import PCA , TruncatedSVD , NMF from sklearn.manifold import Isomap , LocallyLinearEmbedding , MDS , SpectralEmbedding , TSNE from metrics_regression import summary_metrics # Datos datos = pd . read_csv ( 'data/meatspec.csv' ) datos = datos . drop ( columns = datos . columns [ 0 ]) datos [ 'fat' ] = datos [ 'fat' ] . astype ( float ) datos . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 V29 V30 V31 V32 V33 V34 V35 V36 V37 V38 V39 V40 V41 V42 V43 V44 V45 V46 V47 V48 V49 V50 V51 V52 V53 V54 V55 V56 V57 V58 V59 V60 V61 V62 V63 V64 V65 V66 V67 V68 V69 V70 V71 V72 V73 V74 V75 V76 V77 V78 V79 V80 V81 V82 V83 V84 V85 V86 V87 V88 V89 V90 V91 V92 V93 V94 V95 V96 V97 V98 V99 V100 fat 0 2.61776 2.61814 2.61859 2.61912 2.61981 2.62071 2.62186 2.62334 2.62511 2.62722 2.62964 2.63245 2.63565 2.63933 2.64353 2.64825 2.65350 2.65937 2.66585 2.67281 2.68008 2.68733 2.69427 2.70073 2.70684 2.71281 2.71914 2.72628 2.73462 2.74416 2.75466 2.76568 2.77679 2.78790 2.79949 2.81225 2.82706 2.84356 2.86106 2.87857 2.89497 2.90924 2.92085 2.93015 2.93846 2.94771 2.96019 2.97831 3.00306 3.03506 3.07428 3.11963 3.16868 3.21771 3.26254 3.29988 3.32847 3.34899 3.36342 3.37379 3.38152 3.38741 3.39164 3.39418 3.39490 3.39366 3.39045 3.38541 3.37869 3.37041 3.36073 3.34979 3.33769 3.32443 3.31013 3.29487 3.27891 3.26232 3.24542 3.22828 3.21080 3.19287 3.17433 3.15503 3.13475 3.11339 3.09116 3.06850 3.04596 3.02393 3.00247 2.98145 2.96072 2.94013 2.91978 2.89966 2.87964 2.85960 2.83940 2.81920 22.5 1 2.83454 2.83871 2.84283 2.84705 2.85138 2.85587 2.86060 2.86566 2.87093 2.87661 2.88264 2.88898 2.89577 2.90308 2.91097 2.91953 2.92873 2.93863 2.94929 2.96072 2.97272 2.98493 2.99690 3.00833 3.01920 3.02990 3.04101 3.05345 3.06777 3.08416 3.10221 3.12106 3.13983 3.15810 3.17623 3.19519 3.21584 3.23747 3.25889 3.27835 3.29384 3.30362 3.30681 3.30393 3.29700 3.28925 3.28409 3.28505 3.29326 3.30923 3.33267 3.36251 3.39661 3.43188 3.46492 3.49295 3.51458 3.53004 3.54067 3.54797 3.55306 3.55675 3.55921 3.56045 3.56034 3.55876 3.55571 3.55132 3.54585 3.53950 3.53235 3.52442 3.51583 3.50668 3.49700 3.48683 3.47626 3.46552 3.45501 3.44481 3.43477 3.42465 3.41419 3.40303 3.39082 3.37731 3.36265 3.34745 3.33245 3.31818 3.30473 3.29186 3.27921 3.26655 3.25369 3.24045 3.22659 3.21181 3.19600 3.17942 40.1 2 2.58284 2.58458 2.58629 2.58808 2.58996 2.59192 2.59401 2.59627 2.59873 2.60131 2.60414 2.60714 2.61029 2.61361 2.61714 2.62089 2.62486 2.62909 2.63361 2.63835 2.64330 2.64838 2.65354 2.65870 2.66375 2.66880 2.67383 2.67892 2.68411 2.68937 2.69470 2.70012 2.70563 2.71141 2.71775 2.72490 2.73344 2.74327 2.75433 2.76642 2.77931 2.79272 2.80649 2.82064 2.83541 2.85121 2.86872 2.88905 2.91289 2.94088 2.97325 3.00946 3.04780 3.08554 3.11947 3.14696 3.16677 3.17938 3.18631 3.18924 3.18950 3.18801 3.18498 3.18039 3.17411 3.16611 3.15641 3.14512 3.13241 3.11843 3.10329 3.08714 3.07014 3.05237 3.03393 3.01504 2.99569 2.97612 2.95642 2.93660 2.91667 2.89655 2.87622 2.85563 2.83474 2.81361 2.79235 2.77113 2.75015 2.72956 2.70934 2.68951 2.67009 2.65112 2.63262 2.61461 2.59718 2.58034 2.56404 2.54816 8.4 3 2.82286 2.82460 2.82630 2.82814 2.83001 2.83192 2.83392 2.83606 2.83842 2.84097 2.84374 2.84664 2.84975 2.85307 2.85661 2.86038 2.86437 2.86860 2.87308 2.87789 2.88301 2.88832 2.89374 2.89917 2.90457 2.90991 2.91521 2.92043 2.92565 2.93082 2.93604 2.94128 2.94658 2.95202 2.95777 2.96419 2.97159 2.98045 2.99090 3.00284 3.01611 3.03048 3.04579 3.06194 3.07889 3.09686 3.11629 3.13775 3.16217 3.19068 3.22376 3.26172 3.30379 3.34793 3.39093 3.42920 3.45998 3.48227 3.49687 3.50558 3.51026 3.51221 3.51215 3.51036 3.50682 3.50140 3.49398 3.48457 3.47333 3.46041 3.44595 3.43005 3.41285 3.39450 3.37511 3.35482 3.33376 3.31204 3.28986 3.26730 3.24442 3.22117 3.19757 3.17357 3.14915 3.12429 3.09908 3.07366 3.04825 3.02308 2.99820 2.97367 2.94951 2.92576 2.90251 2.87988 2.85794 2.83672 2.81617 2.79622 5.9 4 2.78813 2.78989 2.79167 2.79350 2.79538 2.79746 2.79984 2.80254 2.80553 2.80890 2.81272 2.81704 2.82184 2.82710 2.83294 2.83945 2.84664 2.85458 2.86331 2.87280 2.88291 2.89335 2.90374 2.91371 2.92305 2.93187 2.94060 2.94986 2.96035 2.97241 2.98606 3.00097 3.01652 3.03220 3.04793 3.06413 3.08153 3.10078 3.12185 3.14371 3.16510 3.18470 3.20140 3.21477 3.22544 3.23505 3.24586 3.26027 3.28063 3.30889 3.34543 3.39019 3.44198 3.49800 3.55407 3.60534 3.64789 3.68011 3.70272 3.71815 3.72863 3.73574 3.74059 3.74357 3.74453 3.74336 3.73991 3.73418 3.72638 3.71676 3.70553 3.69289 3.67900 3.66396 3.64785 3.63085 3.61305 3.59463 3.57582 3.55695 3.53796 3.51880 3.49936 3.47938 3.45869 3.43711 3.41458 3.39129 3.36772 3.34450 3.32201 3.30025 3.27907 3.25831 3.23784 3.21765 3.19766 3.17770 3.15770 3.13753 25.5 El set de datos contiene 101 columnas. Las 100 primeras, nombradas como \\(V_1,...,V_{100}\\) recogen el valor de absorbancia para cada una de las 100 longitudes de onda analizadas (predictores), y la columna fat el contenido en grasa medido por t\u00e9cnicas qu\u00edmicas (variable respuesta). Muchas de las variables est\u00e1n altamente correlacionadas (correlaci\u00f3n absoluta > 0.8), lo que supone un problema a la hora de emplear modelos de regresi\u00f3n lineal. # Correlaci\u00f3n entre columnas num\u00e9ricas def tidy_corr_matrix ( corr_mat ): ''' Funci\u00f3n para convertir una matriz de correlaci\u00f3n de pandas en formato tidy ''' corr_mat = corr_mat . stack () . reset_index () corr_mat . columns = [ 'variable_1' , 'variable_2' , 'r' ] corr_mat = corr_mat . loc [ corr_mat [ 'variable_1' ] != corr_mat [ 'variable_2' ], :] corr_mat [ 'abs_r' ] = np . abs ( corr_mat [ 'r' ]) corr_mat = corr_mat . sort_values ( 'abs_r' , ascending = False ) return ( corr_mat ) corr_matrix = datos . select_dtypes ( include = [ 'float64' , 'int' ]) \\ . corr ( method = 'pearson' ) display ( tidy_corr_matrix ( corr_matrix ) . head ( 5 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } variable_1 variable_2 r abs_r 1019 V11 V10 0.999996 0.999996 919 V10 V11 0.999996 0.999996 1021 V11 V12 0.999996 0.999996 1121 V12 V11 0.999996 0.999996 917 V10 V9 0.999996 0.999996 Se procede aplicar el modelo de regresi\u00f3n lineal . # Divisi\u00f3n de los datos en train y test X = datos . drop ( columns = 'fat' ) y = datos [ 'fat' ] X_train , X_test , y_train , y_test = train_test_split ( X , y . values , train_size = 0.7 , random_state = 1234 , shuffle = True ) # Creaci\u00f3n y entrenamiento del modelo modelo = LinearRegression ( normalize = True ) modelo . fit ( X = X_train , y = y_train ) LinearRegression(normalize=True) # Predicciones test predicciones = modelo . predict ( X = X_test ) predicciones = predicciones . flatten () # Error de test del modelo df_pred = pd . DataFrame ({ 'y' : y_test , 'yhat' : predicciones }) df_summary = summary_metrics ( df_pred ) df_summary .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse mape maape wmape mmape smape 0 2.0904 14.743 3.8397 0.1616 0.1484 0.1088 0.1362 0.1665 Ahora se ocuparan los modelos de reducci\u00f3n de dimensionalidad para entrenar el modelo de regresi\u00f3n lineal. Para ello se ocuparan los distintos algoritmos mencionados. Lo primero es crear una funci\u00f3n que pueda realizar esta tarea de manera autom\u00e1tica. # funcion de reduccion de dimensionalidad y aplicacion de la regresion lineal def dr_pipeline ( df , model_dr ): # datos X = df . drop ( columns = 'fat' ) y = df [ 'fat' ] # reduccion de la dimensionalidad embedding = model_dr X = embedding . fit_transform ( X ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.7 , random_state = 1234 , shuffle = True ) # Creaci\u00f3n y entrenamiento del modelo modelo = LinearRegression ( normalize = True ) modelo . fit ( X = X_train , y = y_train ) # Predicciones test predicciones = modelo . predict ( X = X_test ) predicciones = predicciones . flatten () # Error de test del modelo df_pred = pd . DataFrame ({ 'y' : y_test , 'yhat' : predicciones }) df_summary = summary_metrics ( df_pred ) return df_summary Enfoque: Algebra lineal modelos_algebra_lineal = [ ( 'PCA' , PCA ( n_components = 5 )), ( 'SVD' , TruncatedSVD ( n_components = 5 )), ( 'NMF' , NMF ( n_components = 5 )) ] names = [ x [ 0 ] for x in modelos_algebra_lineal ] results = [ dr_pipeline ( datos , x [ 1 ]) for x in modelos_algebra_lineal ] /home/fralfaro/.cache/pypoetry/virtualenvs/mat281-2021-kaeOORRv-py3.8/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26). warnings.warn((\"The 'init' value, when 'init=None' and \" /home/fralfaro/.cache/pypoetry/virtualenvs/mat281-2021-kaeOORRv-py3.8/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence. warnings.warn(\"Maximum number of iterations %d reached. Increase it to\" df_algebra_lineal = pd . concat ( results ) . reset_index ( drop = True ) df_algebra_lineal [ 'metodo' ] = names df_algebra_lineal .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse mape maape wmape mmape smape metodo 0 2.8050 12.2999 3.5071 0.2869 0.2169 0.1460 0.2183 0.2140 PCA 1 2.6344 11.3195 3.3645 0.2605 0.2023 0.1372 0.1987 0.1979 SVD 2 3.5327 16.4079 4.0507 0.4956 0.2859 0.1839 0.3594 0.2930 NMF Enfoque: Manifold Learning modelos_manifold = [ ( 'Isomap' , Isomap ( n_components = 5 )), ( 'LocallyLinearEmbedding' , LocallyLinearEmbedding ( n_components = 5 )), ( 'MDS' , MDS ( n_components = 5 )), ( 'SpectralEmbedding' , SpectralEmbedding ( n_components = 5 )), ( 'TSNE' , TSNE ( n_components = 2 )), ] names = [ x [ 0 ] for x in modelos_manifold ] results = [ dr_pipeline ( datos , x [ 1 ]) for x in modelos_manifold ] df_manifold = pd . concat ( results ) . reset_index ( drop = True ) df_manifold [ 'metodo' ] = names df_manifold .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse mape maape wmape mmape smape metodo 0 9.4021 133.7433 11.5647 0.9914 0.5142 0.4895 0.7646 0.5491 Isomap 1 9.1329 130.6913 11.4320 0.8865 0.5235 0.4755 0.6995 0.5447 LocallyLinearEmbedding 2 7.6519 109.5706 10.4676 0.7519 0.4474 0.3984 0.5787 0.4857 MDS 3 8.9529 130.9545 11.4435 0.8330 0.5061 0.4661 0.6646 0.5261 SpectralEmbedding 4 9.0884 132.0296 11.4904 0.8893 0.5175 0.4732 0.6946 0.5405 TSNE En este caso en particular, funciona de mejor forma aplicar los m\u00e9todos de descomposici\u00f3n del Algebra Lineal en relaci\u00f3n de los m\u00e9todos de Manifold Learning . La ense\u00f1anza que se lleva de esto que, dependiendo del volumen de datos que se trabaje, la capidad de c\u00f3mputo y las habilidades de programaci\u00f3n suficiente, se pueden probar y automatizar varios de estos m\u00e9todos. Por supuesto, quedar\u00e1 como responsabilidad del programador buscar el criterio para poder seleccionar el mejor m\u00e9todo (dependiendo del caso en estudio).","title":"Aplicaci\u00f3n"},{"location":"lectures/ml/analisis_no_supervisado/03_reduccion_dimensionalidad/#referencia","text":"In Depth: Principal Component Analysis Unsupervised dimensionality reduction","title":"Referencia"},{"location":"lectures/ml/analisis_supervisado_clasificacion/02_clasificacion/","text":"Modelo de regresi\u00f3n log\u00edstica Definici\u00f3n A modo de recuerdo, el modelo de regresio\u0301n lineal general supone que, \\(\\boldsymbol{Y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon},\\) donde: \\(\\boldsymbol{X} = (x_1,...,x_n)^{T}\\) : variable explicativa \\(\\boldsymbol{Y} = (y_1,...,y_n)^{T}\\) : variable respuesta \\(\\boldsymbol{\\epsilon} = (\\epsilon_1,...,\\epsilon_n)^{T}\\) : error se asume un ruido blanco, es decir, \\(\\epsilon \\sim \\mathcal{N}( \\boldsymbol{0},\\sigma^2I)\\) \\(\\boldsymbol{\\beta} = (\\beta_1,...,\\beta_n)^{T}\\) : coeficientes de regresio\u0301n. Por otro lado, el modelo de regresi\u00f3n log\u00edstica analiza datos distribuidos binomialmente de la forma: \\(Y_i \\sim B(p_i,n_i)\\) , para \\(i=1,...,m\\) donde los n\u00fameros de ensayos Bernoulli \\(n_{i}\\) son conocidos y las probabilidades de \u00e9xito \\(p_{i}\\) son desconocidas. Un ejemplo de esta distribuci\u00f3n es el porcentaje de semillas $p_{i} $ que germinan despu\u00e9s de que \\(n_{i}\\) son plantadas. El modelo es entonces obtenido a base de lo que cada ensayo (valor de \\(i\\) ) y el conjunto de variables explicativas/independientes puedan informar acerca de la probabilidad final. Estas variables explicativas pueden pensarse como un vector \\(X_i\\) k-dimensional y el modelo toma entonces la forma: \\[p_i=\\mathbb{E}(\\dfrac{Y_i}{n_i}|X_i)\\] Los logits de las probabilidades binomiales desconocidas (i.e., los logaritmos de la raz\u00f3n de momios) son modeladas como una funci\u00f3n lineal de los \\(X_i\\) : \\[logit(p_i) = ln(\\dfrac{p_i}{1-p_i}) = \\beta_0+\\beta_1x_{1,i} +....+\\beta_kx_{k,i}\\] Note que un elemento particular de \\(X_i\\) puede ser ajustado a 1 para todo \\(i\\) obteni\u00e9ndose una constante independiente en el modelo. Los par\u00e1metros desconocidos \\(\\beta _{j}\\) son usualmente estimados a trav\u00e9s de m\u00e1xima verosimilitud. La interpretaci\u00f3n de los estimados del par\u00e1metro \\(\\beta _{j}\\) es como los efectos aditivos en el logaritmo de la raz\u00f3n de momios para una unidad de cambio en la j\u00e9sima variable explicativa. En el caso de una variable explicativa dicot\u00f3mica, por ejemplo g\u00e9nero, \\(e^{\\beta}\\) es la estimaci\u00f3n de la raz\u00f3n de momios (odds ratio) de tener el resultado para, por decir algo, hombres comparados con mujeres. El modelo tiene una formulaci\u00f3n equivalente dada por: \\[p_i=\\dfrac{1}{e^{-(\\beta_0+\\beta_1x_{1,i} +....+\\beta_kx_{k,i})}+1}\\] Interpretaci\u00f3n para el caso binario La idea es que la regresi\u00f3n log\u00edstica aproxime la probabilidad de obtener 0 (no ocurre cierto suceso) o 1 (ocurre el suceso) con el valor de la variable explicativa \\(x\\) . En esas condiciones, la probabilidad aproximada del suceso se aproximar\u00e1 mediante una funci\u00f3n log\u00edstica del tipo: \\[\\pi(x) =\\dfrac{e^{\\beta_0+\\beta_1x}}{e^{\\beta_0+\\beta_1x}+1}=\\dfrac{1}{e^{-(\\beta_0+\\beta_1x)}+1}\\] que puede reducirse al c\u00e1lculo de una regresi\u00f3n lineal para la funci\u00f3n logit de la probabilidad: \\[g(x) = ln(\\dfrac{\\pi(x)}{1- \\pi(x)})=\\beta_0+\\beta_1 x\\] El gr\u00e1fico de la funci\u00f3n log\u00edstica se muestra en la figura que encabeza esta secci\u00f3n, la variable independiente es la combinaci\u00f3n lineal \\(=\\beta_0+\\beta_1\\) y la variable dependiente es la probabilidad estimada $ \\pi (x)$. Si se realiza la regresi\u00f3n lineal, la forma de la probabilidad estimada puede ser f\u00e1cilmente recuperada a partir de los coeficientes calculados. Para hacer la regresi\u00f3n deben tomarse los valores \\(X_i\\) de las observaciones ordenados de mayor a menor y formar la siguiente tabla: Valores categor\u00eda Probabilidad Logit \\(X_1\\) \\(\\epsilon_1\\) \\(\\pi(X_1)\\) \\(g(X_1)\\) \\(X_2\\) \\(\\epsilon_2\\) \\(\\pi(X_2)\\) \\(g(X_2)\\) \\(X_n\\) \\(\\epsilon_n\\) \\(\\pi(X_n)\\) \\(g(X_n)\\) Donde \\(\u03b5_i\\) es \"0\" o \"1\" seg\u00fan el caso y adem\u00e1s: \\[0 \\leq \u03c0(X_i) = \\dfrac{1}{i}\\sum_{k=1}^i \u03b5_k\\leq 1 \\, \\ g(X_i) = ln(\\dfrac{\\pi(X_i)}{1- \\pi(X_i)})=\\beta_0+\\beta_1 X_i \\] Error de Predicci\u00f3n Matriz de confusi\u00f3n Los modelos de clasificacion son ocupadas para predecir valores categ\u00f3ricos, por ejemplo, determinar la especie de una flor basado en el largo (y ancho) de su p\u00e9talo (y s\u00e9palo).Para este caso, es necesario introducir el concepto de matriz de confusi\u00f3n. La matriz de confusi\u00f3n es una herramienta que permite la visualizaci\u00f3n del desempe\u00f1o de un algoritmo Para la clasificaci\u00f3n de dos clases (por ejemplo, 0 y 1), se tiene la siguiente matriz de confusi\u00f3n: Ac\u00e1 se define: TP = Verdadero positivo : el modelo predijo la clase positiva correctamente, para ser una clase positiva. FP = Falso positivo : el modelo predijo la clase negativa incorrectamente, para ser una clase positiva. FN = Falso negativo : el modelo predijo incorrectamente que la clase positiva ser\u00eda la clase negativa. TN = Verdadero negativo : el modelo predijo la clase negativa correctamente, para ser la clase negativa. En este contexto, los valores TP Y TN muestran los valores correctos que tuve al momento de realizar la predicci\u00f3n, mientras que los valores de de FN Y FP denotan los valores que me equivoque de clase. Los conceptos de FN y FP se pueden interpretar con la siguiente imagen: M\u00e9tricas de error ( clasificaci\u00f3n ) En este contexto, se busca maximizar el n\u00famero al m\u00e1ximo la suma de los elementos TP Y TN, mientras que se busca disminuir la suma de los elementos de FN y FP. Para esto se definen las siguientes m\u00e9tricas: Accuracy \\[accuracy(y,\\hat{y}) = \\dfrac{TP+TN}{TP+TN+FP+FN}\\] Recall : \\[recall(y,\\hat{y}) = \\dfrac{TP}{TP+FN}\\] Precision : \\[precision(y,\\hat{y}) = \\dfrac{TP}{TP+FP} \\] F-score : \\[fscore(y,\\hat{y}) = 2\\times \\dfrac{precision(y,\\hat{y})\\times recall(y,\\hat{y})}{precision(y,\\hat{y})+recall(y,\\hat{y})} \\] Curva AUC\u2013ROC La curva AUC\u2013ROC es una representaci\u00f3n gr\u00e1fica de la sensibilidad frente a la especificidad para un sistema clasificador binario seg\u00fan se var\u00eda el umbral de discriminaci\u00f3n. Otra interpretaci\u00f3n de este gr\u00e1fico es la representaci\u00f3n de la raz\u00f3n o proporci\u00f3n de verdaderos positivos (VPR = Raz\u00f3n de Verdaderos Positivos) frente a la raz\u00f3n o proporci\u00f3n de falsos positivos (FPR = Raz\u00f3n de Falsos Positivos) tambi\u00e9n seg\u00fan se var\u00eda el umbral de discriminaci\u00f3n (valor a partir del cual decidimos que un caso es un positivo). ROC tambi\u00e9n puede significar Relative Operating Characteristic (Caracter\u00edstica Operativa Relativa) porque es una comparaci\u00f3n de dos caracter\u00edsticas operativas (VPR y FPR) seg\u00fan cambiamos el umbral para la decisi\u00f3n. En espa\u00f1ol es preferible mantener el acr\u00f3nimo ingl\u00e9s, aunque es posible encontrar el equivalente espa\u00f1ol COR. No se suele utilizar ROC aislado, debemos decir \u201ccurva ROC\u201d o \u201can\u00e1lisis ROC\u201d. El \u00e1rea cubierta por la curva es el \u00e1rea entre la l\u00ednea naranja (ROC) y el eje. Esta \u00e1rea cubierta es AUC. Cuanto m\u00e1s grande sea el \u00e1rea cubierta, mejores ser\u00e1n los modelos de aprendizaje autom\u00e1tico para distinguir las clases dadas. El valor ideal para AUC es 1. Ejemplo: Dataset Iris (regresi\u00f3n log\u00edstica) Veamos un peque\u00f1o ejemplo de como se implementa en python. En este ejemplo voy a utilizar el dataset Iris que ya viene junto con Scikit-learn y es ideal para practicar con regresiones log\u00edstica ; el mismo contiene los tipos de flores basado en en largo y ancho de su s\u00e9palo y p\u00e9talo. # librerias import os import numpy as np import pandas as pd from sklearn import datasets from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import seaborn as sns pd . set_option ( 'display.max_columns' , 500 ) # Ver m\u00e1s columnas de los dataframes # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab % matplotlib inline # cargar datos iris = datasets . load_iris () print ( iris . DESCR ) .. _iris_dataset: Iris plants dataset -------------------- **Data Set Characteristics:** :Number of Instances: 150 (50 in each of three classes) :Number of Attributes: 4 numeric, predictive attributes and the class :Attribute Information: - sepal length in cm - sepal width in cm - petal length in cm - petal width in cm - class: - Iris-Setosa - Iris-Versicolour - Iris-Virginica :Summary Statistics: ============== ==== ==== ======= ===== ==================== Min Max Mean SD Class Correlation ============== ==== ==== ======= ===== ==================== sepal length: 4.3 7.9 5.84 0.83 0.7826 sepal width: 2.0 4.4 3.05 0.43 -0.4194 petal length: 1.0 6.9 3.76 1.76 0.9490 (high!) petal width: 0.1 2.5 1.20 0.76 0.9565 (high!) ============== ==== ==== ======= ===== ==================== :Missing Attribute Values: None :Class Distribution: 33.3% for each of 3 classes. :Creator: R.A. Fisher :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov) :Date: July, 1988 The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher's paper. Note that it's the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points. This is perhaps the best known database to be found in the pattern recognition literature. Fisher's paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. .. topic:: References - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950). - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley & Sons. ISBN 0-471-22361-1. See page 218. - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments\". IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\". IEEE Transactions on Information Theory, May 1972, 431-433. - See also: 1988 MLC Proceedings, 54-64. Cheeseman et al\"s AUTOCLASS II conceptual clustering system finds 3 classes in the data. - Many, many more ... # dejar en formato dataframe iris_df = pd . DataFrame ( iris . data , columns = iris . feature_names ) iris_df [ 'TARGET' ] = iris . target iris_df . head () # estructura de nuestro dataset. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) TARGET 0 5.1 3.5 1.4 0.2 0 1 4.9 3.0 1.4 0.2 0 2 4.7 3.2 1.3 0.2 0 3 4.6 3.1 1.5 0.2 0 4 5.0 3.6 1.4 0.2 0 Para ver gr\u00e1ficamente el modelo de regresi\u00f3n log\u00edstica, ajustemos el modelo solo a dos variables: petal length (cm), petal width (cm). # datos from sklearn.linear_model import LogisticRegression X = iris_df [[ 'sepal length (cm)' , 'sepal width (cm)' ]] Y = iris_df [ 'TARGET' ] # split dataset X_train , X_test , Y_train , Y_test = train_test_split ( X , Y , test_size = 0.2 , random_state = 2 ) # print rows train and test sets print ( 'Separando informacion: \\n ' ) print ( 'numero de filas data original : ' , len ( X )) print ( 'numero de filas train set : ' , len ( X_train )) print ( 'numero de filas test set : ' , len ( X_test )) Separando informacion: numero de filas data original : 150 numero de filas train set : 120 numero de filas test set : 30 # Creando el modelo rlog = LogisticRegression () rlog . fit ( X_train , Y_train ) # ajustando el modelo LogisticRegression() rlog . score ( X_train , Y_train ) 0.825 rlog . predict_log_proba ( X_train ) array([[ -3.99669749, -0.66190745, -0.76409046], [ -0.21107893, -1.90464333, -3.18413364], [ -1.8065849 , -0.52153993, -1.41807287], [ -6.47991623, -3.03584962, -0.05083842], [ -1.05110824, -0.71678289, -1.81936211], [ -0.17004605, -2.74876413, -2.3819842 ], [ -4.68728813, -0.80298536, -0.61101654], [ -2.96770066, -0.71008075, -0.78312858], [ -3.63294174, -0.2154118 , -1.78765395], [ -5.51537947, -1.46091555, -0.26925051], [ -2.98670271, -0.60263228, -0.91086157], [ -3.64615407, -0.71257092, -0.72664838], [ -4.79873937, -1.4202387 , -0.28754402], [ -2.24998139, -0.20623326, -2.51385499], [ -0.06253829, -2.98157867, -4.61419118], [ -4.37794957, -0.51550211, -0.94097214], [ -0.19806777, -2.0180827 , -3.06239155], [ -4.75300091, -1.24804972, -0.35053651], [ -0.10414361, -2.60012179, -3.703401 ], [ -0.1399092 , -2.36113102, -3.31733432], [ -0.33299812, -1.72995085, -2.24492641], [ -1.36052475, -0.39394677, -2.67243331], [ -2.00164029, -1.07409158, -0.64764129], [ -7.3103791 , -3.31679977, -0.03763674], [ -0.34589815, -1.61640035, -2.36655737], [ -4.75300091, -1.24804972, -0.35053651], [ -2.32561455, -1.12330948, -0.54978327], [ -8.62419813, -2.24523278, -0.11214189], [ -5.53049122, -0.33797257, -1.26294071], [ -0.06998861, -2.75798325, -5.47839753], [ -0.18803569, -1.9034554 , -3.80038456], [ -3.06265058, -0.4256791 , -1.20427132], [ -2.4142983 , -0.45329007, -1.29085265], [ -1.43446384, -0.44603054, -2.10707824], [ -9.03137722, -2.37765551, -0.09748871], [ -4.69174057, -0.68098728, -0.72419992], [ -3.66230617, -0.60227249, -0.85153142], [ -0.05027585, -3.34866781, -4.27573592], [ -2.4142983 , -0.45329007, -1.29085265], [ -0.06340959, -3.33994623, -3.6495755 ], [ -2.71090648, -0.4751419 , -1.16562856], [ -3.422256 , -0.38407763, -1.2507754 ], [ -1.82139702, -0.30530631, -2.28964102], [ -0.23342127, -2.28448194, -2.24098902], [ -3.25980162, -0.24347857, -1.72761517], [ -2.33611822, -0.62801104, -0.99521069], [ -4.75300091, -1.24804972, -0.35053651], [ -9.67524275, -2.21910725, -0.11515155], [ -0.34353786, -1.38299432, -3.22095311], [ -2.6423909 , -1.03887894, -0.55345829], [ -0.03480928, -3.73440813, -4.57337069], [ -3.06265058, -0.4256791 , -1.20427132], [ -5.18402425, -1.53076722, -0.25099663], [ -0.24523474, -1.68589809, -3.43575133], [ -3.35645967, -0.10909088, -2.68102926], [ -0.24523474, -1.68589809, -3.43575133], [ -0.07746962, -2.84820416, -4.09855929], [ -0.12756408, -2.60168695, -3.08752732], [ -5.46678423, -1.28586978, -0.32938621], [ -2.6423909 , -1.03887894, -0.55345829], [ -0.11158762, -2.48111528, -3.81957595], [ -0.01601167, -4.64447482, -5.07204482], [ -5.40710339, -0.59393632, -0.81336006], [-10.32214698, -2.06359771, -0.13585311], [ -5.53049122, -0.33797257, -1.26294071], [ -5.07086967, -2.1981711 , -0.12475057], [ -1.66675571, -0.29897084, -2.66556294], [ -3.66230617, -0.60227249, -0.85153142], [ -4.35319491, -0.99654955, -0.48129372], [ -0.10414361, -2.60012179, -3.703401 ], [ -8.46843914, -1.83657274, -0.17384478], [ -2.33611822, -0.62801104, -0.99521069], [ -5.03844127, -0.75293161, -0.64906835], [ -0.13370583, -2.14573723, -4.80718114], [ -2.13015961, -0.44390775, -1.42854624], [ -0.14884726, -2.24361855, -3.43500331], [ -0.15976538, -2.12808615, -3.55465238], [ -3.1182411 , -0.3548191 , -1.36859278], [ -5.51537947, -1.46091555, -0.26925051], [ -2.5357413 , -0.32183203, -1.62975754], [ -4.33853357, -0.72898717, -0.68409426], [ -5.57382666, -1.64581327, -0.21896676], [ -6.14300989, -1.15968166, -0.37940919], [ -3.67733948, -1.1231079 , -0.43164097], [ -5.51537947, -1.46091555, -0.26925051], [ -3.25980162, -0.24347857, -1.72761517], [ -1.50374348, -0.6199054 , -1.42833279], [ -2.64559993, -0.66273639, -0.88286013], [ -3.69170941, -0.5052252 , -0.98966559], [ -3.06265058, -0.4256791 , -1.20427132], [ -1.70787592, -0.80218252, -0.99317107], [ -0.03480928, -3.73440813, -4.57337069], [ -6.17079981, -0.42876844, -1.05958475], [ -1.50374348, -0.6199054 , -1.42833279], [ -3.18432235, -0.29444982, -1.54340497], [ -3.06265058, -0.4256791 , -1.20427132], [ -3.65400219, -0.97332008, -0.51703462], [ -5.05989815, -1.02728954, -0.45306334], [ -0.03094358, -4.23634452, -4.13458123], [ -0.09199006, -2.50522834, -5.0785667 ], [ -3.98888929, -0.78054977, -0.64755132], [ -3.31830082, -1.0388256 , -0.49443459], [ -7.74336497, -1.78746182, -0.18370422], [ -0.05357357, -3.225515 , -4.38776458], [ -6.62509295, -1.61990942, -0.2221981 ], [ -0.03965368, -3.48635147, -4.79567696], [ -4.35319491, -0.99654955, -0.48129372], [ -4.08715355, -1.38461613, -0.31089182], [ -0.34589815, -1.61640035, -2.36655737], [ -3.31177254, -0.65294575, -0.81409912], [ -0.07511489, -2.74125422, -4.84422965], [ -5.0899367 , -1.18377862, -0.37437096], [ -1.76295435, -0.6043599 , -1.26571138], [ -0.11158762, -2.48111528, -3.81957595], [ -4.35319491, -0.99654955, -0.48129372], [ -4.71854451, -1.08714279, -0.42481105], [ -0.07746962, -2.84820416, -4.09855929], [ -0.01626132, -4.2872282 , -6.03778143], [ -5.40710339, -0.59393632, -0.81336006], [ -0.03160108, -4.48990308, -3.91777686]]) Grafiquemos nuestro resultados: # dataframe a matriz X = X . values Y = Y . values # grafica de la regresion logistica plt . figure ( figsize = ( 12 , 4 )) x_min , x_max = X [:, 0 ] . min () - .5 , X [:, 0 ] . max () + .5 y_min , y_max = X [:, 1 ] . min () - .5 , X [:, 1 ] . max () + .5 h = .02 # step size in the mesh xx , yy = np . meshgrid ( np . arange ( x_min , x_max , h ), np . arange ( y_min , y_max , h )) Z = rlog . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) # Put the result into a color plot Z = Z . reshape ( xx . shape ) plt . figure ( 1 , figsize = ( 4 , 3 )) plt . pcolormesh ( xx , yy , Z , cmap = plt . cm . Paired , shading = 'auto' ) # Plot also the training points plt . scatter ( X [:, 0 ], X [:, 1 ], c = Y , edgecolors = 'k' , cmap = plt . cm . Paired ) plt . xlabel ( 'Sepal length' ) plt . ylabel ( 'Sepal width' ) plt . show () Gr\u00e1ficamente podemos decir que el modelo se ajusta bastante bien, puesto que las clasificaciones son adecuadas y el modelo no se confunde entre una clase y otra. Por otro lado, existe valores num\u00e9ricos que tambi\u00e9n nos pueden ayudar a convensernos de estos, que son las m\u00e9tricas que se habian definidos con anterioridad. Para ello, instanciaremos las distintas metricas del archivo metrics_classification.py y calcularemos sus distintos valores. # metrics from metrics_classification import * from sklearn.metrics import confusion_matrix y_true = list ( Y_test ) y_pred = list ( rlog . predict ( X_test )) print ( 'Valores: \\n ' ) print ( 'originales: ' , y_true ) print ( 'predicho: ' , y_pred ) Valores: originales: [0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 1, 1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 0, 0, 2, 0, 2] predicho: [0, 0, 1, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 0, 0, 1, 0, 2] print ( ' \\n Matriz de confusion: \\n ' ) print ( confusion_matrix ( y_true , y_pred )) Matriz de confusion: [[13 1 0] [ 0 4 4] [ 0 2 6]] # ejemplo df_temp = pd . DataFrame ( { 'y' : y_true , 'yhat' : y_pred } ) df_metrics = summary_metrics ( df_temp ) print ( \" \\n Metricas para los regresores : 'sepal length (cm)' y 'sepal width (cm)'\" ) print ( \"\" ) df_metrics Metricas para los regresores : 'sepal length (cm)' y 'sepal width (cm)' .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } accuracy recall precision fscore 0 0.7667 0.7262 0.7238 0.721 Basado en las m\u00e9tricas y en la gr\u00e1fica, podemos concluir que el ajuste realizado es bastante asertado. Ahora, calculamos la curva AUC-ROC para nuestro ejemplo. Cabe destacar que esta curva es efectiva solo para clasificaci\u00f3n binaria, por lo que para efectos pr\u00e1cticos convertiremos nuestro TARGET en binarios (0 \u00f3 1). Para efectos pr\u00e1cticos tranformaremos la clase objetivo (en este caso, la clase 0 ) a 1 , y el resto de las clases (clase 1 y 2 ) las dejaremos en la clase 0 . from sklearn.metrics import roc_curve from sklearn.metrics import roc_auc_score # graficar curva roc def plot_roc_curve ( fpr , tpr ): plt . figure ( figsize = ( 8 , 8 )) plt . plot ( fpr , tpr , color = 'orange' , label = 'ROC' ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], color = 'darkblue' , linestyle = '--' ) plt . xlabel ( 'False Positive Rate' ) plt . ylabel ( 'True Positive Rate' ) plt . title ( 'Receiver Operating Characteristic (ROC) Curve' ) plt . legend () plt . show () # separar clase 0 del resto X = iris_df [[ 'sepal length (cm)' , 'sepal width (cm)' ]] Y = iris_df [ 'TARGET' ] . apply ( lambda x : 1 if x == 2 else 0 ) model = LogisticRegression () # split dataset X_train , X_test , Y_train , Y_test = train_test_split ( X , Y , test_size = 0.3 , random_state = 2 ) # ajustar modelo model . fit ( X_train , Y_train ) LogisticRegression() # calcular score AUC probs = model . predict_proba ( X_test ) # predecir probabilidades para X_test probs_tp = probs [:, 1 ] # mantener solo las probabilidades de la clase positiva auc = roc_auc_score ( Y_test , probs_tp ) # calcular score AUC print ( 'AUC: %.2f ' % auc ) AUC: 0.93 # calcular curva ROC fpr , tpr , thresholds = roc_curve ( Y_test , probs_tp ) # obtener curva ROC plot_roc_curve ( fpr , tpr ) Varios modelos de clasificaci\u00f3n Existen varios modelos de clasificaci\u00f3n que podemos ir comparando unos con otros, dentro de los cuales estacamos los siguientes: Regresi\u00f3n Log\u00edstica Arboles de Decision Random Forest SVM Nos basaremos en un ejemplo de sklearn que muestra los resultados de aplicar estos cuatro modelos sobre tres conjunto de datos distintos ( make_moons , make_circles , make_classification ). Adem\u00e1s, se crea un rutina para comparar los resultados de las distintas m\u00e9tricas. Gr\u00e1ficos Similar al gr\u00e1fico aplicado al conjunto de datos Iris, aca se realiza el mismo ejercicio pero para tres conjunto de datos sobre los distintos modelos. from sklearn.datasets import make_moons , make_circles , make_classification from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from matplotlib.colors import ListedColormap h = .02 # step size in the mesh plt . figure ( figsize = ( 12 , 12 )) names = [ \"Logistic\" , \"RBF SVM\" , \"Decision Tree\" , \"Random Forest\" ] classifiers = [ LogisticRegression (), SVC ( gamma = 2 , C = 1 ), DecisionTreeClassifier ( max_depth = 5 ), RandomForestClassifier ( max_depth = 5 , n_estimators = 10 , max_features = 1 ), ] X , y = make_classification ( n_features = 2 , n_redundant = 0 , n_informative = 2 , random_state = 1 , n_clusters_per_class = 1 ) rng = np . random . RandomState ( 2 ) X += 2 * rng . uniform ( size = X . shape ) linearly_separable = ( X , y ) datasets = [ make_moons ( noise = 0.3 , random_state = 0 ), make_circles ( noise = 0.2 , factor = 0.5 , random_state = 1 ), linearly_separable ] figure = plt . figure ( figsize = ( 27 , 9 )) i = 1 # iterate over datasets for ds_cnt , ds in enumerate ( datasets ): # preprocess dataset, split into training and test part X , y = ds X = StandardScaler () . fit_transform ( X ) X_train , X_test , y_train , y_test = \\ train_test_split ( X , y , test_size = .4 , random_state = 42 ) x_min , x_max = X [:, 0 ] . min () - .5 , X [:, 0 ] . max () + .5 y_min , y_max = X [:, 1 ] . min () - .5 , X [:, 1 ] . max () + .5 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , h ), np . arange ( y_min , y_max , h )) # just plot the dataset first cm = plt . cm . RdBu cm_bright = ListedColormap ([ '#FF0000' , '#0000FF' ]) ax = plt . subplot ( len ( datasets ), len ( classifiers ) + 1 , i ) if ds_cnt == 0 : ax . set_title ( \"Input data\" ) # Plot the training points ax . scatter ( X_train [:, 0 ], X_train [:, 1 ], c = y_train , cmap = cm_bright , edgecolors = 'k' ) # Plot the testing points ax . scatter ( X_test [:, 0 ], X_test [:, 1 ], c = y_test , cmap = cm_bright , alpha = 0.6 , edgecolors = 'k' ) ax . set_xlim ( xx . min (), xx . max ()) ax . set_ylim ( yy . min (), yy . max ()) ax . set_xticks (()) ax . set_yticks (()) i += 1 # iterate over classifiers for name , clf in zip ( names , classifiers ): ax = plt . subplot ( len ( datasets ), len ( classifiers ) + 1 , i ) clf . fit ( X_train , y_train ) score = clf . score ( X_test , y_test ) # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, x_max]x[y_min, y_max]. if hasattr ( clf , \"decision_function\" ): Z = clf . decision_function ( np . c_ [ xx . ravel (), yy . ravel ()]) else : Z = clf . predict_proba ( np . c_ [ xx . ravel (), yy . ravel ()])[:, 1 ] # Put the result into a color plot Z = Z . reshape ( xx . shape ) ax . contourf ( xx , yy , Z , cmap = cm , alpha = .8 ) # Plot the training points ax . scatter ( X_train [:, 0 ], X_train [:, 1 ], c = y_train , cmap = cm_bright , edgecolors = 'k' ) # Plot the testing points ax . scatter ( X_test [:, 0 ], X_test [:, 1 ], c = y_test , cmap = cm_bright , edgecolors = 'k' , alpha = 0.6 ) ax . set_xlim ( xx . min (), xx . max ()) ax . set_ylim ( yy . min (), yy . max ()) ax . set_xticks (()) ax . set_yticks (()) if ds_cnt == 0 : ax . set_title ( name ) ax . text ( xx . max () - .3 , yy . min () + .3 , ( ' %.2f ' % score ) . lstrip ( '0' ), size = 15 , horizontalalignment = 'right' ) i += 1 plt . tight_layout () plt . show () <Figure size 864x864 with 0 Axes> M\u00e9tricas Dado que el sistema de calcular m\u00e9tricas sigue el mismo formato, solo cambiando el conjunto de datos y el modelo, se decide realizar una clase que automatice este proceso. from metrics_classification import * class SklearnClassificationModels : def __init__ ( self , model , name_model ): self . model = model self . name_model = name_model @staticmethod def test_train_model ( X , y , n_size ): X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = n_size , random_state = 42 ) return X_train , X_test , y_train , y_test def fit_model ( self , X , y , test_size ): X_train , X_test , y_train , y_test = self . test_train_model ( X , y , test_size ) return self . model . fit ( X_train , y_train ) def df_testig ( self , X , y , test_size ): X_train , X_test , y_train , y_test = self . test_train_model ( X , y , test_size ) model_fit = self . model . fit ( X_train , y_train ) preds = model_fit . predict ( X_test ) df_temp = pd . DataFrame ( { 'y' : y_test , 'yhat' : model_fit . predict ( X_test ) } ) return df_temp def metrics ( self , X , y , test_size ): df_temp = self . df_testig ( X , y , test_size ) df_metrics = summary_metrics ( df_temp ) df_metrics [ 'model' ] = self . name_model return df_metrics # metrics import itertools # nombre modelos names_models = [ \"Logistic\" , \"RBF SVM\" , \"Decision Tree\" , \"Random Forest\" ] # modelos classifiers = [ LogisticRegression (), SVC ( gamma = 2 , C = 1 ), DecisionTreeClassifier ( max_depth = 5 ), RandomForestClassifier ( max_depth = 5 , n_estimators = 10 , max_features = 1 ), ] # datasets names_dataset = [ 'make_moons' , 'make_circles' , 'linearly_separable' ] X , y = make_classification ( n_features = 2 , n_redundant = 0 , n_informative = 2 , random_state = 1 , n_clusters_per_class = 1 ) rng = np . random . RandomState ( 2 ) X += 2 * rng . uniform ( size = X . shape ) linearly_separable = ( X , y ) datasets = [ make_moons ( noise = 0.3 , random_state = 0 ), make_circles ( noise = 0.2 , factor = 0.5 , random_state = 1 ), linearly_separable ] # juntar informacion list_models = list ( zip ( names_models , classifiers )) list_dataset = list ( zip ( names_dataset , datasets )) frames = [] for x in itertools . product ( list_models , list_dataset ): name_model = x [ 0 ][ 0 ] classifier = x [ 0 ][ 1 ] name_dataset = x [ 1 ][ 0 ] dataset = x [ 1 ][ 1 ] X = dataset [ 0 ] Y = dataset [ 1 ] fit_model = SklearnClassificationModels ( classifier , name_model ) df = fit_model . metrics ( X , Y , 0.2 ) df [ 'dataset' ] = name_dataset frames . append ( df ) /home/fralfaro/.cache/pypoetry/virtualenvs/mat281-2021-kaeOORRv-py3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) # juntar resultados pd . concat ( frames ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } accuracy recall precision fscore model dataset 0 0.90 0.9000 0.9000 0.9000 Logistic make_moons 0 0.35 0.5000 0.1750 0.2593 Logistic make_circles 0 0.95 0.9545 0.9500 0.9499 Logistic linearly_separable 0 0.95 0.9500 0.9545 0.9499 RBF SVM make_moons 0 0.80 0.8462 0.8182 0.7980 RBF SVM make_circles 0 0.95 0.9545 0.9500 0.9499 RBF SVM linearly_separable 0 0.95 0.9500 0.9545 0.9499 Decision Tree make_moons 0 0.75 0.8077 0.7917 0.7494 Decision Tree make_circles 0 0.90 0.8990 0.8990 0.8990 Decision Tree linearly_separable 0 0.95 0.9500 0.9545 0.9499 Random Forest make_moons 0 0.70 0.7363 0.7172 0.6970 Random Forest make_circles 0 0.90 0.9091 0.9091 0.9000 Random Forest linearly_separable Referencia Supervised learning","title":"Clasificaci\u00f3n"},{"location":"lectures/ml/analisis_supervisado_clasificacion/02_clasificacion/#modelo-de-regresion-logistica","text":"","title":"Modelo de regresi\u00f3n log\u00edstica"},{"location":"lectures/ml/analisis_supervisado_clasificacion/02_clasificacion/#definicion","text":"A modo de recuerdo, el modelo de regresio\u0301n lineal general supone que, \\(\\boldsymbol{Y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon},\\) donde: \\(\\boldsymbol{X} = (x_1,...,x_n)^{T}\\) : variable explicativa \\(\\boldsymbol{Y} = (y_1,...,y_n)^{T}\\) : variable respuesta \\(\\boldsymbol{\\epsilon} = (\\epsilon_1,...,\\epsilon_n)^{T}\\) : error se asume un ruido blanco, es decir, \\(\\epsilon \\sim \\mathcal{N}( \\boldsymbol{0},\\sigma^2I)\\) \\(\\boldsymbol{\\beta} = (\\beta_1,...,\\beta_n)^{T}\\) : coeficientes de regresio\u0301n. Por otro lado, el modelo de regresi\u00f3n log\u00edstica analiza datos distribuidos binomialmente de la forma: \\(Y_i \\sim B(p_i,n_i)\\) , para \\(i=1,...,m\\) donde los n\u00fameros de ensayos Bernoulli \\(n_{i}\\) son conocidos y las probabilidades de \u00e9xito \\(p_{i}\\) son desconocidas. Un ejemplo de esta distribuci\u00f3n es el porcentaje de semillas $p_{i} $ que germinan despu\u00e9s de que \\(n_{i}\\) son plantadas. El modelo es entonces obtenido a base de lo que cada ensayo (valor de \\(i\\) ) y el conjunto de variables explicativas/independientes puedan informar acerca de la probabilidad final. Estas variables explicativas pueden pensarse como un vector \\(X_i\\) k-dimensional y el modelo toma entonces la forma: \\[p_i=\\mathbb{E}(\\dfrac{Y_i}{n_i}|X_i)\\] Los logits de las probabilidades binomiales desconocidas (i.e., los logaritmos de la raz\u00f3n de momios) son modeladas como una funci\u00f3n lineal de los \\(X_i\\) : \\[logit(p_i) = ln(\\dfrac{p_i}{1-p_i}) = \\beta_0+\\beta_1x_{1,i} +....+\\beta_kx_{k,i}\\] Note que un elemento particular de \\(X_i\\) puede ser ajustado a 1 para todo \\(i\\) obteni\u00e9ndose una constante independiente en el modelo. Los par\u00e1metros desconocidos \\(\\beta _{j}\\) son usualmente estimados a trav\u00e9s de m\u00e1xima verosimilitud. La interpretaci\u00f3n de los estimados del par\u00e1metro \\(\\beta _{j}\\) es como los efectos aditivos en el logaritmo de la raz\u00f3n de momios para una unidad de cambio en la j\u00e9sima variable explicativa. En el caso de una variable explicativa dicot\u00f3mica, por ejemplo g\u00e9nero, \\(e^{\\beta}\\) es la estimaci\u00f3n de la raz\u00f3n de momios (odds ratio) de tener el resultado para, por decir algo, hombres comparados con mujeres. El modelo tiene una formulaci\u00f3n equivalente dada por: \\[p_i=\\dfrac{1}{e^{-(\\beta_0+\\beta_1x_{1,i} +....+\\beta_kx_{k,i})}+1}\\]","title":"Definici\u00f3n"},{"location":"lectures/ml/analisis_supervisado_clasificacion/02_clasificacion/#interpretacion-para-el-caso-binario","text":"La idea es que la regresi\u00f3n log\u00edstica aproxime la probabilidad de obtener 0 (no ocurre cierto suceso) o 1 (ocurre el suceso) con el valor de la variable explicativa \\(x\\) . En esas condiciones, la probabilidad aproximada del suceso se aproximar\u00e1 mediante una funci\u00f3n log\u00edstica del tipo: \\[\\pi(x) =\\dfrac{e^{\\beta_0+\\beta_1x}}{e^{\\beta_0+\\beta_1x}+1}=\\dfrac{1}{e^{-(\\beta_0+\\beta_1x)}+1}\\] que puede reducirse al c\u00e1lculo de una regresi\u00f3n lineal para la funci\u00f3n logit de la probabilidad: \\[g(x) = ln(\\dfrac{\\pi(x)}{1- \\pi(x)})=\\beta_0+\\beta_1 x\\] El gr\u00e1fico de la funci\u00f3n log\u00edstica se muestra en la figura que encabeza esta secci\u00f3n, la variable independiente es la combinaci\u00f3n lineal \\(=\\beta_0+\\beta_1\\) y la variable dependiente es la probabilidad estimada $ \\pi (x)$. Si se realiza la regresi\u00f3n lineal, la forma de la probabilidad estimada puede ser f\u00e1cilmente recuperada a partir de los coeficientes calculados. Para hacer la regresi\u00f3n deben tomarse los valores \\(X_i\\) de las observaciones ordenados de mayor a menor y formar la siguiente tabla: Valores categor\u00eda Probabilidad Logit \\(X_1\\) \\(\\epsilon_1\\) \\(\\pi(X_1)\\) \\(g(X_1)\\) \\(X_2\\) \\(\\epsilon_2\\) \\(\\pi(X_2)\\) \\(g(X_2)\\) \\(X_n\\) \\(\\epsilon_n\\) \\(\\pi(X_n)\\) \\(g(X_n)\\) Donde \\(\u03b5_i\\) es \"0\" o \"1\" seg\u00fan el caso y adem\u00e1s: \\[0 \\leq \u03c0(X_i) = \\dfrac{1}{i}\\sum_{k=1}^i \u03b5_k\\leq 1 \\, \\ g(X_i) = ln(\\dfrac{\\pi(X_i)}{1- \\pi(X_i)})=\\beta_0+\\beta_1 X_i \\]","title":"Interpretaci\u00f3n para el caso binario"},{"location":"lectures/ml/analisis_supervisado_clasificacion/02_clasificacion/#error-de-prediccion","text":"","title":"Error de Predicci\u00f3n"},{"location":"lectures/ml/analisis_supervisado_clasificacion/02_clasificacion/#matriz-de-confusion","text":"Los modelos de clasificacion son ocupadas para predecir valores categ\u00f3ricos, por ejemplo, determinar la especie de una flor basado en el largo (y ancho) de su p\u00e9talo (y s\u00e9palo).Para este caso, es necesario introducir el concepto de matriz de confusi\u00f3n. La matriz de confusi\u00f3n es una herramienta que permite la visualizaci\u00f3n del desempe\u00f1o de un algoritmo Para la clasificaci\u00f3n de dos clases (por ejemplo, 0 y 1), se tiene la siguiente matriz de confusi\u00f3n: Ac\u00e1 se define: TP = Verdadero positivo : el modelo predijo la clase positiva correctamente, para ser una clase positiva. FP = Falso positivo : el modelo predijo la clase negativa incorrectamente, para ser una clase positiva. FN = Falso negativo : el modelo predijo incorrectamente que la clase positiva ser\u00eda la clase negativa. TN = Verdadero negativo : el modelo predijo la clase negativa correctamente, para ser la clase negativa. En este contexto, los valores TP Y TN muestran los valores correctos que tuve al momento de realizar la predicci\u00f3n, mientras que los valores de de FN Y FP denotan los valores que me equivoque de clase. Los conceptos de FN y FP se pueden interpretar con la siguiente imagen:","title":"Matriz de confusi\u00f3n"},{"location":"lectures/ml/analisis_supervisado_clasificacion/02_clasificacion/#metricas-de-error-clasificacion","text":"En este contexto, se busca maximizar el n\u00famero al m\u00e1ximo la suma de los elementos TP Y TN, mientras que se busca disminuir la suma de los elementos de FN y FP. Para esto se definen las siguientes m\u00e9tricas: Accuracy \\[accuracy(y,\\hat{y}) = \\dfrac{TP+TN}{TP+TN+FP+FN}\\] Recall : \\[recall(y,\\hat{y}) = \\dfrac{TP}{TP+FN}\\] Precision : \\[precision(y,\\hat{y}) = \\dfrac{TP}{TP+FP} \\] F-score : \\[fscore(y,\\hat{y}) = 2\\times \\dfrac{precision(y,\\hat{y})\\times recall(y,\\hat{y})}{precision(y,\\hat{y})+recall(y,\\hat{y})} \\]","title":"M\u00e9tricas de error  ( clasificaci\u00f3n )"},{"location":"lectures/ml/analisis_supervisado_clasificacion/02_clasificacion/#curva-aucroc","text":"La curva AUC\u2013ROC es una representaci\u00f3n gr\u00e1fica de la sensibilidad frente a la especificidad para un sistema clasificador binario seg\u00fan se var\u00eda el umbral de discriminaci\u00f3n. Otra interpretaci\u00f3n de este gr\u00e1fico es la representaci\u00f3n de la raz\u00f3n o proporci\u00f3n de verdaderos positivos (VPR = Raz\u00f3n de Verdaderos Positivos) frente a la raz\u00f3n o proporci\u00f3n de falsos positivos (FPR = Raz\u00f3n de Falsos Positivos) tambi\u00e9n seg\u00fan se var\u00eda el umbral de discriminaci\u00f3n (valor a partir del cual decidimos que un caso es un positivo). ROC tambi\u00e9n puede significar Relative Operating Characteristic (Caracter\u00edstica Operativa Relativa) porque es una comparaci\u00f3n de dos caracter\u00edsticas operativas (VPR y FPR) seg\u00fan cambiamos el umbral para la decisi\u00f3n. En espa\u00f1ol es preferible mantener el acr\u00f3nimo ingl\u00e9s, aunque es posible encontrar el equivalente espa\u00f1ol COR. No se suele utilizar ROC aislado, debemos decir \u201ccurva ROC\u201d o \u201can\u00e1lisis ROC\u201d. El \u00e1rea cubierta por la curva es el \u00e1rea entre la l\u00ednea naranja (ROC) y el eje. Esta \u00e1rea cubierta es AUC. Cuanto m\u00e1s grande sea el \u00e1rea cubierta, mejores ser\u00e1n los modelos de aprendizaje autom\u00e1tico para distinguir las clases dadas. El valor ideal para AUC es 1.","title":"Curva  AUC\u2013ROC"},{"location":"lectures/ml/analisis_supervisado_clasificacion/02_clasificacion/#ejemplo-dataset-iris-regresion-logistica","text":"Veamos un peque\u00f1o ejemplo de como se implementa en python. En este ejemplo voy a utilizar el dataset Iris que ya viene junto con Scikit-learn y es ideal para practicar con regresiones log\u00edstica ; el mismo contiene los tipos de flores basado en en largo y ancho de su s\u00e9palo y p\u00e9talo. # librerias import os import numpy as np import pandas as pd from sklearn import datasets from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import seaborn as sns pd . set_option ( 'display.max_columns' , 500 ) # Ver m\u00e1s columnas de los dataframes # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab % matplotlib inline # cargar datos iris = datasets . load_iris () print ( iris . DESCR ) .. _iris_dataset: Iris plants dataset -------------------- **Data Set Characteristics:** :Number of Instances: 150 (50 in each of three classes) :Number of Attributes: 4 numeric, predictive attributes and the class :Attribute Information: - sepal length in cm - sepal width in cm - petal length in cm - petal width in cm - class: - Iris-Setosa - Iris-Versicolour - Iris-Virginica :Summary Statistics: ============== ==== ==== ======= ===== ==================== Min Max Mean SD Class Correlation ============== ==== ==== ======= ===== ==================== sepal length: 4.3 7.9 5.84 0.83 0.7826 sepal width: 2.0 4.4 3.05 0.43 -0.4194 petal length: 1.0 6.9 3.76 1.76 0.9490 (high!) petal width: 0.1 2.5 1.20 0.76 0.9565 (high!) ============== ==== ==== ======= ===== ==================== :Missing Attribute Values: None :Class Distribution: 33.3% for each of 3 classes. :Creator: R.A. Fisher :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov) :Date: July, 1988 The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher's paper. Note that it's the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points. This is perhaps the best known database to be found in the pattern recognition literature. Fisher's paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. .. topic:: References - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950). - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley & Sons. ISBN 0-471-22361-1. See page 218. - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments\". IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\". IEEE Transactions on Information Theory, May 1972, 431-433. - See also: 1988 MLC Proceedings, 54-64. Cheeseman et al\"s AUTOCLASS II conceptual clustering system finds 3 classes in the data. - Many, many more ... # dejar en formato dataframe iris_df = pd . DataFrame ( iris . data , columns = iris . feature_names ) iris_df [ 'TARGET' ] = iris . target iris_df . head () # estructura de nuestro dataset. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) TARGET 0 5.1 3.5 1.4 0.2 0 1 4.9 3.0 1.4 0.2 0 2 4.7 3.2 1.3 0.2 0 3 4.6 3.1 1.5 0.2 0 4 5.0 3.6 1.4 0.2 0 Para ver gr\u00e1ficamente el modelo de regresi\u00f3n log\u00edstica, ajustemos el modelo solo a dos variables: petal length (cm), petal width (cm). # datos from sklearn.linear_model import LogisticRegression X = iris_df [[ 'sepal length (cm)' , 'sepal width (cm)' ]] Y = iris_df [ 'TARGET' ] # split dataset X_train , X_test , Y_train , Y_test = train_test_split ( X , Y , test_size = 0.2 , random_state = 2 ) # print rows train and test sets print ( 'Separando informacion: \\n ' ) print ( 'numero de filas data original : ' , len ( X )) print ( 'numero de filas train set : ' , len ( X_train )) print ( 'numero de filas test set : ' , len ( X_test )) Separando informacion: numero de filas data original : 150 numero de filas train set : 120 numero de filas test set : 30 # Creando el modelo rlog = LogisticRegression () rlog . fit ( X_train , Y_train ) # ajustando el modelo LogisticRegression() rlog . score ( X_train , Y_train ) 0.825 rlog . predict_log_proba ( X_train ) array([[ -3.99669749, -0.66190745, -0.76409046], [ -0.21107893, -1.90464333, -3.18413364], [ -1.8065849 , -0.52153993, -1.41807287], [ -6.47991623, -3.03584962, -0.05083842], [ -1.05110824, -0.71678289, -1.81936211], [ -0.17004605, -2.74876413, -2.3819842 ], [ -4.68728813, -0.80298536, -0.61101654], [ -2.96770066, -0.71008075, -0.78312858], [ -3.63294174, -0.2154118 , -1.78765395], [ -5.51537947, -1.46091555, -0.26925051], [ -2.98670271, -0.60263228, -0.91086157], [ -3.64615407, -0.71257092, -0.72664838], [ -4.79873937, -1.4202387 , -0.28754402], [ -2.24998139, -0.20623326, -2.51385499], [ -0.06253829, -2.98157867, -4.61419118], [ -4.37794957, -0.51550211, -0.94097214], [ -0.19806777, -2.0180827 , -3.06239155], [ -4.75300091, -1.24804972, -0.35053651], [ -0.10414361, -2.60012179, -3.703401 ], [ -0.1399092 , -2.36113102, -3.31733432], [ -0.33299812, -1.72995085, -2.24492641], [ -1.36052475, -0.39394677, -2.67243331], [ -2.00164029, -1.07409158, -0.64764129], [ -7.3103791 , -3.31679977, -0.03763674], [ -0.34589815, -1.61640035, -2.36655737], [ -4.75300091, -1.24804972, -0.35053651], [ -2.32561455, -1.12330948, -0.54978327], [ -8.62419813, -2.24523278, -0.11214189], [ -5.53049122, -0.33797257, -1.26294071], [ -0.06998861, -2.75798325, -5.47839753], [ -0.18803569, -1.9034554 , -3.80038456], [ -3.06265058, -0.4256791 , -1.20427132], [ -2.4142983 , -0.45329007, -1.29085265], [ -1.43446384, -0.44603054, -2.10707824], [ -9.03137722, -2.37765551, -0.09748871], [ -4.69174057, -0.68098728, -0.72419992], [ -3.66230617, -0.60227249, -0.85153142], [ -0.05027585, -3.34866781, -4.27573592], [ -2.4142983 , -0.45329007, -1.29085265], [ -0.06340959, -3.33994623, -3.6495755 ], [ -2.71090648, -0.4751419 , -1.16562856], [ -3.422256 , -0.38407763, -1.2507754 ], [ -1.82139702, -0.30530631, -2.28964102], [ -0.23342127, -2.28448194, -2.24098902], [ -3.25980162, -0.24347857, -1.72761517], [ -2.33611822, -0.62801104, -0.99521069], [ -4.75300091, -1.24804972, -0.35053651], [ -9.67524275, -2.21910725, -0.11515155], [ -0.34353786, -1.38299432, -3.22095311], [ -2.6423909 , -1.03887894, -0.55345829], [ -0.03480928, -3.73440813, -4.57337069], [ -3.06265058, -0.4256791 , -1.20427132], [ -5.18402425, -1.53076722, -0.25099663], [ -0.24523474, -1.68589809, -3.43575133], [ -3.35645967, -0.10909088, -2.68102926], [ -0.24523474, -1.68589809, -3.43575133], [ -0.07746962, -2.84820416, -4.09855929], [ -0.12756408, -2.60168695, -3.08752732], [ -5.46678423, -1.28586978, -0.32938621], [ -2.6423909 , -1.03887894, -0.55345829], [ -0.11158762, -2.48111528, -3.81957595], [ -0.01601167, -4.64447482, -5.07204482], [ -5.40710339, -0.59393632, -0.81336006], [-10.32214698, -2.06359771, -0.13585311], [ -5.53049122, -0.33797257, -1.26294071], [ -5.07086967, -2.1981711 , -0.12475057], [ -1.66675571, -0.29897084, -2.66556294], [ -3.66230617, -0.60227249, -0.85153142], [ -4.35319491, -0.99654955, -0.48129372], [ -0.10414361, -2.60012179, -3.703401 ], [ -8.46843914, -1.83657274, -0.17384478], [ -2.33611822, -0.62801104, -0.99521069], [ -5.03844127, -0.75293161, -0.64906835], [ -0.13370583, -2.14573723, -4.80718114], [ -2.13015961, -0.44390775, -1.42854624], [ -0.14884726, -2.24361855, -3.43500331], [ -0.15976538, -2.12808615, -3.55465238], [ -3.1182411 , -0.3548191 , -1.36859278], [ -5.51537947, -1.46091555, -0.26925051], [ -2.5357413 , -0.32183203, -1.62975754], [ -4.33853357, -0.72898717, -0.68409426], [ -5.57382666, -1.64581327, -0.21896676], [ -6.14300989, -1.15968166, -0.37940919], [ -3.67733948, -1.1231079 , -0.43164097], [ -5.51537947, -1.46091555, -0.26925051], [ -3.25980162, -0.24347857, -1.72761517], [ -1.50374348, -0.6199054 , -1.42833279], [ -2.64559993, -0.66273639, -0.88286013], [ -3.69170941, -0.5052252 , -0.98966559], [ -3.06265058, -0.4256791 , -1.20427132], [ -1.70787592, -0.80218252, -0.99317107], [ -0.03480928, -3.73440813, -4.57337069], [ -6.17079981, -0.42876844, -1.05958475], [ -1.50374348, -0.6199054 , -1.42833279], [ -3.18432235, -0.29444982, -1.54340497], [ -3.06265058, -0.4256791 , -1.20427132], [ -3.65400219, -0.97332008, -0.51703462], [ -5.05989815, -1.02728954, -0.45306334], [ -0.03094358, -4.23634452, -4.13458123], [ -0.09199006, -2.50522834, -5.0785667 ], [ -3.98888929, -0.78054977, -0.64755132], [ -3.31830082, -1.0388256 , -0.49443459], [ -7.74336497, -1.78746182, -0.18370422], [ -0.05357357, -3.225515 , -4.38776458], [ -6.62509295, -1.61990942, -0.2221981 ], [ -0.03965368, -3.48635147, -4.79567696], [ -4.35319491, -0.99654955, -0.48129372], [ -4.08715355, -1.38461613, -0.31089182], [ -0.34589815, -1.61640035, -2.36655737], [ -3.31177254, -0.65294575, -0.81409912], [ -0.07511489, -2.74125422, -4.84422965], [ -5.0899367 , -1.18377862, -0.37437096], [ -1.76295435, -0.6043599 , -1.26571138], [ -0.11158762, -2.48111528, -3.81957595], [ -4.35319491, -0.99654955, -0.48129372], [ -4.71854451, -1.08714279, -0.42481105], [ -0.07746962, -2.84820416, -4.09855929], [ -0.01626132, -4.2872282 , -6.03778143], [ -5.40710339, -0.59393632, -0.81336006], [ -0.03160108, -4.48990308, -3.91777686]]) Grafiquemos nuestro resultados: # dataframe a matriz X = X . values Y = Y . values # grafica de la regresion logistica plt . figure ( figsize = ( 12 , 4 )) x_min , x_max = X [:, 0 ] . min () - .5 , X [:, 0 ] . max () + .5 y_min , y_max = X [:, 1 ] . min () - .5 , X [:, 1 ] . max () + .5 h = .02 # step size in the mesh xx , yy = np . meshgrid ( np . arange ( x_min , x_max , h ), np . arange ( y_min , y_max , h )) Z = rlog . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) # Put the result into a color plot Z = Z . reshape ( xx . shape ) plt . figure ( 1 , figsize = ( 4 , 3 )) plt . pcolormesh ( xx , yy , Z , cmap = plt . cm . Paired , shading = 'auto' ) # Plot also the training points plt . scatter ( X [:, 0 ], X [:, 1 ], c = Y , edgecolors = 'k' , cmap = plt . cm . Paired ) plt . xlabel ( 'Sepal length' ) plt . ylabel ( 'Sepal width' ) plt . show () Gr\u00e1ficamente podemos decir que el modelo se ajusta bastante bien, puesto que las clasificaciones son adecuadas y el modelo no se confunde entre una clase y otra. Por otro lado, existe valores num\u00e9ricos que tambi\u00e9n nos pueden ayudar a convensernos de estos, que son las m\u00e9tricas que se habian definidos con anterioridad. Para ello, instanciaremos las distintas metricas del archivo metrics_classification.py y calcularemos sus distintos valores. # metrics from metrics_classification import * from sklearn.metrics import confusion_matrix y_true = list ( Y_test ) y_pred = list ( rlog . predict ( X_test )) print ( 'Valores: \\n ' ) print ( 'originales: ' , y_true ) print ( 'predicho: ' , y_pred ) Valores: originales: [0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 1, 1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 0, 0, 2, 0, 2] predicho: [0, 0, 1, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 0, 0, 1, 0, 2] print ( ' \\n Matriz de confusion: \\n ' ) print ( confusion_matrix ( y_true , y_pred )) Matriz de confusion: [[13 1 0] [ 0 4 4] [ 0 2 6]] # ejemplo df_temp = pd . DataFrame ( { 'y' : y_true , 'yhat' : y_pred } ) df_metrics = summary_metrics ( df_temp ) print ( \" \\n Metricas para los regresores : 'sepal length (cm)' y 'sepal width (cm)'\" ) print ( \"\" ) df_metrics Metricas para los regresores : 'sepal length (cm)' y 'sepal width (cm)' .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } accuracy recall precision fscore 0 0.7667 0.7262 0.7238 0.721 Basado en las m\u00e9tricas y en la gr\u00e1fica, podemos concluir que el ajuste realizado es bastante asertado. Ahora, calculamos la curva AUC-ROC para nuestro ejemplo. Cabe destacar que esta curva es efectiva solo para clasificaci\u00f3n binaria, por lo que para efectos pr\u00e1cticos convertiremos nuestro TARGET en binarios (0 \u00f3 1). Para efectos pr\u00e1cticos tranformaremos la clase objetivo (en este caso, la clase 0 ) a 1 , y el resto de las clases (clase 1 y 2 ) las dejaremos en la clase 0 . from sklearn.metrics import roc_curve from sklearn.metrics import roc_auc_score # graficar curva roc def plot_roc_curve ( fpr , tpr ): plt . figure ( figsize = ( 8 , 8 )) plt . plot ( fpr , tpr , color = 'orange' , label = 'ROC' ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], color = 'darkblue' , linestyle = '--' ) plt . xlabel ( 'False Positive Rate' ) plt . ylabel ( 'True Positive Rate' ) plt . title ( 'Receiver Operating Characteristic (ROC) Curve' ) plt . legend () plt . show () # separar clase 0 del resto X = iris_df [[ 'sepal length (cm)' , 'sepal width (cm)' ]] Y = iris_df [ 'TARGET' ] . apply ( lambda x : 1 if x == 2 else 0 ) model = LogisticRegression () # split dataset X_train , X_test , Y_train , Y_test = train_test_split ( X , Y , test_size = 0.3 , random_state = 2 ) # ajustar modelo model . fit ( X_train , Y_train ) LogisticRegression() # calcular score AUC probs = model . predict_proba ( X_test ) # predecir probabilidades para X_test probs_tp = probs [:, 1 ] # mantener solo las probabilidades de la clase positiva auc = roc_auc_score ( Y_test , probs_tp ) # calcular score AUC print ( 'AUC: %.2f ' % auc ) AUC: 0.93 # calcular curva ROC fpr , tpr , thresholds = roc_curve ( Y_test , probs_tp ) # obtener curva ROC plot_roc_curve ( fpr , tpr )","title":"Ejemplo: Dataset Iris (regresi\u00f3n log\u00edstica)"},{"location":"lectures/ml/analisis_supervisado_clasificacion/02_clasificacion/#varios-modelos-de-clasificacion","text":"Existen varios modelos de clasificaci\u00f3n que podemos ir comparando unos con otros, dentro de los cuales estacamos los siguientes: Regresi\u00f3n Log\u00edstica Arboles de Decision Random Forest SVM Nos basaremos en un ejemplo de sklearn que muestra los resultados de aplicar estos cuatro modelos sobre tres conjunto de datos distintos ( make_moons , make_circles , make_classification ). Adem\u00e1s, se crea un rutina para comparar los resultados de las distintas m\u00e9tricas.","title":"Varios modelos de clasificaci\u00f3n"},{"location":"lectures/ml/analisis_supervisado_clasificacion/02_clasificacion/#graficos","text":"Similar al gr\u00e1fico aplicado al conjunto de datos Iris, aca se realiza el mismo ejercicio pero para tres conjunto de datos sobre los distintos modelos. from sklearn.datasets import make_moons , make_circles , make_classification from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from matplotlib.colors import ListedColormap h = .02 # step size in the mesh plt . figure ( figsize = ( 12 , 12 )) names = [ \"Logistic\" , \"RBF SVM\" , \"Decision Tree\" , \"Random Forest\" ] classifiers = [ LogisticRegression (), SVC ( gamma = 2 , C = 1 ), DecisionTreeClassifier ( max_depth = 5 ), RandomForestClassifier ( max_depth = 5 , n_estimators = 10 , max_features = 1 ), ] X , y = make_classification ( n_features = 2 , n_redundant = 0 , n_informative = 2 , random_state = 1 , n_clusters_per_class = 1 ) rng = np . random . RandomState ( 2 ) X += 2 * rng . uniform ( size = X . shape ) linearly_separable = ( X , y ) datasets = [ make_moons ( noise = 0.3 , random_state = 0 ), make_circles ( noise = 0.2 , factor = 0.5 , random_state = 1 ), linearly_separable ] figure = plt . figure ( figsize = ( 27 , 9 )) i = 1 # iterate over datasets for ds_cnt , ds in enumerate ( datasets ): # preprocess dataset, split into training and test part X , y = ds X = StandardScaler () . fit_transform ( X ) X_train , X_test , y_train , y_test = \\ train_test_split ( X , y , test_size = .4 , random_state = 42 ) x_min , x_max = X [:, 0 ] . min () - .5 , X [:, 0 ] . max () + .5 y_min , y_max = X [:, 1 ] . min () - .5 , X [:, 1 ] . max () + .5 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , h ), np . arange ( y_min , y_max , h )) # just plot the dataset first cm = plt . cm . RdBu cm_bright = ListedColormap ([ '#FF0000' , '#0000FF' ]) ax = plt . subplot ( len ( datasets ), len ( classifiers ) + 1 , i ) if ds_cnt == 0 : ax . set_title ( \"Input data\" ) # Plot the training points ax . scatter ( X_train [:, 0 ], X_train [:, 1 ], c = y_train , cmap = cm_bright , edgecolors = 'k' ) # Plot the testing points ax . scatter ( X_test [:, 0 ], X_test [:, 1 ], c = y_test , cmap = cm_bright , alpha = 0.6 , edgecolors = 'k' ) ax . set_xlim ( xx . min (), xx . max ()) ax . set_ylim ( yy . min (), yy . max ()) ax . set_xticks (()) ax . set_yticks (()) i += 1 # iterate over classifiers for name , clf in zip ( names , classifiers ): ax = plt . subplot ( len ( datasets ), len ( classifiers ) + 1 , i ) clf . fit ( X_train , y_train ) score = clf . score ( X_test , y_test ) # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, x_max]x[y_min, y_max]. if hasattr ( clf , \"decision_function\" ): Z = clf . decision_function ( np . c_ [ xx . ravel (), yy . ravel ()]) else : Z = clf . predict_proba ( np . c_ [ xx . ravel (), yy . ravel ()])[:, 1 ] # Put the result into a color plot Z = Z . reshape ( xx . shape ) ax . contourf ( xx , yy , Z , cmap = cm , alpha = .8 ) # Plot the training points ax . scatter ( X_train [:, 0 ], X_train [:, 1 ], c = y_train , cmap = cm_bright , edgecolors = 'k' ) # Plot the testing points ax . scatter ( X_test [:, 0 ], X_test [:, 1 ], c = y_test , cmap = cm_bright , edgecolors = 'k' , alpha = 0.6 ) ax . set_xlim ( xx . min (), xx . max ()) ax . set_ylim ( yy . min (), yy . max ()) ax . set_xticks (()) ax . set_yticks (()) if ds_cnt == 0 : ax . set_title ( name ) ax . text ( xx . max () - .3 , yy . min () + .3 , ( ' %.2f ' % score ) . lstrip ( '0' ), size = 15 , horizontalalignment = 'right' ) i += 1 plt . tight_layout () plt . show () <Figure size 864x864 with 0 Axes>","title":"Gr\u00e1ficos"},{"location":"lectures/ml/analisis_supervisado_clasificacion/02_clasificacion/#metricas","text":"Dado que el sistema de calcular m\u00e9tricas sigue el mismo formato, solo cambiando el conjunto de datos y el modelo, se decide realizar una clase que automatice este proceso. from metrics_classification import * class SklearnClassificationModels : def __init__ ( self , model , name_model ): self . model = model self . name_model = name_model @staticmethod def test_train_model ( X , y , n_size ): X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = n_size , random_state = 42 ) return X_train , X_test , y_train , y_test def fit_model ( self , X , y , test_size ): X_train , X_test , y_train , y_test = self . test_train_model ( X , y , test_size ) return self . model . fit ( X_train , y_train ) def df_testig ( self , X , y , test_size ): X_train , X_test , y_train , y_test = self . test_train_model ( X , y , test_size ) model_fit = self . model . fit ( X_train , y_train ) preds = model_fit . predict ( X_test ) df_temp = pd . DataFrame ( { 'y' : y_test , 'yhat' : model_fit . predict ( X_test ) } ) return df_temp def metrics ( self , X , y , test_size ): df_temp = self . df_testig ( X , y , test_size ) df_metrics = summary_metrics ( df_temp ) df_metrics [ 'model' ] = self . name_model return df_metrics # metrics import itertools # nombre modelos names_models = [ \"Logistic\" , \"RBF SVM\" , \"Decision Tree\" , \"Random Forest\" ] # modelos classifiers = [ LogisticRegression (), SVC ( gamma = 2 , C = 1 ), DecisionTreeClassifier ( max_depth = 5 ), RandomForestClassifier ( max_depth = 5 , n_estimators = 10 , max_features = 1 ), ] # datasets names_dataset = [ 'make_moons' , 'make_circles' , 'linearly_separable' ] X , y = make_classification ( n_features = 2 , n_redundant = 0 , n_informative = 2 , random_state = 1 , n_clusters_per_class = 1 ) rng = np . random . RandomState ( 2 ) X += 2 * rng . uniform ( size = X . shape ) linearly_separable = ( X , y ) datasets = [ make_moons ( noise = 0.3 , random_state = 0 ), make_circles ( noise = 0.2 , factor = 0.5 , random_state = 1 ), linearly_separable ] # juntar informacion list_models = list ( zip ( names_models , classifiers )) list_dataset = list ( zip ( names_dataset , datasets )) frames = [] for x in itertools . product ( list_models , list_dataset ): name_model = x [ 0 ][ 0 ] classifier = x [ 0 ][ 1 ] name_dataset = x [ 1 ][ 0 ] dataset = x [ 1 ][ 1 ] X = dataset [ 0 ] Y = dataset [ 1 ] fit_model = SklearnClassificationModels ( classifier , name_model ) df = fit_model . metrics ( X , Y , 0.2 ) df [ 'dataset' ] = name_dataset frames . append ( df ) /home/fralfaro/.cache/pypoetry/virtualenvs/mat281-2021-kaeOORRv-py3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) # juntar resultados pd . concat ( frames ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } accuracy recall precision fscore model dataset 0 0.90 0.9000 0.9000 0.9000 Logistic make_moons 0 0.35 0.5000 0.1750 0.2593 Logistic make_circles 0 0.95 0.9545 0.9500 0.9499 Logistic linearly_separable 0 0.95 0.9500 0.9545 0.9499 RBF SVM make_moons 0 0.80 0.8462 0.8182 0.7980 RBF SVM make_circles 0 0.95 0.9545 0.9500 0.9499 RBF SVM linearly_separable 0 0.95 0.9500 0.9545 0.9499 Decision Tree make_moons 0 0.75 0.8077 0.7917 0.7494 Decision Tree make_circles 0 0.90 0.8990 0.8990 0.8990 Decision Tree linearly_separable 0 0.95 0.9500 0.9545 0.9499 Random Forest make_moons 0 0.70 0.7363 0.7172 0.6970 Random Forest make_circles 0 0.90 0.9091 0.9091 0.9000 Random Forest linearly_separable","title":"M\u00e9tricas"},{"location":"lectures/ml/analisis_supervisado_clasificacion/02_clasificacion/#referencia","text":"Supervised learning","title":"Referencia"},{"location":"lectures/ml/analisis_supervisado_regresion/02_modelos_regresion/","text":"Modelos de regressi\u00f3n multiple Los modelos de regresi\u00f3n multiple son los m\u00e1s utilizados en el mundo de machine learning, puestos que se dispone de varios caracter\u00edstica de la poblaci\u00f3n objetivo. A menudo, se estar\u00e1 abordando estos modelos de la perspectiva de los modelos lineales, por lo cual se debetener en mente algunos supuestos antes de comenzar: No colinialidad o multicolinialidad : En los modelos lineales m\u00faltiples los predictores deben ser independientes, no debe de haber colinialidad entre ellos Parsimonia : Este t\u00e9rmino hace referencia a que el mejor modelo es aquel capaz de explicar con mayor precisi\u00f3n la variabilidad observada en la variable respuesta empleando el menor n\u00famero de predictores, por lo tanto, con menos asunciones. Homocedasticidad :La varianza de los residuos debe de ser constante en todo el rango de observaciones. Otros Factores : Distribuci\u00f3n normal de los residuos No autocorrelaci\u00f3n (Independencia) Valores at\u00edpicos, con alto leverage o influyentes Tama\u00f1o de la muestra Por otro lado, existen otros tipos de modelos de regresi\u00f3n, en los cuales se necesitan menos supuestos que los modelos de regresi\u00f3n lineal, a cambio se pierde un poco de interpretabilidad en sus par\u00e1metros y centran su atenci\u00f3n en los resultados obtenidos de las predicciones. Aplicaci\u00f3n con python Dataset Boston house prices En este ejemplo se va utilizar el dataset Boston que ya viene junto con sklearn y es ideal para practicar con Regresiones Lineales; el mismo contiene precios de casas de varias \u00e1reas de la ciudad de Boston. # librerias import os import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns pd . set_option ( 'display.max_columns' , 500 ) # Ver m\u00e1s columnas de los dataframes # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab % matplotlib inline # sklearn models from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression # cargar datos boston = datasets . load_boston () # dejar en formato dataframe boston_df = pd . DataFrame ( boston . data , columns = boston . feature_names ) boston_df [ 'TARGET' ] = boston . target boston_df . head () # estructura de nuestro dataset. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT TARGET 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2 # descripcion del conjunto de datos boston_df . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT TARGET count 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 mean 3.613524 11.363636 11.136779 0.069170 0.554695 6.284634 68.574901 3.795043 9.549407 408.237154 18.455534 356.674032 12.653063 22.532806 std 8.601545 23.322453 6.860353 0.253994 0.115878 0.702617 28.148861 2.105710 8.707259 168.537116 2.164946 91.294864 7.141062 9.197104 min 0.006320 0.000000 0.460000 0.000000 0.385000 3.561000 2.900000 1.129600 1.000000 187.000000 12.600000 0.320000 1.730000 5.000000 25% 0.082045 0.000000 5.190000 0.000000 0.449000 5.885500 45.025000 2.100175 4.000000 279.000000 17.400000 375.377500 6.950000 17.025000 50% 0.256510 0.000000 9.690000 0.000000 0.538000 6.208500 77.500000 3.207450 5.000000 330.000000 19.050000 391.440000 11.360000 21.200000 75% 3.677083 12.500000 18.100000 0.000000 0.624000 6.623500 94.075000 5.188425 24.000000 666.000000 20.200000 396.225000 16.955000 25.000000 max 88.976200 100.000000 27.740000 1.000000 0.871000 8.780000 100.000000 12.126500 24.000000 711.000000 22.000000 396.900000 37.970000 50.000000 #matriz de correlacion corr_mat = boston_df . corr ( method = 'pearson' ) plt . figure ( figsize = ( 20 , 10 )) sns . heatmap ( corr_mat , vmax = 1 , square = True , annot = True , cmap = 'cubehelix' , fmt = '.2f' ) plt . show () Apliquemos el modelo de regresi\u00f3n lineal multiple con sklearn # datos para la regresion lineal simple X = boston . data Y = boston_df [ \"TARGET\" ] # split dataset X_train , X_test , Y_train , Y_test = train_test_split ( X , Y , test_size = 0.2 , random_state = 2 ) # ajustar el modelo model_rl = LinearRegression () # Creando el modelo. model_rl . fit ( X_train , Y_train ) # ajustando el modelo # prediciones Y_predict = model_rl . predict ( X_test ) from metrics_regression import * from sklearn.metrics import r2_score # ejemplo: boston df df_temp = pd . DataFrame ( { 'y' : Y_test , 'yhat' : model_rl . predict ( X_test ) } ) df_metrics = summary_metrics ( df_temp ) df_metrics [ 'r2' ] = round ( r2_score ( Y_test , model_rl . predict ( X_test )), 4 ) print ( ' \\n Metricas para el regresor CRIM:' ) df_metrics Metricas para el regresor CRIM: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse mape maape wmape mmape smape r2 0 3.113 18.4954 4.3006 0.1604 0.1525 0.136 0.1507 0.1694 0.7789 Cuando se aplica el modelo de regresi\u00f3n lineal con todas las variables regresoras, las m\u00e9tricas disminuyen considerablemente, lo implica una mejora en el modelo Un problema que se tiene, a diferencia de la regresi\u00f3n lineal simple,es que no se puede ver gr\u00e1ficamente la calidad del ajuste, por lo que solo se puede confiar en las m\u00e9tricas calculadas. Adem\u00e1s, se dejan las siguientes preguntas: \u00bf Entre m\u00e1s regresores, mejor ser\u00e1 el modelo de regresi\u00f3n lineal? \u00bf Qu\u00e9 se debe tener en cuenta antes de agregar otro variable regresora al modelo de regresi\u00f3n lineal ? \u00bf Qu\u00e9 sucede si se tienen outliers ? \u00bf Existen otros modelos mejor que la regresi\u00f3n lineal ? Ya se han discutido algunos de estos puntos, por lo que la atenci\u00f3n estar\u00e1 en abordar otros modelos. Otros modelos de Regresi\u00f3n Existen varios modelos de regresi\u00f3n, sin embargo, la intepretaci\u00f3n de sus par\u00e1metros y el an\u00e1lisis de confiabilidad no es tan directo como los modelos de regresi\u00f3n lineal. Por este motivo, la atenci\u00f3n estar\u00e1 centrada en la predicci\u00f3n m\u00e1s que en la confiabilidad como tal del modelo. Modelos lineales Existen varios modelos lineales que podemos trabajar en sklearn (ver referencia ), los cualeas podemos utilizar e ir comparando unos con otros. De lo modelos lineales, destacamos los siguientes: regresi\u00f3n lineal cl\u00e1sica : regresi\u00f3n cl\u00e1sica por m\u00ednimos cudrados. $$(P)\\ \\min \\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2 $$ lasso : se ocupa cuando tenemos un gran n\u00famero de regresores y queremos que disminuya el problema de colinealidad (es decir, estimar como cero los par\u00e1metros poco relevantes). $$(P)\\ \\min \\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2 + \\lambda \\sum_{i=1}^n |\\beta_{i}| $$ ridge : tambi\u00e9n sirve para disminuir el problema de colinealidad, y adem\u00e1s trata de que los coeficientes sean m\u00e1s rocuesto bajo outliers. $$(P)\\ \\min \\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2 + \\lambda \\sum_{i=1}^n \\beta_{i}^2 $$ Dado que en sklearn, la forma de entrenar, estimar y predecir modelos de regresi\u00f3n siguen una misma estructura, para fectos pr\u00e1cticos, definimos una rutina para estimar las distintas m\u00e9tricas de la siguiente manera: Bayesian Regression En estad\u00edstica, la regresi\u00f3n lineal bayesiana es un enfoque de regresi\u00f3n lineal en el que el an\u00e1lisis estad\u00edstico se realiza dentro del contexto de la inferencia bayesiana. Cuando el modelo de regresi\u00f3n tiene errores que tienen una distribuci\u00f3n normal, y si se asume una forma particular de distribuci\u00f3n previa, los resultados expl\u00edcitos est\u00e1n disponibles para las distribuciones de probabilidad posteriores de los par\u00e1metros del modelo. k-vecinos m\u00e1s cercanos Knn El m\u00e9todo de los \\(k\\) vecinos m\u00e1s cercanos (en ingl\u00e9s, k-nearest neighbors, abreviado \\(knn\\) ) es un m\u00e9todo de clasificaci\u00f3n supervisada (Aprendizaje, estimaci\u00f3n basada en un conjunto de entrenamiento y prototipos) que sirve para estimar la funci\u00f3n de densidad \\(F(x/C_j)\\) de las predictoras \\(x\\) por cada clase \\(C_{j}\\) . Este es un m\u00e9todo de clasificaci\u00f3n no param\u00e9trico, que estima el valor de la funci\u00f3n de densidad de probabilidad o directamente la probabilidad a posteriori de que un elemento \\(x\\) pertenezca a la clase \\(C_j\\) a partir de la informaci\u00f3n proporcionada por el conjunto de prototipos. En el proceso de aprendizaje no se hace ninguna suposici\u00f3n acerca de la distribuci\u00f3n de las variables predictoras. En el reconocimiento de patrones, el algoritmo \\(knn\\) es usado como m\u00e9todo de clasificaci\u00f3n de objetos (elementos) basado en un entrenamiento mediante ejemplos cercanos en el espacio de los elementos. \\(knn\\) es un tipo de aprendizaje vago (lazy learning), donde la funci\u00f3n se aproxima solo localmente y todo el c\u00f3mputo es diferido a la clasificaci\u00f3n. La normalizaci\u00f3n de datos puede mejorar considerablemente la exactitud del algoritmo \\(knn\\) . Decision Tree Regressor Un \u00e1rbol de decisi\u00f3n es un modelo de predicci\u00f3n utilizado en diversos \u00e1mbitos que van desde la inteligencia artificial hasta la Econom\u00eda. Dado un conjunto de datos se fabrican diagramas de construcciones l\u00f3gicas, muy similares a los sistemas de predicci\u00f3n basados en reglas, que sirven para representar y categorizar una serie de condiciones que ocurren de forma sucesiva, para la resoluci\u00f3n de un problema. Vamos a explicar c\u00f3mo se construye un \u00e1rbol de decisi\u00f3n. Para ello, vamos a hacer hincapi\u00e9 en varios aspectos Elementos Los \u00e1rboles de decisi\u00f3n est\u00e1n formados por nodos, vectores de n\u00fameros, flechas y etiquetas. Cada nodo se puede definir como el momento en el que se ha de tomar una decisi\u00f3n de entre varias posibles, lo que va haciendo que a medida que aumenta el n\u00famero de nodos aumente el n\u00famero de posibles finales a los que puede llegar el individuo. Esto hace que un \u00e1rbol con muchos nodos sea complicado de dibujar a mano y de analizar debido a la existencia de numerosos caminos que se pueden seguir. Los vectores de n\u00fameros ser\u00edan la soluci\u00f3n final a la que se llega en funci\u00f3n de las diversas posibilidades que se tienen, dan las utilidades en esa soluci\u00f3n. Las flechas son las uniones entre un nodo y otro y representan cada acci\u00f3n distinta. Las etiquetas se encuentran en cada nodo y cada flecha y dan nombre a cada acci\u00f3n. Conceptos Cuando tratemos en el desarrollo de \u00e1rboles utilizaremos frecuentemente estos conceptos: Costo. Se refiere a dos conceptos diferentes: el costo de medici\u00f3n para determinar el valor de una determinada propiedad (atributo) exhibida por el objeto y el costo de clasificaci\u00f3n err\u00f3nea al decidir que el objeto pertenece a la clase \\(X\\) cuando su clase real es \\(Y\\) . Sobreajuste (Overfitting). Se produce cuando los datos de entrenamiento son pocos o contienen incoherencias. Al tomar un espacio de hip\u00f3tesis \\(H\\) , se dice que una hip\u00f3tesis \\(h \u2208 H\\) sobreajusta un conjunto de entrenamiento \\(C\\) si existe alguna hip\u00f3tesis alternativa \\(h' \u2208 H\\) tal que \\(h\\) clasifica mejor que \\(h'\\) los elementos del conjunto de entrenamiento, pero \\(h'\\) clasifica mejor que h el conjunto completo de posibles instancias. Poda (Prunning). La poda consiste en eliminar una rama de un nodo transform\u00e1ndolo en una hoja (terminal), asign\u00e1ndole la clasificaci\u00f3n m\u00e1s com\u00fan de los ejemplos de entrenamiento considerados en ese nodo. La validaci\u00f3n cruzada. Es el proceso de construir un \u00e1rbol con la mayor\u00eda de los datos y luego usar la parte restante de los datos para probar la precisi\u00f3n del \u00e1rbol. Reglas En los \u00e1rboles de decisi\u00f3n se tiene que cumplir una serie de reglas. Al comienzo del juego se da un nodo inicial que no es apuntado por ninguna flecha, es el \u00fanico del juego con esta caracter\u00edstica. El resto de los nodos del juego son apuntados por una \u00fanica flecha. De esto se deduce que hay un \u00fanico camino para llegar del nodo inicial a cada uno de los nodos del juego. No hay varias formas de llegar a la misma soluci\u00f3n final, las decisiones son excluyentes. En los \u00e1rboles de decisiones las decisiones que se eligen son lineales, a medida que vas seleccionando entre varias opciones se van cerrando otras, lo que implica normalmente que no hay marcha atr\u00e1s. En general se podr\u00eda decir que las normas siguen una forma condicional: \\[\\textrm{Opci\u00f3n }1->\\textrm{opci\u00f3n }2->\\textrm{opci\u00f3n }3->\\textrm{Resultado Final }X\\] Estas reglas suelen ir impl\u00edcitas en el conjunto de datos a ra\u00edz del cual se construye el \u00e1rbol de decisi\u00f3n. SVM Las m\u00e1quinas de vectores de soporte (del ingl\u00e9s Support Vector Machines, SVM) son un conjunto de algoritmos de aprendizaje supervisado desarrollados por Vladimir Vapnik y su equipo en los laboratorios AT&T. Estos m\u00e9todos est\u00e1n propiamente relacionados con problemas de clasificaci\u00f3n y regresi\u00f3n. Dado un conjunto de ejemplos de entrenamiento (de muestras) podemos etiquetar las clases y entrenar una SVM para construir un modelo que prediga la clase de una nueva muestra. Intuitivamente, una SVM es un modelo que representa a los puntos de muestra en el espacio, separando las clases a 2 espacios lo m\u00e1s amplios posibles mediante un hiperplano de separaci\u00f3n definido como el vector entre los 2 puntos, de las 2 clases, m\u00e1s cercanos al que se llama vector soporte. Cuando las nuevas muestras se ponen en correspondencia con dicho modelo, en funci\u00f3n de los espacios a los que pertenezcan, pueden ser clasificadas a una o la otra clase. M\u00e1s formalmente, una SVM construye un hiperplano o conjunto de hiperplanos en un espacio de dimensionalidad muy alta (o incluso infinita) que puede ser utilizado en problemas de clasificaci\u00f3n o regresi\u00f3n. Una buena separaci\u00f3n entre las clases permitir\u00e1 una clasificaci\u00f3n correcta. Idea B\u00e1sica Dado un conjunto de puntos, subconjunto de un conjunto mayor (espacio), en el que cada uno de ellos pertenece a una de dos posibles categor\u00edas, un algoritmo basado en SVM construye un modelo capaz de predecir si un punto nuevo (cuya categor\u00eda desconocemos) pertenece a una categor\u00eda o a la otra. Como en la mayor\u00eda de los m\u00e9todos de clasificaci\u00f3n supervisada, los datos de entrada (los puntos) son vistos como un vector \\(p-dimensional\\) (una lista ordenada de \\(p\\) n\u00fameros). La SVM busca un hiperplano que separe de forma \u00f3ptima a los puntos de una clase de la de otra, que eventualmente han podido ser previamente proyectados a un espacio de dimensionalidad superior. En ese concepto de \"separaci\u00f3n \u00f3ptima\" es donde reside la caracter\u00edstica fundamental de las SVM: este tipo de algoritmos buscan el hiperplano que tenga la m\u00e1xima distancia (margen) con los puntos que est\u00e9n m\u00e1s cerca de \u00e9l mismo. Por eso tambi\u00e9n a veces se les conoce a las SVM como clasificadores de margen m\u00e1ximo. De esta forma, los puntos del vector que son etiquetados con una categor\u00eda estar\u00e1n a un lado del hiperplano y los casos que se encuentren en la otra categor\u00eda estar\u00e1n al otro lado. Los algoritmos SVM pertenecen a la familia de los clasificadores lineales. Tambi\u00e9n pueden ser considerados un caso especial de la regularizaci\u00f3n de Tikhonov. En la literatura de las SVM, se llama atributo a la variable predictora y caracter\u00edstica a un atributo transformado que es usado para definir el hiperplano. La elecci\u00f3n de la representaci\u00f3n m\u00e1s adecuada del universo estudiado, se realiza mediante un proceso denominado selecci\u00f3n de caracter\u00edsticas. Al vector formado por los puntos m\u00e1s cercanos al hiperplano se le llama vector de soporte. Los modelos basados en SVM est\u00e1n estrechamente relacionados con las redes neuronales. Usando una funci\u00f3n kernel, resultan un m\u00e9todo de entrenamiento alternativo para clasificadores polinomiales, funciones de base radial y perceptr\u00f3n multicapa. Aplicando varios modelos al mismo tiempo Veremos el performance de los distintos modelos estudiados. from sklearn import linear_model from sklearn import tree from sklearn import svm from sklearn import neighbors class SklearnRegressionModels : def __init__ ( self , model , name_model ): self . model = model self . name_model = name_model @staticmethod def test_train_model ( X , y , n_size ): X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = n_size , random_state = 42 ) return X_train , X_test , y_train , y_test def fit_model ( self , X , y , test_size ): X_train , X_test , y_train , y_test = self . test_train_model ( X , y , test_size ) return self . model . fit ( X_train , y_train ) def df_testig ( self , X , y , test_size ): X_train , X_test , y_train , y_test = self . test_train_model ( X , y , test_size ) model_fit = self . model . fit ( X_train , y_train ) preds = model_fit . predict ( X_test ) df_temp = pd . DataFrame ( { 'y' : y_test , 'yhat' : model_fit . predict ( X_test ) } ) return df_temp def metrics ( self , X , y , test_size ): df_temp = self . df_testig ( X , y , test_size ) df_metrics = summary_metrics ( df_temp ) df_metrics [ 'r2' ] = round ( r2_score ( df_temp [ 'y' ], df_temp [ 'yhat' ]), 4 ) df_metrics [ 'model' ] = self . name_model return df_metrics def parameters ( self , X , y , test_size ): model_fit = self . fit_model ( X , y , test_size ) list_betas = [ ( 'beta_0' , model_fit . intercept_ ) ] betas = model_fit . coef_ for num , beta in enumerate ( betas ): name_beta = f 'beta_ { num + 1 } ' list_betas . append (( name_beta , round ( beta , 2 ))) result = pd . DataFrame ( columns = [ 'coef' , 'value' ], data = list_betas ) result [ 'model' ] = self . name_model return result # boston dataframe X = boston . data Y = boston_df [ \"TARGET\" ] # models reg_lineal = linear_model . LinearRegression () reg_ridge = linear_model . Ridge ( alpha = .5 ) reg_lasso = linear_model . Lasso ( alpha = 0.1 ) reg_knn = neighbors . KNeighborsRegressor ( 5 , weights = 'uniform' ) reg_bayesian = linear_model . BayesianRidge () reg_tree = tree . DecisionTreeRegressor ( max_depth = 5 ) reg_svm = svm . SVR ( kernel = 'linear' ) list_models = [ [ reg_lineal , 'lineal' ], [ reg_ridge , 'ridge' ], [ reg_lasso , 'lasso' ], [ reg_knn , 'knn' ], [ reg_bayesian , 'bayesian' ], [ reg_tree , 'decision_tree' ], [ reg_svm , 'svm' ], ] frames_metrics = [] frames_coef = [] for model , name_models in list_models : fit_model = SklearnRegressionModels ( model , name_models ) frames_metrics . append ( fit_model . metrics ( X , Y , 0.2 )) if name_models in [ 'lineal' , 'ridge' , 'lasso' ]: frames_coef . append ( fit_model . parameters ( X , Y , 0.2 )) # juntar resultados: metricas pd . concat ( frames_metrics ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse mape maape wmape mmape smape r2 model 0 3.1891 24.2911 4.9286 0.1687 0.1538 0.1484 0.1579 0.1790 0.6688 lineal 0 3.1493 24.3776 4.9374 0.1668 0.1516 0.1466 0.1562 0.1769 0.6676 ridge 0 3.1452 25.1556 5.0155 0.1675 0.1512 0.1464 0.1569 0.1765 0.6570 lasso 0 3.6639 25.8601 5.0853 0.1889 0.1789 0.1705 0.1769 0.1777 0.6474 knn 0 3.1251 24.6471 4.9646 0.1654 0.1496 0.1454 0.1550 0.1748 0.6639 bayesian 0 2.3082 8.5539 2.9247 0.1309 0.1242 0.1074 0.1219 0.1233 0.8834 decision_tree 0 3.1404 29.4359 5.4255 0.1677 0.1496 0.1461 0.1568 0.1847 0.5986 svm Basados en los distintos estad\u00edsticos, el mejor modelo corresponde al modelo de decision_tree . Por otro lado, podemos analizar los coeficientes de los modelos l\u00edneales ordinarios,Ridge y Lasso. # juntar resultados: coeficientes pd . concat ( frames_coef ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef value model 0 beta_0 30.246751 lineal 1 beta_1 -0.110000 lineal 2 beta_2 0.030000 lineal 3 beta_3 0.040000 lineal 4 beta_4 2.780000 lineal 5 beta_5 -17.200000 lineal 6 beta_6 4.440000 lineal 7 beta_7 -0.010000 lineal 8 beta_8 -1.450000 lineal 9 beta_9 0.260000 lineal 10 beta_10 -0.010000 lineal 11 beta_11 -0.920000 lineal 12 beta_12 0.010000 lineal 13 beta_13 -0.510000 lineal 0 beta_0 26.891132 ridge 1 beta_1 -0.110000 ridge 2 beta_2 0.030000 ridge 3 beta_3 0.020000 ridge 4 beta_4 2.640000 ridge 5 beta_5 -12.270000 ridge 6 beta_6 4.460000 ridge 7 beta_7 -0.010000 ridge 8 beta_8 -1.380000 ridge 9 beta_9 0.250000 ridge 10 beta_10 -0.010000 ridge 11 beta_11 -0.860000 ridge 12 beta_12 0.010000 ridge 13 beta_13 -0.520000 ridge 0 beta_0 19.859769 lasso 1 beta_1 -0.100000 lasso 2 beta_2 0.030000 lasso 3 beta_3 -0.020000 lasso 4 beta_4 0.920000 lasso 5 beta_5 -0.000000 lasso 6 beta_6 4.310000 lasso 7 beta_7 -0.020000 lasso 8 beta_8 -1.150000 lasso 9 beta_9 0.240000 lasso 10 beta_10 -0.010000 lasso 11 beta_11 -0.730000 lasso 12 beta_12 0.010000 lasso 13 beta_13 -0.560000 lasso Al comparar los resultados entre ambos modelos, se observa que hay coeficientes en la regresi\u00f3n Lasso que se van a cero directamente, pudiendo eliminar estas variables del modelo. Por otro lado, queda como tarea para el lector, hacer una eliminaci\u00f3n de outliers del modelo y probar estos modelos lineales para ver si existe alg\u00fan tipo de diferencia. Conclusi\u00f3n Existen distintos modelos de regresi\u00f3n lineal: normal, Ridge y Lasso. Cada uno con sus respectivs ventajas y desventajas. Existen otros tipos de modelos de regresi\u00f3n (bayesiano, knn, arboles de decisi\u00f3n, svm, entre otros). Por ahora, nos interesa saber como funcionan, para poder configurar los hiperpar\u00e1metros de los modelos ocupados en python (principalmente de la librer\u00eda sklearn ). En el mundo del machine learning se estar\u00e1 interesado m\u00e1s en predecir con el menor error posible (siempre tomando como referencia alguna de las m\u00e9tricas mencionadas) que hacer un an\u00e1lisis exhaustivo de la confiabilidad del modelo. Siendo este el caso y si la capacidad computacional lo permite, lo ideal es probar varios modelos al mismo tiempo y poder discriminar bajo un determinado criterio (a menudo el error cuadr\u00e1tico medio (rmse) o el mape ). Referencia Supervised learning","title":"Modelos de regressi\u00f3n multiple"},{"location":"lectures/ml/analisis_supervisado_regresion/02_modelos_regresion/#modelos-de-regression-multiple","text":"Los modelos de regresi\u00f3n multiple son los m\u00e1s utilizados en el mundo de machine learning, puestos que se dispone de varios caracter\u00edstica de la poblaci\u00f3n objetivo. A menudo, se estar\u00e1 abordando estos modelos de la perspectiva de los modelos lineales, por lo cual se debetener en mente algunos supuestos antes de comenzar: No colinialidad o multicolinialidad : En los modelos lineales m\u00faltiples los predictores deben ser independientes, no debe de haber colinialidad entre ellos Parsimonia : Este t\u00e9rmino hace referencia a que el mejor modelo es aquel capaz de explicar con mayor precisi\u00f3n la variabilidad observada en la variable respuesta empleando el menor n\u00famero de predictores, por lo tanto, con menos asunciones. Homocedasticidad :La varianza de los residuos debe de ser constante en todo el rango de observaciones. Otros Factores : Distribuci\u00f3n normal de los residuos No autocorrelaci\u00f3n (Independencia) Valores at\u00edpicos, con alto leverage o influyentes Tama\u00f1o de la muestra Por otro lado, existen otros tipos de modelos de regresi\u00f3n, en los cuales se necesitan menos supuestos que los modelos de regresi\u00f3n lineal, a cambio se pierde un poco de interpretabilidad en sus par\u00e1metros y centran su atenci\u00f3n en los resultados obtenidos de las predicciones.","title":"Modelos de regressi\u00f3n multiple"},{"location":"lectures/ml/analisis_supervisado_regresion/02_modelos_regresion/#aplicacion-con-python","text":"","title":"Aplicaci\u00f3n con python"},{"location":"lectures/ml/analisis_supervisado_regresion/02_modelos_regresion/#dataset-boston-house-prices","text":"En este ejemplo se va utilizar el dataset Boston que ya viene junto con sklearn y es ideal para practicar con Regresiones Lineales; el mismo contiene precios de casas de varias \u00e1reas de la ciudad de Boston. # librerias import os import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns pd . set_option ( 'display.max_columns' , 500 ) # Ver m\u00e1s columnas de los dataframes # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab % matplotlib inline # sklearn models from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression # cargar datos boston = datasets . load_boston () # dejar en formato dataframe boston_df = pd . DataFrame ( boston . data , columns = boston . feature_names ) boston_df [ 'TARGET' ] = boston . target boston_df . head () # estructura de nuestro dataset. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT TARGET 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2 # descripcion del conjunto de datos boston_df . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT TARGET count 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 mean 3.613524 11.363636 11.136779 0.069170 0.554695 6.284634 68.574901 3.795043 9.549407 408.237154 18.455534 356.674032 12.653063 22.532806 std 8.601545 23.322453 6.860353 0.253994 0.115878 0.702617 28.148861 2.105710 8.707259 168.537116 2.164946 91.294864 7.141062 9.197104 min 0.006320 0.000000 0.460000 0.000000 0.385000 3.561000 2.900000 1.129600 1.000000 187.000000 12.600000 0.320000 1.730000 5.000000 25% 0.082045 0.000000 5.190000 0.000000 0.449000 5.885500 45.025000 2.100175 4.000000 279.000000 17.400000 375.377500 6.950000 17.025000 50% 0.256510 0.000000 9.690000 0.000000 0.538000 6.208500 77.500000 3.207450 5.000000 330.000000 19.050000 391.440000 11.360000 21.200000 75% 3.677083 12.500000 18.100000 0.000000 0.624000 6.623500 94.075000 5.188425 24.000000 666.000000 20.200000 396.225000 16.955000 25.000000 max 88.976200 100.000000 27.740000 1.000000 0.871000 8.780000 100.000000 12.126500 24.000000 711.000000 22.000000 396.900000 37.970000 50.000000 #matriz de correlacion corr_mat = boston_df . corr ( method = 'pearson' ) plt . figure ( figsize = ( 20 , 10 )) sns . heatmap ( corr_mat , vmax = 1 , square = True , annot = True , cmap = 'cubehelix' , fmt = '.2f' ) plt . show () Apliquemos el modelo de regresi\u00f3n lineal multiple con sklearn # datos para la regresion lineal simple X = boston . data Y = boston_df [ \"TARGET\" ] # split dataset X_train , X_test , Y_train , Y_test = train_test_split ( X , Y , test_size = 0.2 , random_state = 2 ) # ajustar el modelo model_rl = LinearRegression () # Creando el modelo. model_rl . fit ( X_train , Y_train ) # ajustando el modelo # prediciones Y_predict = model_rl . predict ( X_test ) from metrics_regression import * from sklearn.metrics import r2_score # ejemplo: boston df df_temp = pd . DataFrame ( { 'y' : Y_test , 'yhat' : model_rl . predict ( X_test ) } ) df_metrics = summary_metrics ( df_temp ) df_metrics [ 'r2' ] = round ( r2_score ( Y_test , model_rl . predict ( X_test )), 4 ) print ( ' \\n Metricas para el regresor CRIM:' ) df_metrics Metricas para el regresor CRIM: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse mape maape wmape mmape smape r2 0 3.113 18.4954 4.3006 0.1604 0.1525 0.136 0.1507 0.1694 0.7789 Cuando se aplica el modelo de regresi\u00f3n lineal con todas las variables regresoras, las m\u00e9tricas disminuyen considerablemente, lo implica una mejora en el modelo Un problema que se tiene, a diferencia de la regresi\u00f3n lineal simple,es que no se puede ver gr\u00e1ficamente la calidad del ajuste, por lo que solo se puede confiar en las m\u00e9tricas calculadas. Adem\u00e1s, se dejan las siguientes preguntas: \u00bf Entre m\u00e1s regresores, mejor ser\u00e1 el modelo de regresi\u00f3n lineal? \u00bf Qu\u00e9 se debe tener en cuenta antes de agregar otro variable regresora al modelo de regresi\u00f3n lineal ? \u00bf Qu\u00e9 sucede si se tienen outliers ? \u00bf Existen otros modelos mejor que la regresi\u00f3n lineal ? Ya se han discutido algunos de estos puntos, por lo que la atenci\u00f3n estar\u00e1 en abordar otros modelos.","title":"Dataset  Boston house prices"},{"location":"lectures/ml/analisis_supervisado_regresion/02_modelos_regresion/#otros-modelos-de-regresion","text":"Existen varios modelos de regresi\u00f3n, sin embargo, la intepretaci\u00f3n de sus par\u00e1metros y el an\u00e1lisis de confiabilidad no es tan directo como los modelos de regresi\u00f3n lineal. Por este motivo, la atenci\u00f3n estar\u00e1 centrada en la predicci\u00f3n m\u00e1s que en la confiabilidad como tal del modelo.","title":"Otros modelos de Regresi\u00f3n"},{"location":"lectures/ml/analisis_supervisado_regresion/02_modelos_regresion/#modelos-lineales","text":"Existen varios modelos lineales que podemos trabajar en sklearn (ver referencia ), los cualeas podemos utilizar e ir comparando unos con otros. De lo modelos lineales, destacamos los siguientes: regresi\u00f3n lineal cl\u00e1sica : regresi\u00f3n cl\u00e1sica por m\u00ednimos cudrados. $$(P)\\ \\min \\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2 $$ lasso : se ocupa cuando tenemos un gran n\u00famero de regresores y queremos que disminuya el problema de colinealidad (es decir, estimar como cero los par\u00e1metros poco relevantes). $$(P)\\ \\min \\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2 + \\lambda \\sum_{i=1}^n |\\beta_{i}| $$ ridge : tambi\u00e9n sirve para disminuir el problema de colinealidad, y adem\u00e1s trata de que los coeficientes sean m\u00e1s rocuesto bajo outliers. $$(P)\\ \\min \\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2 + \\lambda \\sum_{i=1}^n \\beta_{i}^2 $$ Dado que en sklearn, la forma de entrenar, estimar y predecir modelos de regresi\u00f3n siguen una misma estructura, para fectos pr\u00e1cticos, definimos una rutina para estimar las distintas m\u00e9tricas de la siguiente manera:","title":"Modelos lineales"},{"location":"lectures/ml/analisis_supervisado_regresion/02_modelos_regresion/#bayesian-regression","text":"En estad\u00edstica, la regresi\u00f3n lineal bayesiana es un enfoque de regresi\u00f3n lineal en el que el an\u00e1lisis estad\u00edstico se realiza dentro del contexto de la inferencia bayesiana. Cuando el modelo de regresi\u00f3n tiene errores que tienen una distribuci\u00f3n normal, y si se asume una forma particular de distribuci\u00f3n previa, los resultados expl\u00edcitos est\u00e1n disponibles para las distribuciones de probabilidad posteriores de los par\u00e1metros del modelo.","title":"Bayesian Regression"},{"location":"lectures/ml/analisis_supervisado_regresion/02_modelos_regresion/#k-vecinos-mas-cercanos-knn","text":"El m\u00e9todo de los \\(k\\) vecinos m\u00e1s cercanos (en ingl\u00e9s, k-nearest neighbors, abreviado \\(knn\\) ) es un m\u00e9todo de clasificaci\u00f3n supervisada (Aprendizaje, estimaci\u00f3n basada en un conjunto de entrenamiento y prototipos) que sirve para estimar la funci\u00f3n de densidad \\(F(x/C_j)\\) de las predictoras \\(x\\) por cada clase \\(C_{j}\\) . Este es un m\u00e9todo de clasificaci\u00f3n no param\u00e9trico, que estima el valor de la funci\u00f3n de densidad de probabilidad o directamente la probabilidad a posteriori de que un elemento \\(x\\) pertenezca a la clase \\(C_j\\) a partir de la informaci\u00f3n proporcionada por el conjunto de prototipos. En el proceso de aprendizaje no se hace ninguna suposici\u00f3n acerca de la distribuci\u00f3n de las variables predictoras. En el reconocimiento de patrones, el algoritmo \\(knn\\) es usado como m\u00e9todo de clasificaci\u00f3n de objetos (elementos) basado en un entrenamiento mediante ejemplos cercanos en el espacio de los elementos. \\(knn\\) es un tipo de aprendizaje vago (lazy learning), donde la funci\u00f3n se aproxima solo localmente y todo el c\u00f3mputo es diferido a la clasificaci\u00f3n. La normalizaci\u00f3n de datos puede mejorar considerablemente la exactitud del algoritmo \\(knn\\) .","title":"k-vecinos m\u00e1s cercanos  Knn"},{"location":"lectures/ml/analisis_supervisado_regresion/02_modelos_regresion/#decision-tree-regressor","text":"Un \u00e1rbol de decisi\u00f3n es un modelo de predicci\u00f3n utilizado en diversos \u00e1mbitos que van desde la inteligencia artificial hasta la Econom\u00eda. Dado un conjunto de datos se fabrican diagramas de construcciones l\u00f3gicas, muy similares a los sistemas de predicci\u00f3n basados en reglas, que sirven para representar y categorizar una serie de condiciones que ocurren de forma sucesiva, para la resoluci\u00f3n de un problema. Vamos a explicar c\u00f3mo se construye un \u00e1rbol de decisi\u00f3n. Para ello, vamos a hacer hincapi\u00e9 en varios aspectos","title":"Decision Tree Regressor"},{"location":"lectures/ml/analisis_supervisado_regresion/02_modelos_regresion/#elementos","text":"Los \u00e1rboles de decisi\u00f3n est\u00e1n formados por nodos, vectores de n\u00fameros, flechas y etiquetas. Cada nodo se puede definir como el momento en el que se ha de tomar una decisi\u00f3n de entre varias posibles, lo que va haciendo que a medida que aumenta el n\u00famero de nodos aumente el n\u00famero de posibles finales a los que puede llegar el individuo. Esto hace que un \u00e1rbol con muchos nodos sea complicado de dibujar a mano y de analizar debido a la existencia de numerosos caminos que se pueden seguir. Los vectores de n\u00fameros ser\u00edan la soluci\u00f3n final a la que se llega en funci\u00f3n de las diversas posibilidades que se tienen, dan las utilidades en esa soluci\u00f3n. Las flechas son las uniones entre un nodo y otro y representan cada acci\u00f3n distinta. Las etiquetas se encuentran en cada nodo y cada flecha y dan nombre a cada acci\u00f3n.","title":"Elementos"},{"location":"lectures/ml/analisis_supervisado_regresion/02_modelos_regresion/#conceptos","text":"Cuando tratemos en el desarrollo de \u00e1rboles utilizaremos frecuentemente estos conceptos: Costo. Se refiere a dos conceptos diferentes: el costo de medici\u00f3n para determinar el valor de una determinada propiedad (atributo) exhibida por el objeto y el costo de clasificaci\u00f3n err\u00f3nea al decidir que el objeto pertenece a la clase \\(X\\) cuando su clase real es \\(Y\\) . Sobreajuste (Overfitting). Se produce cuando los datos de entrenamiento son pocos o contienen incoherencias. Al tomar un espacio de hip\u00f3tesis \\(H\\) , se dice que una hip\u00f3tesis \\(h \u2208 H\\) sobreajusta un conjunto de entrenamiento \\(C\\) si existe alguna hip\u00f3tesis alternativa \\(h' \u2208 H\\) tal que \\(h\\) clasifica mejor que \\(h'\\) los elementos del conjunto de entrenamiento, pero \\(h'\\) clasifica mejor que h el conjunto completo de posibles instancias. Poda (Prunning). La poda consiste en eliminar una rama de un nodo transform\u00e1ndolo en una hoja (terminal), asign\u00e1ndole la clasificaci\u00f3n m\u00e1s com\u00fan de los ejemplos de entrenamiento considerados en ese nodo. La validaci\u00f3n cruzada. Es el proceso de construir un \u00e1rbol con la mayor\u00eda de los datos y luego usar la parte restante de los datos para probar la precisi\u00f3n del \u00e1rbol.","title":"Conceptos"},{"location":"lectures/ml/analisis_supervisado_regresion/02_modelos_regresion/#reglas","text":"En los \u00e1rboles de decisi\u00f3n se tiene que cumplir una serie de reglas. Al comienzo del juego se da un nodo inicial que no es apuntado por ninguna flecha, es el \u00fanico del juego con esta caracter\u00edstica. El resto de los nodos del juego son apuntados por una \u00fanica flecha. De esto se deduce que hay un \u00fanico camino para llegar del nodo inicial a cada uno de los nodos del juego. No hay varias formas de llegar a la misma soluci\u00f3n final, las decisiones son excluyentes. En los \u00e1rboles de decisiones las decisiones que se eligen son lineales, a medida que vas seleccionando entre varias opciones se van cerrando otras, lo que implica normalmente que no hay marcha atr\u00e1s. En general se podr\u00eda decir que las normas siguen una forma condicional: \\[\\textrm{Opci\u00f3n }1->\\textrm{opci\u00f3n }2->\\textrm{opci\u00f3n }3->\\textrm{Resultado Final }X\\] Estas reglas suelen ir impl\u00edcitas en el conjunto de datos a ra\u00edz del cual se construye el \u00e1rbol de decisi\u00f3n.","title":"Reglas"},{"location":"lectures/ml/analisis_supervisado_regresion/02_modelos_regresion/#svm","text":"Las m\u00e1quinas de vectores de soporte (del ingl\u00e9s Support Vector Machines, SVM) son un conjunto de algoritmos de aprendizaje supervisado desarrollados por Vladimir Vapnik y su equipo en los laboratorios AT&T. Estos m\u00e9todos est\u00e1n propiamente relacionados con problemas de clasificaci\u00f3n y regresi\u00f3n. Dado un conjunto de ejemplos de entrenamiento (de muestras) podemos etiquetar las clases y entrenar una SVM para construir un modelo que prediga la clase de una nueva muestra. Intuitivamente, una SVM es un modelo que representa a los puntos de muestra en el espacio, separando las clases a 2 espacios lo m\u00e1s amplios posibles mediante un hiperplano de separaci\u00f3n definido como el vector entre los 2 puntos, de las 2 clases, m\u00e1s cercanos al que se llama vector soporte. Cuando las nuevas muestras se ponen en correspondencia con dicho modelo, en funci\u00f3n de los espacios a los que pertenezcan, pueden ser clasificadas a una o la otra clase. M\u00e1s formalmente, una SVM construye un hiperplano o conjunto de hiperplanos en un espacio de dimensionalidad muy alta (o incluso infinita) que puede ser utilizado en problemas de clasificaci\u00f3n o regresi\u00f3n. Una buena separaci\u00f3n entre las clases permitir\u00e1 una clasificaci\u00f3n correcta.","title":"SVM"},{"location":"lectures/ml/analisis_supervisado_regresion/02_modelos_regresion/#idea-basica","text":"Dado un conjunto de puntos, subconjunto de un conjunto mayor (espacio), en el que cada uno de ellos pertenece a una de dos posibles categor\u00edas, un algoritmo basado en SVM construye un modelo capaz de predecir si un punto nuevo (cuya categor\u00eda desconocemos) pertenece a una categor\u00eda o a la otra. Como en la mayor\u00eda de los m\u00e9todos de clasificaci\u00f3n supervisada, los datos de entrada (los puntos) son vistos como un vector \\(p-dimensional\\) (una lista ordenada de \\(p\\) n\u00fameros). La SVM busca un hiperplano que separe de forma \u00f3ptima a los puntos de una clase de la de otra, que eventualmente han podido ser previamente proyectados a un espacio de dimensionalidad superior. En ese concepto de \"separaci\u00f3n \u00f3ptima\" es donde reside la caracter\u00edstica fundamental de las SVM: este tipo de algoritmos buscan el hiperplano que tenga la m\u00e1xima distancia (margen) con los puntos que est\u00e9n m\u00e1s cerca de \u00e9l mismo. Por eso tambi\u00e9n a veces se les conoce a las SVM como clasificadores de margen m\u00e1ximo. De esta forma, los puntos del vector que son etiquetados con una categor\u00eda estar\u00e1n a un lado del hiperplano y los casos que se encuentren en la otra categor\u00eda estar\u00e1n al otro lado. Los algoritmos SVM pertenecen a la familia de los clasificadores lineales. Tambi\u00e9n pueden ser considerados un caso especial de la regularizaci\u00f3n de Tikhonov. En la literatura de las SVM, se llama atributo a la variable predictora y caracter\u00edstica a un atributo transformado que es usado para definir el hiperplano. La elecci\u00f3n de la representaci\u00f3n m\u00e1s adecuada del universo estudiado, se realiza mediante un proceso denominado selecci\u00f3n de caracter\u00edsticas. Al vector formado por los puntos m\u00e1s cercanos al hiperplano se le llama vector de soporte. Los modelos basados en SVM est\u00e1n estrechamente relacionados con las redes neuronales. Usando una funci\u00f3n kernel, resultan un m\u00e9todo de entrenamiento alternativo para clasificadores polinomiales, funciones de base radial y perceptr\u00f3n multicapa.","title":"Idea B\u00e1sica"},{"location":"lectures/ml/analisis_supervisado_regresion/02_modelos_regresion/#aplicando-varios-modelos-al-mismo-tiempo","text":"Veremos el performance de los distintos modelos estudiados. from sklearn import linear_model from sklearn import tree from sklearn import svm from sklearn import neighbors class SklearnRegressionModels : def __init__ ( self , model , name_model ): self . model = model self . name_model = name_model @staticmethod def test_train_model ( X , y , n_size ): X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = n_size , random_state = 42 ) return X_train , X_test , y_train , y_test def fit_model ( self , X , y , test_size ): X_train , X_test , y_train , y_test = self . test_train_model ( X , y , test_size ) return self . model . fit ( X_train , y_train ) def df_testig ( self , X , y , test_size ): X_train , X_test , y_train , y_test = self . test_train_model ( X , y , test_size ) model_fit = self . model . fit ( X_train , y_train ) preds = model_fit . predict ( X_test ) df_temp = pd . DataFrame ( { 'y' : y_test , 'yhat' : model_fit . predict ( X_test ) } ) return df_temp def metrics ( self , X , y , test_size ): df_temp = self . df_testig ( X , y , test_size ) df_metrics = summary_metrics ( df_temp ) df_metrics [ 'r2' ] = round ( r2_score ( df_temp [ 'y' ], df_temp [ 'yhat' ]), 4 ) df_metrics [ 'model' ] = self . name_model return df_metrics def parameters ( self , X , y , test_size ): model_fit = self . fit_model ( X , y , test_size ) list_betas = [ ( 'beta_0' , model_fit . intercept_ ) ] betas = model_fit . coef_ for num , beta in enumerate ( betas ): name_beta = f 'beta_ { num + 1 } ' list_betas . append (( name_beta , round ( beta , 2 ))) result = pd . DataFrame ( columns = [ 'coef' , 'value' ], data = list_betas ) result [ 'model' ] = self . name_model return result # boston dataframe X = boston . data Y = boston_df [ \"TARGET\" ] # models reg_lineal = linear_model . LinearRegression () reg_ridge = linear_model . Ridge ( alpha = .5 ) reg_lasso = linear_model . Lasso ( alpha = 0.1 ) reg_knn = neighbors . KNeighborsRegressor ( 5 , weights = 'uniform' ) reg_bayesian = linear_model . BayesianRidge () reg_tree = tree . DecisionTreeRegressor ( max_depth = 5 ) reg_svm = svm . SVR ( kernel = 'linear' ) list_models = [ [ reg_lineal , 'lineal' ], [ reg_ridge , 'ridge' ], [ reg_lasso , 'lasso' ], [ reg_knn , 'knn' ], [ reg_bayesian , 'bayesian' ], [ reg_tree , 'decision_tree' ], [ reg_svm , 'svm' ], ] frames_metrics = [] frames_coef = [] for model , name_models in list_models : fit_model = SklearnRegressionModels ( model , name_models ) frames_metrics . append ( fit_model . metrics ( X , Y , 0.2 )) if name_models in [ 'lineal' , 'ridge' , 'lasso' ]: frames_coef . append ( fit_model . parameters ( X , Y , 0.2 )) # juntar resultados: metricas pd . concat ( frames_metrics ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse mape maape wmape mmape smape r2 model 0 3.1891 24.2911 4.9286 0.1687 0.1538 0.1484 0.1579 0.1790 0.6688 lineal 0 3.1493 24.3776 4.9374 0.1668 0.1516 0.1466 0.1562 0.1769 0.6676 ridge 0 3.1452 25.1556 5.0155 0.1675 0.1512 0.1464 0.1569 0.1765 0.6570 lasso 0 3.6639 25.8601 5.0853 0.1889 0.1789 0.1705 0.1769 0.1777 0.6474 knn 0 3.1251 24.6471 4.9646 0.1654 0.1496 0.1454 0.1550 0.1748 0.6639 bayesian 0 2.3082 8.5539 2.9247 0.1309 0.1242 0.1074 0.1219 0.1233 0.8834 decision_tree 0 3.1404 29.4359 5.4255 0.1677 0.1496 0.1461 0.1568 0.1847 0.5986 svm Basados en los distintos estad\u00edsticos, el mejor modelo corresponde al modelo de decision_tree . Por otro lado, podemos analizar los coeficientes de los modelos l\u00edneales ordinarios,Ridge y Lasso. # juntar resultados: coeficientes pd . concat ( frames_coef ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef value model 0 beta_0 30.246751 lineal 1 beta_1 -0.110000 lineal 2 beta_2 0.030000 lineal 3 beta_3 0.040000 lineal 4 beta_4 2.780000 lineal 5 beta_5 -17.200000 lineal 6 beta_6 4.440000 lineal 7 beta_7 -0.010000 lineal 8 beta_8 -1.450000 lineal 9 beta_9 0.260000 lineal 10 beta_10 -0.010000 lineal 11 beta_11 -0.920000 lineal 12 beta_12 0.010000 lineal 13 beta_13 -0.510000 lineal 0 beta_0 26.891132 ridge 1 beta_1 -0.110000 ridge 2 beta_2 0.030000 ridge 3 beta_3 0.020000 ridge 4 beta_4 2.640000 ridge 5 beta_5 -12.270000 ridge 6 beta_6 4.460000 ridge 7 beta_7 -0.010000 ridge 8 beta_8 -1.380000 ridge 9 beta_9 0.250000 ridge 10 beta_10 -0.010000 ridge 11 beta_11 -0.860000 ridge 12 beta_12 0.010000 ridge 13 beta_13 -0.520000 ridge 0 beta_0 19.859769 lasso 1 beta_1 -0.100000 lasso 2 beta_2 0.030000 lasso 3 beta_3 -0.020000 lasso 4 beta_4 0.920000 lasso 5 beta_5 -0.000000 lasso 6 beta_6 4.310000 lasso 7 beta_7 -0.020000 lasso 8 beta_8 -1.150000 lasso 9 beta_9 0.240000 lasso 10 beta_10 -0.010000 lasso 11 beta_11 -0.730000 lasso 12 beta_12 0.010000 lasso 13 beta_13 -0.560000 lasso Al comparar los resultados entre ambos modelos, se observa que hay coeficientes en la regresi\u00f3n Lasso que se van a cero directamente, pudiendo eliminar estas variables del modelo. Por otro lado, queda como tarea para el lector, hacer una eliminaci\u00f3n de outliers del modelo y probar estos modelos lineales para ver si existe alg\u00fan tipo de diferencia.","title":"Aplicando varios modelos al mismo tiempo"},{"location":"lectures/ml/analisis_supervisado_regresion/02_modelos_regresion/#conclusion","text":"Existen distintos modelos de regresi\u00f3n lineal: normal, Ridge y Lasso. Cada uno con sus respectivs ventajas y desventajas. Existen otros tipos de modelos de regresi\u00f3n (bayesiano, knn, arboles de decisi\u00f3n, svm, entre otros). Por ahora, nos interesa saber como funcionan, para poder configurar los hiperpar\u00e1metros de los modelos ocupados en python (principalmente de la librer\u00eda sklearn ). En el mundo del machine learning se estar\u00e1 interesado m\u00e1s en predecir con el menor error posible (siempre tomando como referencia alguna de las m\u00e9tricas mencionadas) que hacer un an\u00e1lisis exhaustivo de la confiabilidad del modelo. Siendo este el caso y si la capacidad computacional lo permite, lo ideal es probar varios modelos al mismo tiempo y poder discriminar bajo un determinado criterio (a menudo el error cuadr\u00e1tico medio (rmse) o el mape ).","title":"Conclusi\u00f3n"},{"location":"lectures/ml/analisis_supervisado_regresion/02_modelos_regresion/#referencia","text":"Supervised learning","title":"Referencia"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/","text":"Modelos de regressi\u00f3n lineal Definici\u00f3n El modelo de regresio\u0301n lineal general o modelo de regresi\u00f3n multiple , supone que, \\(\\boldsymbol{Y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon},\\) donde: \\(\\boldsymbol{X} = (x_1,...,x_n)^{T}\\) : variable explicativa \\(\\boldsymbol{Y} = (y_1,...,y_n)^{T}\\) : variable respuesta \\(\\boldsymbol{\\epsilon} = (\\epsilon_1,...,\\epsilon_n)^{T}\\) : error se asume un ruido blanco, es decir, \\(\\epsilon \\sim \\mathcal{N}( \\boldsymbol{0},\\sigma^2I)\\) \\(\\boldsymbol{\\beta} = (\\beta_1,...,\\beta_n)^{T}\\) : coeficientes de regresio\u0301n. La idea es tratar de establecer la relaci\u00f3n entre las variables independientes y dependientes por medio de ajustar el mejor hyper plano con respecto a los puntos. Por ejemplo, para el caso de la regresi\u00f3n lineal simple , se tiene la siguiente estructura: \\(y_i=\\beta_0+\\beta_1x_i+\\epsilon_i.\\) En este caso, la regresi\u00f3n lineal corresponder\u00e1 a la recta que mejor pasa por los puntos observados. Existen algunas situaciones donde los modelos lineales no son apropiados: El rango de valores de \\(Y\\) est\u00e1 restringido (ejemplo: datos binarios o de conteos). La varianza de \\(Y\\) depende de la media. Mejores par\u00e9metros: M\u00e9todo de minimos cudrados El m\u00e9todo de m\u00ednimos cudrados es un m\u00e9todo de optimizaci\u00f3n que busca encontrar la mejor aproximaci\u00f3n mediante la minimizaci\u00f3n de los residuos al cuadrado, es decir, se buscar encontrar: \\[(P)\\ \\min \\sum_{i=1}^n e_{i}^2 =\\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2 \\] Para el caso de la regresi\u00f3n lineal simple, se busca una funci\u00f3n \\( \\(f(x;\\beta) = \\beta_{0} + \\beta_{1}x,\\) \\) por lo tanto el problema que se debe resolver es el siguiente: \\[(P)\\ \\min \\sum_{i=1}^n e_{i}^2 =\\dfrac{1}{n}\\sum_{i=1}^{n}\\left ( y_{i}-(\\beta_{0} + \\beta_{1}x_{i})\\right )^2\\] Lo que significa, que para este problema, se debe encontrar \\(\\beta = (\\beta_{0},\\beta_{1})\\) que minimicen el problema de optimizaci\u00f3n. En este caso la soluci\u00f3n viene dada por: \\[\\hat{\\beta}_{1} = \\dfrac{\\sum(x-\\bar{x})(y-\\bar{y})}{\\sum(x-\\bar{x})^2} = \\rho (x,y)\\ ; \\ \\hat{\\beta}_{0} = \\bar{y}-\\hat{\\beta}_{1} \\bar{x} \\] La metodolog\u00eda para encontrar los par\u00e1metros \\(\\beta\\) para el caso de la regresi\u00f3n lineal multiple se extienden de manera natural del modelo de regresi\u00f3n lineal multiple, cuya soluci\u00f3n viene dada por: \\[\\beta = (XX^{\\top})^{-1}X^{\\top}y\\] Selecci\u00f3n de modelos Criterio de informaci\u00f3n de Akaike (AIC) El criterio de informaci\u00f3n de Akaike (AIC) es una medida de la calidad relativa de un modelo estad\u00edstico, para un conjunto dado de datos. Como tal, el AIC proporciona un medio para la selecci\u00f3n del modelo. AIC maneja un trade-off entre la bondad de ajuste del modelo y la complejidad del modelo. Se basa en la entrop\u00eda de informaci\u00f3n: se ofrece una estimaci\u00f3n relativa de la informaci\u00f3n perdida cuando se utiliza un modelo determinado para representar el proceso que genera los datos. AIC no proporciona una prueba de un modelo en el sentido de probar una hip\u00f3tesis nula, es decir AIC no puede decir nada acerca de la calidad del modelo en un sentido absoluto. Si todos los modelos candidatos encajan mal, AIC no dar\u00e1 ning\u00fan aviso de ello. En el caso general, el AIC es \\[AIC = 2k-2\\ln(L)\\] donde \\(k\\) es el n\u00famero de par\u00e1metros en el modelo estad\u00edstico , y \\(L\\) es el m\u00e1ximo valor de la funci\u00f3n de verosimilitud para el modelo estimado. Criterio de informaci\u00f3n bayesiano (BIC) En estad\u00edstica, el criterio de informaci\u00f3n bayesiano (BIC) o el m\u00e1s general criterio de Schwarz (SBC tambi\u00e9n, SBIC) es un criterio para la selecci\u00f3n de modelos entre un conjunto finito de modelos. Se basa, en parte, de la funci\u00f3n de probabilidad y que est\u00e1 estrechamente relacionado con el Criterio de Informaci\u00f3n de Akaike (AIC). Cuando el ajuste de modelos, es posible aumentar la probabilidad mediante la adici\u00f3n de par\u00e1metros, pero si lo hace puede resultar en sobreajuste. Tanto el BIC y AIC resuelven este problema mediante la introducci\u00f3n de un t\u00e9rmino de penalizaci\u00f3n para el n\u00famero de par\u00e1metros en el modelo, el t\u00e9rmino de penalizaci\u00f3n es mayor en el BIC que en el AIC. El BIC fue desarrollado por Gideon E. Schwarz, quien dio un argumento bayesiano a favor de su adopci\u00f3n.1\u200b Akaike tambi\u00e9n desarroll\u00f3 su propio formalismo Bayesiano, que ahora se conoce como la ABIC por Criterio de Informaci\u00f3n Bayesiano de Akaike \" En el caso general, el BIC es \\[BIC =k\\ln(n)-2\\ln(L)\\] donde \\(k\\) es el n\u00famero de par\u00e1metros en el modelo estad\u00edstico, \\(n\\) es la cantidad de datos disponibles y \\(L\\) es el m\u00e1ximo valor de la funci\u00f3n de verosimilitud para el modelo estimado. R-cuadrado El coeficiente de determinaci\u00f3n o R-cuadrado ( \\(r^2\\) ) , es un estad\u00edstico usado en el contexto de un modelo estad\u00edstico cuyo principal prop\u00f3sito es predecir futuros resultados o probar una hip\u00f3tesis. El coeficiente determina la calidad del modelo para replicar los resultados, y la proporci\u00f3n de variaci\u00f3n de los resultados que puede explicarse por el modelo. El valor del \\(r^2\\) habitualmente entre 0 y 1, donde 0 significa una mala calidad de ajuste en el modelo y 1 corresponde a un ajuste lineal perfecto. A menudo, este estad\u00edstico es ocupado para modelos lineales. Se define por la f\u00f3rmula: \\[r^2 = \\dfrac{SS_{reg}}{SS_{tot}} = 1 - \\dfrac{SS_{res}}{SS_{tot}},\\] donde: \\(SS_{reg}\\) ( suma explicada de cuadrados (ESS)): \\(\\sum_{i}(\\hat{y}-\\bar{y})^2\\) \\(SS_{res}\\) : ( suma residual de cuadrados (RSS)): \\(\\sum_{i}(y_{i}-\\hat{y})^2 = \\sum_{i}e_{i}^2\\) \\(SS_{tot}\\) : ( varianza): \\(\\sum_{i}(y_{i}-\\bar{y})\\) , donde: \\(SS_{tot}=SS_{reg}+SS_{res}\\) En una forma general, se puede ver que \\(r^2\\) est\u00e1 relacionado con la fracci\u00f3n de varianza inexplicada (FVU), ya que el segundo t\u00e9rmino compara la varianza inexplicada (varianza de los errores del modelo) con la varianza total (de los datos). Las \u00e1reas de los cuadrados azules representan los residuos cuadrados con respecto a la regresi\u00f3n lineal ( \\(SS_{tot}\\) ). Las \u00e1reas de los cuadrados rojos representan los residuos al cuadrado con respecto al valor promedio ( \\(SS_{res}\\) ). Por otro lado, a medida que m\u00e1s variables explicativas se agregan al modelo, el \\(r^2\\) aumenta de forma autom\u00e1tica, es decir, entre m\u00e1s variables explicativas se agreguen, mejor ser\u00e1 la calidad ser\u00e1 el ajuste (un falso argumento). Es por ello que se define el R cuadrado ajustado , que viene a ser una modificaci\u00f3n del \\(r^2\\) , ajustando por el n\u00famero de variables explicativas en un modelo ( \\(p\\) ) en relaci\u00f3n con el n\u00famero de puntos de datos ( \\(n\\) ). \\[r^2_{ajustado} = 1-(1-r^2)\\dfrac{n-1}{n-p-1} ,\\] Error de un modelo Definici\u00f3n El error corresponde a la diferencia entre el valor original y el valor predicho,es decir: \\[e_{i}=y_{i}-\\hat{y}_{i} \\] Formas de medir el error de un modelo Para medir el ajuste de un modelo se ocupan las denominadas funciones de distancias o m\u00e9tricas . Existen varias m\u00e9tricas, dentro de las cuales encontramos: M\u00e9tricas absolutas : Las m\u00e9tricas absolutas o no escalada miden el error sin escalar los valores. Las m\u00e9trica absolutas m\u00e1s ocupadas son: Mean Absolute Error (MAE) \\[\\textrm{MAE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n}\\left | y_{t}-\\hat{y}_{t}\\right |\\] Mean squared error (MSE): \\[\\textrm{MSE}(y,\\hat{y}) =\\dfrac{1}{n}\\sum_{t=1}^{n}\\left ( y_{t}-\\hat{y}_{t}\\right )^2\\] M\u00e9tricas Porcentuales : Las m\u00e9tricas porcentuales o escaladas miden el error de manera escalada, es decir, se busca acotar el error entre valores de 0 a 1, donde 0 significa que el ajuste es perfecto, mientras que 1 ser\u00eda un mal ajuste. Cabe destacar que muchas veces las m\u00e9tricas porcentuales puden tener valores mayores a 1.Las m\u00e9trica Porcentuales m\u00e1s ocupadas son: Mean absolute percentage error (MAPE): \\[\\textrm{MAPE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n}\\left | \\frac{y_{t}-\\hat{y}_{t}}{y_{t}} \\right |\\] Symmetric mean absolute percentage error (sMAPE): \\[\\textrm{sMAPE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n} \\frac{\\left |y_{t}-\\hat{y}_{t}\\right |}{(\\left | y_{t} \\right |^2+\\left | \\hat{y}_{t} \\right |^2)/2}\\] Otros estad\u00edsticos interesantes del modelo Test F EL test F para regresi\u00f3n lineal prueba si alguna de las variables independientes en un modelo de regresi\u00f3n lineal m\u00faltiple es significativa. En t\u00e9rminos de test de hip\u00f3tesis, se quiere contrastar lo siguiente: \\(H_0: \\beta_1 = \\beta_2 = ... = \\beta_{p-1} = 0\\) \\(H_1: \\beta_j \u2260 0\\) , para al menos un valor de \\(j\\) Test Omnibus EL test Omnibus esta relacionado con la simetr\u00eda y curtosis del resido. Se espera ver un valor cercano a cero que indicar\u00eda normalidad. El Prob (Omnibus) realiza una prueba estad\u00edstica que indica la probabilidad de que los residuos se distribuyan normalmente. Test Durbin-Watson El Test Durbin-Watson es un test de homocedasticidad. Para ver los l\u00edmites relacionados de este test, se puede consultar la siguiente tablas de valores . Test Jarque-Bera Como el test Omnibus en que prueba tanto el sesgo como la curtosis. Esperamos ver en esta prueba una confirmaci\u00f3n de la prueba Omnibus. IMPORTANTE : Cabe destacar que el coeficiente \\(r^2\\) funciona bien en el contexto del mundo de las regresiones lineales. Para el an\u00e1lisis de modelos no lineales , esto coeficiente pierde su interpretaci\u00f3n. Se deja la siguiente refrerencia para comprender conceptos claves de test de hip\u00f3tesis, intervalos de confianza, p-valor. Estos t\u00e9rminos son escenciales para comprender la significancia del ajuste realizado. Existen muchas m\u00e1s m\u00e9tricas, pero estas son las m\u00e1s usulaes de encontrar. En el archivo metrics.py se definen las distintas m\u00e9tricas presentadas, las cuales serpan de utilidad m\u00e1s adelante. Aplicaci\u00f3n con python Ejemplo sencillo Para comprender los modelos de regresi\u00f3n lineal, mostraremos un caso sencillo de uso. Para ello realizaremos un simulaci\u00f3n de una recta, en el cual le agregaremos un ruido blanco. # librerias import os import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns pd . set_option ( 'display.max_columns' , 500 ) # Ver m\u00e1s columnas de los dataframes # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab % matplotlib inline # ejemplo sencillo n = 100 np . random . seed ( n ) beta = np . array ([ 1 , 1 ]) # coeficientes x = np . random . rand ( n ) # variable independiente mu , sigma = 0 , 0.1 # media y desviacion estandar epsilon = np . random . normal ( mu , sigma , n ) # ruido blanco y = np . dot ( np . c_ [ np . ones ( n ), x ] , beta ) + epsilon # variables dependientes # generar dataframe df = pd . DataFrame ({ 'x' : x , 'y' : y }) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y 0 0.543405 1.612417 1 0.278369 1.347058 2 0.424518 1.267849 3 0.844776 1.935274 4 0.004719 1.082601 Grafiquemos los puntos en el plano cartesiano. # grafico de puntos sns . set ( rc = { 'figure.figsize' :( 10 , 8 )}) sns . scatterplot ( x = 'x' , y = 'y' , data = df , ) plt . show () Lo primero que debemos hacer es separar nuestro datos en los conjuntos de training set y test set . Concepto de Train set y Test set Al momento de entrenar los modelos de machine leraning, se debe tener un conjunto para poder entrenar el modelo y otro conjunto para poder evaluar el modelo. Es por esto que el conjunto de datos se separ\u00e1 en dos conjuntos: Train set : Conjunto de entrenamiento con el cual se entrenar\u00e1n los algoritmos de machine learning. Test set : Conjunto de testeo para averiguar la confiabilidad del modelo, es decir, cuan bueno es el ajuste del modelo. Tama\u00f1o ideal de cada conjunto La respuesta depende fuertemente del tama\u00f1o del conjunto de datos. A modo de regla emp\u00edrica, se considerar\u00e1 el tama\u00f1o \u00f3ptimo basado en la siguiente tabla: n\u00famero de filas train set test set entre 100-1000 67% 33% entre 1.000- 100.000 80% 20% mayor a 100.000 99% 1% from sklearn import datasets from sklearn.model_selection import train_test_split # import some data to play with X = df [[ 'x' ]] # we only take the first two features. y = df [ 'y' ] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # print rows train and test sets print ( 'Separando informacion: \\n ' ) print ( 'numero de filas data original : ' , len ( X )) print ( 'numero de filas train set : ' , len ( X_train )) print ( 'numero de filas test set : ' , len ( X_test )) Separando informacion: numero de filas data original : 100 numero de filas train set : 80 numero de filas test set : 20 Existen varias librer\u00edas para poder aplicar modelos de regresi\u00f3n, de los cuales la atenci\u00f3n estar\u00e1 enfocada en las librer\u00edas de statsmodels y sklearn . Ejemplo con Statsmodel Para trabajar los modelos de statsmodel , basta con instanciar el comando OLS . El modelo no considera intercepto , por lo tanto, para agregar el intercepto, a las variables independientes se le debe agregar un vector de unos (tanto para el conjunto de entranamiento como de testeo). import statsmodels.api as sm model = sm . OLS ( y_train , sm . add_constant ( X_train )) results = model . fit () En statsmodel existe un comando para ver informaci\u00f3n del modelo en estudio mediante el comando summary # resultados del modelo print ( results . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.894 Model: OLS Adj. R-squared: 0.893 Method: Least Squares F-statistic: 658.4 Date: Fri, 12 Mar 2021 Prob (F-statistic): 8.98e-40 Time: 15:12:04 Log-Likelihood: 69.472 No. Observations: 80 AIC: -134.9 Df Residuals: 78 BIC: -130.2 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.9805 0.021 46.338 0.000 0.938 1.023 x 1.0099 0.039 25.659 0.000 0.932 1.088 ============================================================================== Omnibus: 0.424 Durbin-Watson: 1.753 Prob(Omnibus): 0.809 Jarque-Bera (JB): 0.587 Skew: 0.102 Prob(JB): 0.746 Kurtosis: 2.633 Cond. No. 4.17 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. A continuaci\u00f3n se dara una interpretaci\u00f3n de esta tabla: Descripci\u00f3n del Modelo Estos son estad\u00edsticas relacionadas a la ejecuci\u00f3n del modelo. Variable Descripi\u00f3n Dep. Variable Nombre de la variables dependiente Model Nombre del modelo ocupado Method M\u00e9todo para encontrar los par\u00e1metros \u00f3ptimos Date Fecha de ejecuci\u00f3n No. Observations N\u00famero de observaciones Df Residuals Grados de libertas de los residuos Df Model Grados de libertad del modelo Covariance Type Tipo de covarianza Ajustes del Modelo Estos son estad\u00edsticas relacionadas con la verosimilitud y la confiabilidad del modelo. Variable Descripi\u00f3n R-squared Valor del R-cuadrado Adj. R-squared Valor del R-cuadrado ajustado F-statistic Test para ver si todos los par\u00e1metros son iguales a cero Prob (F-statistic) Probabilidad Asociada al test Log-Likelihood Logaritmo de la funci\u00f3n de verosimilitud AIC Valor del estad\u00edstico AIC BIC Valor del estad\u00edstico BIC En este caso, tanto el r-cuadrado como el r-cuadrado ajustado est\u00e1n cerca del 0.9, se tiene un buen ajuste lineal de los datos. Adem\u00e1s, el test F nos da una probabilidad menor al 0.05, se rechaza la hip\u00f3tess nula que los coeficientes son iguales de cero. Par\u00e1metros del modelo La tabla muestra los valores asociados a los par\u00e1metros del modelo coef std err t P>|t| [0.025 0.975] const 0.9805 0.021 46.338 0.000 0.938 1.023 x 1.0099 0.039 25.659 0.000 0.932 1.088 Ac\u00e1 se tiene: Variables : Las variables en estudio son const (intercepto) y x . coef : Valor estimado del coeficiente. std err : Desviaci\u00f3n estandar del estimador. t : t = estimate/std error. P>|t| :p-valor individual para cada par\u00e1metro para aceptar o rechazar hip\u00f3tesis nula (par\u00e1metros significativamente distinto de cero). [0.025 | 0.975] : Intervalo de confianza de los par\u00e1metros En este caso, los valores estimados son cercanos a 1 (algo esperable debido a la simulaci\u00f3n realizadas), adem\u00e1s, se observa que cada uno de los par\u00e1metros es significativamente distinto de cero. Estad\u00edsticos interesantes del modelo Variable Descripci\u00f3n Omnibus Prueba de la asimetr\u00eda y curtosis de los residuos Prob(Omnibus) Probabilidad de que los residuos se distribuyan normalmente Skew Medida de simetr\u00eda de los datos Kurtosis Medida de curvatura de los datos Durbin-Watson Pruebas de homocedasticidad Jarque-Bera (JB) Como la prueba Omnibus, prueba tanto el sesgo como la curtosis. Prob(JB) Probabilidad de que los residuos se distribuyan normalmente Cond. No. N\u00famero de condici\u00f3n. Mide la sensibilidad de la salida de una funci\u00f3n en comparaci\u00f3n con su entrada En este caso: Tanto el test de Omnibus como el test Jarque-Bera nos arroja una probabilidad cercana a uno, lo cual confirma la hip\u00f3tesis que los residuos se distribuyen de manera normal. Para el test de Durbin-Watson, basados en la tablas de valores (tama\u00f1o de la muestra 80 y n\u00famero de variables 2), se tiene que los l\u00edmites para asumir que no existe correlaci\u00f3n en los residuos es de: \\([d_u,4-d_u]=[1.66,2.34]\\) , dado que el valor obtenido (1.753) se encuentra dentro de este rango, se concluye que no hay autocorrelaci\u00f3n de los residuos. El n\u00famero de condici\u00f3n es peque\u00f1o (podemos asumir que menor a 30 es un buen resultado) por lo que podemos asumir que no hay colinealidad de los datos. Ahora, para convencernos de manera visual de los resultados, realicemos un gr\u00e1fico con el ajuste lineal: # grafico de puntos sns . lmplot ( x = 'x' , y = 'y' , data = df , height = 8 , ) plt . show () An\u00e1lisis del error Predicciones Ahora que ya se tiene el modelo entrenado y se ha analizado sus principales caracter\u00edsticas, se pueden realizar predicciones de los valores que se desconocen, de la siguiente manera: # predicciones y_pred = results . predict ( sm . add_constant ( X_test )) Ahora, analizaremos las m\u00e9tricas de error asociado a las predicciones del modelo: from metrics_regression import * from sklearn.metrics import r2_score # ejemplo df_temp = pd . DataFrame ( { 'y' : y_test , 'yhat' : y_pred } ) print ( ' \\n Metricas para el regresor consumo_litros_milla: \\n ' ) summary_metrics ( df_temp ) Metricas para el regresor consumo_litros_milla: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse mape maape wmape mmape smape 0 0.1028 0.0171 0.1309 0.0677 0.0674 0.0666 0.0406 0.068 Normalidad de los residuos Basados en los distintos test (Durbin-Watson,Omnibus,Jarque-Bera ) se concluye que los residuos del modelo son un ruido blanco. Para convencernos de esto de manera gr\u00e1fica, se realizan los siguientes gr\u00e1ficos de inter\u00e9s. Funci\u00f3n de Autocorrelaci\u00f3n La funci\u00f3n de autocorrelaci\u00f3n muestra que los residuos se encuentra dentro de la banda de valores cr\u00edticos \\((-0.2,0.2)\\) , concluyendo que no existe correlaci\u00f3n entre los residuos. from statsmodels.graphics.tsaplots import plot_acf sns . set ( rc = { 'figure.figsize' :( 12 , 8 )}) # funcion de autocorrelation plot_acf ( results . resid ) plt . show () QQ-plot La gr\u00e1fica de qq-plot nos muestra una comparaci\u00f3n en las distribuci\u00f3n de los residuos respecto a una poblaci\u00f3n con una distribuci\u00f3n normal. En este caso, los puntos (que representan la distribuci\u00f3n de los errores) se encuentran cercana a la recta (distribuci\u00f3n normal), concluyendo que la distribuci\u00f3n de los residuos sigue una distribuci\u00f3n normal. import scipy.stats as stats fig = sm . qqplot ( results . resid , stats . t , fit = True , line = \"45\" ) plt . show () Histograma Esta es una comparaci\u00f3n directa enntre la distribuci\u00f3n de los residuos versus la distribuci\u00f3n de una variable normal mediante un histograma. df_hist = pd . DataFrame ({ 'error' : results . resid }) sns . histplot ( x = 'error' , data = df_hist , kde = True , bins = 15 ) plt . show () A modo de conclusi\u00f3n, es correcto asumir que los errores siguen la distribuci\u00f3n de un ruido blanco, cumpliendo correctamente con los supuestos de la regresi\u00f3n lineal. Outliers Un outlier (o valor at\u00edpico) una observaci\u00f3n que es num\u00e9ricamente distante del resto de los datos. Las estad\u00edsticas derivadas de los conjuntos de datos que incluyen valores at\u00edpicos ser\u00e1n frecuentemente enga\u00f1osas. Estos valores pueden afectar fuertemente al modelo de regresi\u00f3n log\u00edstica. Veamos un ejemplo: # ejemplo sencillo n = 100 np . random . seed ( n ) beta = np . array ([ 1 , 1 ]) # coeficientes x = np . random . rand ( n ) # variable independiente mu , sigma = 0 , 0.1 # media y desviacion estandar epsilon = np . random . normal ( mu , sigma , n ) # ruido blanco y = np . dot ( np . c_ [ np . ones ( n ), x ] , beta ) + epsilon # variables dependientes y [: 10 ] = 3.1 # contaminacion x [ 10 ] = x [ 10 ] - 1 y [ 10 ] = y [ 10 ] - 1 x [ 11 ] = x [ 11 ] + 1 y [ 11 ] = y [ 11 ] + 1 # etiqueta outlier = np . zeros ( n ) outlier [: 10 ] = 1 outlier [ 10 : 12 ] = 2 # generar dataframe df = pd . DataFrame ({ 'x' : x , 'y' : y , 'outlier' : outlier }) # grafico de puntos sns . set ( rc = { 'figure.figsize' :( 10 , 8 )}) sns . scatterplot ( x = 'x' , y = 'y' , hue = 'outlier' , data = df , palette = [ 'blue' , 'red' , 'black' ] ) plt . show () plt . show () En este caso, se tiene dos tipos de outliers en este caso: Significativos : Aquellos outliers que afectan la regresi\u00f3n cambiando la tendencia a este grupo de outliers (puntos rojos). No significativo : Si bien son datos at\u00edpicos puesto que se encuentran fuera de la nube de puntos, el ajuste de la regresi\u00f3n lineal no se ve afectado (puntos negros). Veamos el ajuste lineal. # grafico de puntos sns . lmplot ( x = 'x' , y = 'y' , data = df , height = 8 , ) plt . show () Otro gr\u00e1fico de inter\u00e9s, es el gr\u00e1fico de influencia , que analiza la distancia de Cook de los residuos. # modelos de influencia X = df [[ 'x' ]] # we only take the first two features. y = df [ 'y' ] model = sm . OLS ( y , sm . add_constant ( X )) results = model . fit () sm . graphics . influence_plot ( results ) plt . show () Los puntos grandes se interpretan como puntos que tienen una alta influencia sobre la regresi\u00f3n lineal, mientras aquellos puntos peque\u00f1os tienen una influencia menor. \u00bf Qu\u00e9 hacer ante la presencia de outliers? En este caso, la recta se ve fuertemente afectadas por estos valores. Para estos casos se pueden hacer varias cosas: Eliminaci\u00f3n de los outliers : Una vez identificado los outliers (algo que no es tan trivial de identificar para datos multivariables), se puden eliminar y seguir con el paso de modelado. Ventajas : F\u00e1cil de trabajar la data para los modelos que dependen fuertemente de la media de los datos. Desventajas : Para el caso multivariables no es t\u00e1n trivial encontrar outliers. Modelos m\u00e1s robustos a outliers : Se pueden aplicar otros modelos de regresi\u00f3n cuya estimaci\u00f3n de los par\u00e1metros, no se vea afectado por los valores de outliers. Ventajas : El an\u00e1lisis se vuelve independiente de los datos. Desventajas : Modelos m\u00e1s costoso computacionalmente y/o m\u00e1s complejos de implementar. Conclusi\u00f3n Los modelos de regresi\u00f3n lineal son una gran herramienta para realizar predicciones. Los outliers afectan considerablemente a la regresi\u00f3n lineal, por lo que se debn buscar estrategias para abordar esta problem\u00e1tica. En esta oportunidad se hizo un detalle t\u00e9cnico de disntintos est\u00e1disticos asociados a la regresi\u00f3n l\u00edneal (apuntando a un an\u00e1lisis inferencial ), no obstante, en los pr\u00f3ximos modelos, se estar\u00e1 interesado en analizar las predicciones del modelo y los errores asociados a ella, por lo cual los aspectos t\u00e9cnico quedar\u00e1n como lecturas complementarias. Existen varios casos donde los modelos de regresi\u00f3n l\u00edneal no realizan un correcto ajuste de los datos, pero es una gran herramienta para comenzar. Referencia Linear Regression in Python","title":"Regresi\u00f3n"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#modelos-de-regression-lineal","text":"","title":"Modelos de regressi\u00f3n lineal"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#definicion","text":"El modelo de regresio\u0301n lineal general o modelo de regresi\u00f3n multiple , supone que, \\(\\boldsymbol{Y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon},\\) donde: \\(\\boldsymbol{X} = (x_1,...,x_n)^{T}\\) : variable explicativa \\(\\boldsymbol{Y} = (y_1,...,y_n)^{T}\\) : variable respuesta \\(\\boldsymbol{\\epsilon} = (\\epsilon_1,...,\\epsilon_n)^{T}\\) : error se asume un ruido blanco, es decir, \\(\\epsilon \\sim \\mathcal{N}( \\boldsymbol{0},\\sigma^2I)\\) \\(\\boldsymbol{\\beta} = (\\beta_1,...,\\beta_n)^{T}\\) : coeficientes de regresio\u0301n. La idea es tratar de establecer la relaci\u00f3n entre las variables independientes y dependientes por medio de ajustar el mejor hyper plano con respecto a los puntos. Por ejemplo, para el caso de la regresi\u00f3n lineal simple , se tiene la siguiente estructura: \\(y_i=\\beta_0+\\beta_1x_i+\\epsilon_i.\\) En este caso, la regresi\u00f3n lineal corresponder\u00e1 a la recta que mejor pasa por los puntos observados. Existen algunas situaciones donde los modelos lineales no son apropiados: El rango de valores de \\(Y\\) est\u00e1 restringido (ejemplo: datos binarios o de conteos). La varianza de \\(Y\\) depende de la media.","title":"Definici\u00f3n"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#mejores-paremetros-metodo-de-minimos-cudrados","text":"El m\u00e9todo de m\u00ednimos cudrados es un m\u00e9todo de optimizaci\u00f3n que busca encontrar la mejor aproximaci\u00f3n mediante la minimizaci\u00f3n de los residuos al cuadrado, es decir, se buscar encontrar: \\[(P)\\ \\min \\sum_{i=1}^n e_{i}^2 =\\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2 \\] Para el caso de la regresi\u00f3n lineal simple, se busca una funci\u00f3n \\( \\(f(x;\\beta) = \\beta_{0} + \\beta_{1}x,\\) \\) por lo tanto el problema que se debe resolver es el siguiente: \\[(P)\\ \\min \\sum_{i=1}^n e_{i}^2 =\\dfrac{1}{n}\\sum_{i=1}^{n}\\left ( y_{i}-(\\beta_{0} + \\beta_{1}x_{i})\\right )^2\\] Lo que significa, que para este problema, se debe encontrar \\(\\beta = (\\beta_{0},\\beta_{1})\\) que minimicen el problema de optimizaci\u00f3n. En este caso la soluci\u00f3n viene dada por: \\[\\hat{\\beta}_{1} = \\dfrac{\\sum(x-\\bar{x})(y-\\bar{y})}{\\sum(x-\\bar{x})^2} = \\rho (x,y)\\ ; \\ \\hat{\\beta}_{0} = \\bar{y}-\\hat{\\beta}_{1} \\bar{x} \\] La metodolog\u00eda para encontrar los par\u00e1metros \\(\\beta\\) para el caso de la regresi\u00f3n lineal multiple se extienden de manera natural del modelo de regresi\u00f3n lineal multiple, cuya soluci\u00f3n viene dada por: \\[\\beta = (XX^{\\top})^{-1}X^{\\top}y\\]","title":"Mejores par\u00e9metros: M\u00e9todo de minimos cudrados"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#seleccion-de-modelos","text":"","title":"Selecci\u00f3n de modelos"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#criterio-de-informacion-de-akaike-aic","text":"El criterio de informaci\u00f3n de Akaike (AIC) es una medida de la calidad relativa de un modelo estad\u00edstico, para un conjunto dado de datos. Como tal, el AIC proporciona un medio para la selecci\u00f3n del modelo. AIC maneja un trade-off entre la bondad de ajuste del modelo y la complejidad del modelo. Se basa en la entrop\u00eda de informaci\u00f3n: se ofrece una estimaci\u00f3n relativa de la informaci\u00f3n perdida cuando se utiliza un modelo determinado para representar el proceso que genera los datos. AIC no proporciona una prueba de un modelo en el sentido de probar una hip\u00f3tesis nula, es decir AIC no puede decir nada acerca de la calidad del modelo en un sentido absoluto. Si todos los modelos candidatos encajan mal, AIC no dar\u00e1 ning\u00fan aviso de ello. En el caso general, el AIC es \\[AIC = 2k-2\\ln(L)\\] donde \\(k\\) es el n\u00famero de par\u00e1metros en el modelo estad\u00edstico , y \\(L\\) es el m\u00e1ximo valor de la funci\u00f3n de verosimilitud para el modelo estimado.","title":"Criterio de informaci\u00f3n de Akaike (AIC)"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#criterio-de-informacion-bayesiano-bic","text":"En estad\u00edstica, el criterio de informaci\u00f3n bayesiano (BIC) o el m\u00e1s general criterio de Schwarz (SBC tambi\u00e9n, SBIC) es un criterio para la selecci\u00f3n de modelos entre un conjunto finito de modelos. Se basa, en parte, de la funci\u00f3n de probabilidad y que est\u00e1 estrechamente relacionado con el Criterio de Informaci\u00f3n de Akaike (AIC). Cuando el ajuste de modelos, es posible aumentar la probabilidad mediante la adici\u00f3n de par\u00e1metros, pero si lo hace puede resultar en sobreajuste. Tanto el BIC y AIC resuelven este problema mediante la introducci\u00f3n de un t\u00e9rmino de penalizaci\u00f3n para el n\u00famero de par\u00e1metros en el modelo, el t\u00e9rmino de penalizaci\u00f3n es mayor en el BIC que en el AIC. El BIC fue desarrollado por Gideon E. Schwarz, quien dio un argumento bayesiano a favor de su adopci\u00f3n.1\u200b Akaike tambi\u00e9n desarroll\u00f3 su propio formalismo Bayesiano, que ahora se conoce como la ABIC por Criterio de Informaci\u00f3n Bayesiano de Akaike \" En el caso general, el BIC es \\[BIC =k\\ln(n)-2\\ln(L)\\] donde \\(k\\) es el n\u00famero de par\u00e1metros en el modelo estad\u00edstico, \\(n\\) es la cantidad de datos disponibles y \\(L\\) es el m\u00e1ximo valor de la funci\u00f3n de verosimilitud para el modelo estimado.","title":"Criterio de informaci\u00f3n bayesiano (BIC)"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#r-cuadrado","text":"El coeficiente de determinaci\u00f3n o R-cuadrado ( \\(r^2\\) ) , es un estad\u00edstico usado en el contexto de un modelo estad\u00edstico cuyo principal prop\u00f3sito es predecir futuros resultados o probar una hip\u00f3tesis. El coeficiente determina la calidad del modelo para replicar los resultados, y la proporci\u00f3n de variaci\u00f3n de los resultados que puede explicarse por el modelo. El valor del \\(r^2\\) habitualmente entre 0 y 1, donde 0 significa una mala calidad de ajuste en el modelo y 1 corresponde a un ajuste lineal perfecto. A menudo, este estad\u00edstico es ocupado para modelos lineales. Se define por la f\u00f3rmula: \\[r^2 = \\dfrac{SS_{reg}}{SS_{tot}} = 1 - \\dfrac{SS_{res}}{SS_{tot}},\\] donde: \\(SS_{reg}\\) ( suma explicada de cuadrados (ESS)): \\(\\sum_{i}(\\hat{y}-\\bar{y})^2\\) \\(SS_{res}\\) : ( suma residual de cuadrados (RSS)): \\(\\sum_{i}(y_{i}-\\hat{y})^2 = \\sum_{i}e_{i}^2\\) \\(SS_{tot}\\) : ( varianza): \\(\\sum_{i}(y_{i}-\\bar{y})\\) , donde: \\(SS_{tot}=SS_{reg}+SS_{res}\\) En una forma general, se puede ver que \\(r^2\\) est\u00e1 relacionado con la fracci\u00f3n de varianza inexplicada (FVU), ya que el segundo t\u00e9rmino compara la varianza inexplicada (varianza de los errores del modelo) con la varianza total (de los datos). Las \u00e1reas de los cuadrados azules representan los residuos cuadrados con respecto a la regresi\u00f3n lineal ( \\(SS_{tot}\\) ). Las \u00e1reas de los cuadrados rojos representan los residuos al cuadrado con respecto al valor promedio ( \\(SS_{res}\\) ). Por otro lado, a medida que m\u00e1s variables explicativas se agregan al modelo, el \\(r^2\\) aumenta de forma autom\u00e1tica, es decir, entre m\u00e1s variables explicativas se agreguen, mejor ser\u00e1 la calidad ser\u00e1 el ajuste (un falso argumento). Es por ello que se define el R cuadrado ajustado , que viene a ser una modificaci\u00f3n del \\(r^2\\) , ajustando por el n\u00famero de variables explicativas en un modelo ( \\(p\\) ) en relaci\u00f3n con el n\u00famero de puntos de datos ( \\(n\\) ). \\[r^2_{ajustado} = 1-(1-r^2)\\dfrac{n-1}{n-p-1} ,\\]","title":"R-cuadrado"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#error-de-un-modelo","text":"","title":"Error de un modelo"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#definicion_1","text":"El error corresponde a la diferencia entre el valor original y el valor predicho,es decir: \\[e_{i}=y_{i}-\\hat{y}_{i} \\]","title":"Definici\u00f3n"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#formas-de-medir-el-error-de-un-modelo","text":"Para medir el ajuste de un modelo se ocupan las denominadas funciones de distancias o m\u00e9tricas . Existen varias m\u00e9tricas, dentro de las cuales encontramos: M\u00e9tricas absolutas : Las m\u00e9tricas absolutas o no escalada miden el error sin escalar los valores. Las m\u00e9trica absolutas m\u00e1s ocupadas son: Mean Absolute Error (MAE) \\[\\textrm{MAE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n}\\left | y_{t}-\\hat{y}_{t}\\right |\\] Mean squared error (MSE): \\[\\textrm{MSE}(y,\\hat{y}) =\\dfrac{1}{n}\\sum_{t=1}^{n}\\left ( y_{t}-\\hat{y}_{t}\\right )^2\\] M\u00e9tricas Porcentuales : Las m\u00e9tricas porcentuales o escaladas miden el error de manera escalada, es decir, se busca acotar el error entre valores de 0 a 1, donde 0 significa que el ajuste es perfecto, mientras que 1 ser\u00eda un mal ajuste. Cabe destacar que muchas veces las m\u00e9tricas porcentuales puden tener valores mayores a 1.Las m\u00e9trica Porcentuales m\u00e1s ocupadas son: Mean absolute percentage error (MAPE): \\[\\textrm{MAPE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n}\\left | \\frac{y_{t}-\\hat{y}_{t}}{y_{t}} \\right |\\] Symmetric mean absolute percentage error (sMAPE): \\[\\textrm{sMAPE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n} \\frac{\\left |y_{t}-\\hat{y}_{t}\\right |}{(\\left | y_{t} \\right |^2+\\left | \\hat{y}_{t} \\right |^2)/2}\\]","title":"Formas de medir el error de un modelo"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#otros-estadisticos-interesantes-del-modelo","text":"","title":"Otros estad\u00edsticos interesantes del modelo"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#test-f","text":"EL test F para regresi\u00f3n lineal prueba si alguna de las variables independientes en un modelo de regresi\u00f3n lineal m\u00faltiple es significativa. En t\u00e9rminos de test de hip\u00f3tesis, se quiere contrastar lo siguiente: \\(H_0: \\beta_1 = \\beta_2 = ... = \\beta_{p-1} = 0\\) \\(H_1: \\beta_j \u2260 0\\) , para al menos un valor de \\(j\\)","title":"Test F"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#test-omnibus","text":"EL test Omnibus esta relacionado con la simetr\u00eda y curtosis del resido. Se espera ver un valor cercano a cero que indicar\u00eda normalidad. El Prob (Omnibus) realiza una prueba estad\u00edstica que indica la probabilidad de que los residuos se distribuyan normalmente.","title":"Test Omnibus"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#test-durbin-watson","text":"El Test Durbin-Watson es un test de homocedasticidad. Para ver los l\u00edmites relacionados de este test, se puede consultar la siguiente tablas de valores .","title":"Test Durbin-Watson"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#test-jarque-bera","text":"Como el test Omnibus en que prueba tanto el sesgo como la curtosis. Esperamos ver en esta prueba una confirmaci\u00f3n de la prueba Omnibus. IMPORTANTE : Cabe destacar que el coeficiente \\(r^2\\) funciona bien en el contexto del mundo de las regresiones lineales. Para el an\u00e1lisis de modelos no lineales , esto coeficiente pierde su interpretaci\u00f3n. Se deja la siguiente refrerencia para comprender conceptos claves de test de hip\u00f3tesis, intervalos de confianza, p-valor. Estos t\u00e9rminos son escenciales para comprender la significancia del ajuste realizado. Existen muchas m\u00e1s m\u00e9tricas, pero estas son las m\u00e1s usulaes de encontrar. En el archivo metrics.py se definen las distintas m\u00e9tricas presentadas, las cuales serpan de utilidad m\u00e1s adelante.","title":"Test Jarque-Bera"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#aplicacion-con-python","text":"","title":"Aplicaci\u00f3n con python"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#ejemplo-sencillo","text":"Para comprender los modelos de regresi\u00f3n lineal, mostraremos un caso sencillo de uso. Para ello realizaremos un simulaci\u00f3n de una recta, en el cual le agregaremos un ruido blanco. # librerias import os import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns pd . set_option ( 'display.max_columns' , 500 ) # Ver m\u00e1s columnas de los dataframes # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab % matplotlib inline # ejemplo sencillo n = 100 np . random . seed ( n ) beta = np . array ([ 1 , 1 ]) # coeficientes x = np . random . rand ( n ) # variable independiente mu , sigma = 0 , 0.1 # media y desviacion estandar epsilon = np . random . normal ( mu , sigma , n ) # ruido blanco y = np . dot ( np . c_ [ np . ones ( n ), x ] , beta ) + epsilon # variables dependientes # generar dataframe df = pd . DataFrame ({ 'x' : x , 'y' : y }) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y 0 0.543405 1.612417 1 0.278369 1.347058 2 0.424518 1.267849 3 0.844776 1.935274 4 0.004719 1.082601 Grafiquemos los puntos en el plano cartesiano. # grafico de puntos sns . set ( rc = { 'figure.figsize' :( 10 , 8 )}) sns . scatterplot ( x = 'x' , y = 'y' , data = df , ) plt . show () Lo primero que debemos hacer es separar nuestro datos en los conjuntos de training set y test set . Concepto de Train set y Test set Al momento de entrenar los modelos de machine leraning, se debe tener un conjunto para poder entrenar el modelo y otro conjunto para poder evaluar el modelo. Es por esto que el conjunto de datos se separ\u00e1 en dos conjuntos: Train set : Conjunto de entrenamiento con el cual se entrenar\u00e1n los algoritmos de machine learning. Test set : Conjunto de testeo para averiguar la confiabilidad del modelo, es decir, cuan bueno es el ajuste del modelo. Tama\u00f1o ideal de cada conjunto La respuesta depende fuertemente del tama\u00f1o del conjunto de datos. A modo de regla emp\u00edrica, se considerar\u00e1 el tama\u00f1o \u00f3ptimo basado en la siguiente tabla: n\u00famero de filas train set test set entre 100-1000 67% 33% entre 1.000- 100.000 80% 20% mayor a 100.000 99% 1% from sklearn import datasets from sklearn.model_selection import train_test_split # import some data to play with X = df [[ 'x' ]] # we only take the first two features. y = df [ 'y' ] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # print rows train and test sets print ( 'Separando informacion: \\n ' ) print ( 'numero de filas data original : ' , len ( X )) print ( 'numero de filas train set : ' , len ( X_train )) print ( 'numero de filas test set : ' , len ( X_test )) Separando informacion: numero de filas data original : 100 numero de filas train set : 80 numero de filas test set : 20 Existen varias librer\u00edas para poder aplicar modelos de regresi\u00f3n, de los cuales la atenci\u00f3n estar\u00e1 enfocada en las librer\u00edas de statsmodels y sklearn .","title":"Ejemplo sencillo"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#ejemplo-con-statsmodel","text":"Para trabajar los modelos de statsmodel , basta con instanciar el comando OLS . El modelo no considera intercepto , por lo tanto, para agregar el intercepto, a las variables independientes se le debe agregar un vector de unos (tanto para el conjunto de entranamiento como de testeo). import statsmodels.api as sm model = sm . OLS ( y_train , sm . add_constant ( X_train )) results = model . fit () En statsmodel existe un comando para ver informaci\u00f3n del modelo en estudio mediante el comando summary # resultados del modelo print ( results . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.894 Model: OLS Adj. R-squared: 0.893 Method: Least Squares F-statistic: 658.4 Date: Fri, 12 Mar 2021 Prob (F-statistic): 8.98e-40 Time: 15:12:04 Log-Likelihood: 69.472 No. Observations: 80 AIC: -134.9 Df Residuals: 78 BIC: -130.2 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.9805 0.021 46.338 0.000 0.938 1.023 x 1.0099 0.039 25.659 0.000 0.932 1.088 ============================================================================== Omnibus: 0.424 Durbin-Watson: 1.753 Prob(Omnibus): 0.809 Jarque-Bera (JB): 0.587 Skew: 0.102 Prob(JB): 0.746 Kurtosis: 2.633 Cond. No. 4.17 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. A continuaci\u00f3n se dara una interpretaci\u00f3n de esta tabla: Descripci\u00f3n del Modelo Estos son estad\u00edsticas relacionadas a la ejecuci\u00f3n del modelo. Variable Descripi\u00f3n Dep. Variable Nombre de la variables dependiente Model Nombre del modelo ocupado Method M\u00e9todo para encontrar los par\u00e1metros \u00f3ptimos Date Fecha de ejecuci\u00f3n No. Observations N\u00famero de observaciones Df Residuals Grados de libertas de los residuos Df Model Grados de libertad del modelo Covariance Type Tipo de covarianza Ajustes del Modelo Estos son estad\u00edsticas relacionadas con la verosimilitud y la confiabilidad del modelo. Variable Descripi\u00f3n R-squared Valor del R-cuadrado Adj. R-squared Valor del R-cuadrado ajustado F-statistic Test para ver si todos los par\u00e1metros son iguales a cero Prob (F-statistic) Probabilidad Asociada al test Log-Likelihood Logaritmo de la funci\u00f3n de verosimilitud AIC Valor del estad\u00edstico AIC BIC Valor del estad\u00edstico BIC En este caso, tanto el r-cuadrado como el r-cuadrado ajustado est\u00e1n cerca del 0.9, se tiene un buen ajuste lineal de los datos. Adem\u00e1s, el test F nos da una probabilidad menor al 0.05, se rechaza la hip\u00f3tess nula que los coeficientes son iguales de cero. Par\u00e1metros del modelo La tabla muestra los valores asociados a los par\u00e1metros del modelo coef std err t P>|t| [0.025 0.975] const 0.9805 0.021 46.338 0.000 0.938 1.023 x 1.0099 0.039 25.659 0.000 0.932 1.088 Ac\u00e1 se tiene: Variables : Las variables en estudio son const (intercepto) y x . coef : Valor estimado del coeficiente. std err : Desviaci\u00f3n estandar del estimador. t : t = estimate/std error. P>|t| :p-valor individual para cada par\u00e1metro para aceptar o rechazar hip\u00f3tesis nula (par\u00e1metros significativamente distinto de cero). [0.025 | 0.975] : Intervalo de confianza de los par\u00e1metros En este caso, los valores estimados son cercanos a 1 (algo esperable debido a la simulaci\u00f3n realizadas), adem\u00e1s, se observa que cada uno de los par\u00e1metros es significativamente distinto de cero. Estad\u00edsticos interesantes del modelo Variable Descripci\u00f3n Omnibus Prueba de la asimetr\u00eda y curtosis de los residuos Prob(Omnibus) Probabilidad de que los residuos se distribuyan normalmente Skew Medida de simetr\u00eda de los datos Kurtosis Medida de curvatura de los datos Durbin-Watson Pruebas de homocedasticidad Jarque-Bera (JB) Como la prueba Omnibus, prueba tanto el sesgo como la curtosis. Prob(JB) Probabilidad de que los residuos se distribuyan normalmente Cond. No. N\u00famero de condici\u00f3n. Mide la sensibilidad de la salida de una funci\u00f3n en comparaci\u00f3n con su entrada En este caso: Tanto el test de Omnibus como el test Jarque-Bera nos arroja una probabilidad cercana a uno, lo cual confirma la hip\u00f3tesis que los residuos se distribuyen de manera normal. Para el test de Durbin-Watson, basados en la tablas de valores (tama\u00f1o de la muestra 80 y n\u00famero de variables 2), se tiene que los l\u00edmites para asumir que no existe correlaci\u00f3n en los residuos es de: \\([d_u,4-d_u]=[1.66,2.34]\\) , dado que el valor obtenido (1.753) se encuentra dentro de este rango, se concluye que no hay autocorrelaci\u00f3n de los residuos. El n\u00famero de condici\u00f3n es peque\u00f1o (podemos asumir que menor a 30 es un buen resultado) por lo que podemos asumir que no hay colinealidad de los datos. Ahora, para convencernos de manera visual de los resultados, realicemos un gr\u00e1fico con el ajuste lineal: # grafico de puntos sns . lmplot ( x = 'x' , y = 'y' , data = df , height = 8 , ) plt . show ()","title":"Ejemplo con Statsmodel"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#analisis-del-error","text":"","title":"An\u00e1lisis del error"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#predicciones","text":"Ahora que ya se tiene el modelo entrenado y se ha analizado sus principales caracter\u00edsticas, se pueden realizar predicciones de los valores que se desconocen, de la siguiente manera: # predicciones y_pred = results . predict ( sm . add_constant ( X_test )) Ahora, analizaremos las m\u00e9tricas de error asociado a las predicciones del modelo: from metrics_regression import * from sklearn.metrics import r2_score # ejemplo df_temp = pd . DataFrame ( { 'y' : y_test , 'yhat' : y_pred } ) print ( ' \\n Metricas para el regresor consumo_litros_milla: \\n ' ) summary_metrics ( df_temp ) Metricas para el regresor consumo_litros_milla: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse mape maape wmape mmape smape 0 0.1028 0.0171 0.1309 0.0677 0.0674 0.0666 0.0406 0.068","title":"Predicciones"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#normalidad-de-los-residuos","text":"Basados en los distintos test (Durbin-Watson,Omnibus,Jarque-Bera ) se concluye que los residuos del modelo son un ruido blanco. Para convencernos de esto de manera gr\u00e1fica, se realizan los siguientes gr\u00e1ficos de inter\u00e9s. Funci\u00f3n de Autocorrelaci\u00f3n La funci\u00f3n de autocorrelaci\u00f3n muestra que los residuos se encuentra dentro de la banda de valores cr\u00edticos \\((-0.2,0.2)\\) , concluyendo que no existe correlaci\u00f3n entre los residuos. from statsmodels.graphics.tsaplots import plot_acf sns . set ( rc = { 'figure.figsize' :( 12 , 8 )}) # funcion de autocorrelation plot_acf ( results . resid ) plt . show () QQ-plot La gr\u00e1fica de qq-plot nos muestra una comparaci\u00f3n en las distribuci\u00f3n de los residuos respecto a una poblaci\u00f3n con una distribuci\u00f3n normal. En este caso, los puntos (que representan la distribuci\u00f3n de los errores) se encuentran cercana a la recta (distribuci\u00f3n normal), concluyendo que la distribuci\u00f3n de los residuos sigue una distribuci\u00f3n normal. import scipy.stats as stats fig = sm . qqplot ( results . resid , stats . t , fit = True , line = \"45\" ) plt . show () Histograma Esta es una comparaci\u00f3n directa enntre la distribuci\u00f3n de los residuos versus la distribuci\u00f3n de una variable normal mediante un histograma. df_hist = pd . DataFrame ({ 'error' : results . resid }) sns . histplot ( x = 'error' , data = df_hist , kde = True , bins = 15 ) plt . show () A modo de conclusi\u00f3n, es correcto asumir que los errores siguen la distribuci\u00f3n de un ruido blanco, cumpliendo correctamente con los supuestos de la regresi\u00f3n lineal.","title":"Normalidad de los residuos"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#outliers","text":"Un outlier (o valor at\u00edpico) una observaci\u00f3n que es num\u00e9ricamente distante del resto de los datos. Las estad\u00edsticas derivadas de los conjuntos de datos que incluyen valores at\u00edpicos ser\u00e1n frecuentemente enga\u00f1osas. Estos valores pueden afectar fuertemente al modelo de regresi\u00f3n log\u00edstica. Veamos un ejemplo: # ejemplo sencillo n = 100 np . random . seed ( n ) beta = np . array ([ 1 , 1 ]) # coeficientes x = np . random . rand ( n ) # variable independiente mu , sigma = 0 , 0.1 # media y desviacion estandar epsilon = np . random . normal ( mu , sigma , n ) # ruido blanco y = np . dot ( np . c_ [ np . ones ( n ), x ] , beta ) + epsilon # variables dependientes y [: 10 ] = 3.1 # contaminacion x [ 10 ] = x [ 10 ] - 1 y [ 10 ] = y [ 10 ] - 1 x [ 11 ] = x [ 11 ] + 1 y [ 11 ] = y [ 11 ] + 1 # etiqueta outlier = np . zeros ( n ) outlier [: 10 ] = 1 outlier [ 10 : 12 ] = 2 # generar dataframe df = pd . DataFrame ({ 'x' : x , 'y' : y , 'outlier' : outlier }) # grafico de puntos sns . set ( rc = { 'figure.figsize' :( 10 , 8 )}) sns . scatterplot ( x = 'x' , y = 'y' , hue = 'outlier' , data = df , palette = [ 'blue' , 'red' , 'black' ] ) plt . show () plt . show () En este caso, se tiene dos tipos de outliers en este caso: Significativos : Aquellos outliers que afectan la regresi\u00f3n cambiando la tendencia a este grupo de outliers (puntos rojos). No significativo : Si bien son datos at\u00edpicos puesto que se encuentran fuera de la nube de puntos, el ajuste de la regresi\u00f3n lineal no se ve afectado (puntos negros). Veamos el ajuste lineal. # grafico de puntos sns . lmplot ( x = 'x' , y = 'y' , data = df , height = 8 , ) plt . show () Otro gr\u00e1fico de inter\u00e9s, es el gr\u00e1fico de influencia , que analiza la distancia de Cook de los residuos. # modelos de influencia X = df [[ 'x' ]] # we only take the first two features. y = df [ 'y' ] model = sm . OLS ( y , sm . add_constant ( X )) results = model . fit () sm . graphics . influence_plot ( results ) plt . show () Los puntos grandes se interpretan como puntos que tienen una alta influencia sobre la regresi\u00f3n lineal, mientras aquellos puntos peque\u00f1os tienen una influencia menor.","title":"Outliers"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#que-hacer-ante-la-presencia-de-outliers","text":"En este caso, la recta se ve fuertemente afectadas por estos valores. Para estos casos se pueden hacer varias cosas: Eliminaci\u00f3n de los outliers : Una vez identificado los outliers (algo que no es tan trivial de identificar para datos multivariables), se puden eliminar y seguir con el paso de modelado. Ventajas : F\u00e1cil de trabajar la data para los modelos que dependen fuertemente de la media de los datos. Desventajas : Para el caso multivariables no es t\u00e1n trivial encontrar outliers. Modelos m\u00e1s robustos a outliers : Se pueden aplicar otros modelos de regresi\u00f3n cuya estimaci\u00f3n de los par\u00e1metros, no se vea afectado por los valores de outliers. Ventajas : El an\u00e1lisis se vuelve independiente de los datos. Desventajas : Modelos m\u00e1s costoso computacionalmente y/o m\u00e1s complejos de implementar.","title":"\u00bf Qu\u00e9 hacer ante la presencia de outliers?"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#conclusion","text":"Los modelos de regresi\u00f3n lineal son una gran herramienta para realizar predicciones. Los outliers afectan considerablemente a la regresi\u00f3n lineal, por lo que se debn buscar estrategias para abordar esta problem\u00e1tica. En esta oportunidad se hizo un detalle t\u00e9cnico de disntintos est\u00e1disticos asociados a la regresi\u00f3n l\u00edneal (apuntando a un an\u00e1lisis inferencial ), no obstante, en los pr\u00f3ximos modelos, se estar\u00e1 interesado en analizar las predicciones del modelo y los errores asociados a ella, por lo cual los aspectos t\u00e9cnico quedar\u00e1n como lecturas complementarias. Existen varios casos donde los modelos de regresi\u00f3n l\u00edneal no realizan un correcto ajuste de los datos, pero es una gran herramienta para comenzar.","title":"Conclusi\u00f3n"},{"location":"lectures/ml/analisis_supervisado_regresion/02_regresion_lineal/#referencia","text":"Linear Regression in Python","title":"Referencia"},{"location":"lectures/ml/introduccion_ml/introduccion/","text":"Introducci\u00f3n \u00bfQu\u00e9 es el Machine learning? Podemos resumir Machine Learning en cuatro puntos: Estudia y construye sistemas que pueden aprender de los datos, m\u00e1s que seguir instrucciones expl\u00edcitamente programadas. Conjunto de t\u00e9cnicas y modelos que permiten el modelamiento predictivo de datos, reunidas a partir de la intersecci\u00f3n de elementos de probabilidad, estad\u00edstica e inteligencia artificial. T\u00edpicamente, alguien que trabaja en Machine Learning est\u00e1 en la Academia y busca realizar investigaci\u00f3n y publicar art\u00edculos. Pregunta fundamental: \u00bfQu\u00e9 conocimiento emerge a partir de los datos? \u00bfQu\u00e9 modelo/t\u00e9cnica otorga la mejor predicci\u00f3n para estos datos? Para poder avanzar en el estudio de machine learning es de vital importancia definir el concepto de modelo . \u00bfQu\u00e9 se entiende por modelo? Un modelo se entinde como: * Una representaci\u00f3n abstracta y conveniente de un sistema. * Una simplificaci\u00f3n del mundo real. * Un medio de exploraci\u00f3n y de explicaci\u00f3n para nuestro entendimiento de la realidad. Por otro lado, un modelo no es: Igual al mundo real. Un sustituto para mediciones o experimentos. Los modelos nos permiten: reproducir experimentos donde factores no pueden ser f\u00e1cilmente controlados. Ejemplo : Comportamiento de c\u00e1psula lunar en gravedad cero y al atravesar atm\u00f3sfera a gran velocidad. simplificar el entendimiento de sistemas complejos, al permitir el an\u00e1lisis y exploraci\u00f3n de cada componente del sistema por separado. Ejemplo : Adelgazamiento de la capa de hielo polar por calentamiento global. \u00bf Qu\u00e9 tipos de problemas podemos abordar con machine learning? Los problemas que se pueden resolver con machine learning se pueden englobar en tres tipos: aprendeizaje supervisado , aprendizaje no supervisado y aprendizaje reforzado . Aprendizaje supervisado El sistema aprende en base a datos estructurados o no estructurados. Clasificados previamente (se conoce la respuesta). El algoritmo produce una funci\u00f3n que establece una correspondencia entre las entradas y las salidas deseadas del sistema. Aprendizaje no supervisado Modelo se construye usando un conjunto de datos como entrada, los cuales no han sido clasificados previamente. El sistema tiene que ser capaz de reconocer patrones para poder etiquetar las nuevas entradas. Aprendizaje por refuerzo Aprendizaje por refuerzo o Aprendizaje reforzado es un \u00e1rea del aprendizaje autom\u00e1tico inspirada en la psicolog\u00eda conductista, cuya ocupaci\u00f3n es determinar qu\u00e9 acciones debe escoger un agente de software en un entorno dado con el fin de maximizar alguna noci\u00f3n de \"recompensa\" o premio acumulado. Algoritmos m\u00e1s utilizados Los algoritmos que m\u00e1s se suelen utilizar en los problemas de Machine Learning son los siguientes: Regresi\u00f3n Lineal Regresi\u00f3n Log\u00edstica Arboles de Decision Random Forest SVM KNN K-means \u00bf Qu\u00e9 se necesita para aprender machine learning? Se necesita tener conocimientos de los siguientes t\u00f3picos. Algebra Lineal Probabilidad y estad\u00edstica Optimizaci\u00f3n Librer\u00edas de machine learning en python Una de las grandes ventajas que ofrece python sobre otros lenguajes de programaci\u00f3n; es lo grande y prolifera que es la comunidad de desarrolladores que lo rodean; comunidad que ha contribuido con una gran variedad de librer\u00edas de primer nivel que extienden la funcionalidades del lenguaje. Para el caso de Machine Learning , las principales librer\u00edas que podemos utilizar son: Scikit-Learn Scikit-learn es la principal librer\u00eda que existe para trabajar con Machine Learning, incluye la implementaci\u00f3n de un gran n\u00famero de algoritmos de aprendizaje. La podemos utilizar para clasificaciones , extraccion de caracter\u00edsticas , regresiones , agrupaciones , reducci\u00f3n de dimensiones , selecci\u00f3n de modelos , o preprocesamiento . Statsmodels Statsmodels es otra gran librer\u00eda que hace foco en modelos estad\u00edsticos y se utiliza principalmente para an\u00e1lisis predictivos y exploratorios. Las pruebas estad\u00edsticas que ofrece son bastante amplias y abarcan tareas de validaci\u00f3n para la mayor\u00eda de los casos. Conceptos claves en machine learning Esquema machine learning EL proceso de machine learning se puede resumir a grandes rasgo por el siguiente esquema. Recolectar los datos . Podemos recolectar los datos desde muchas fuentes, podemos por ejemplo extraer los datos de un sitio web o obtener los datos utilizando una API o desde una base de datos. Preprocesar los datos . Es pr\u00e1cticamente inevitable tener que realizar varias tareas de preprocesamiento antes de poder utilizar los datos. Igualmente este punto suele ser mucho m\u00e1s sencillo que el paso anterior. Explorar los datos . Una vez que ya tenemos los datos y est\u00e1n con el formato correcto, podemos realizar un pre an\u00e1lisis para corregir los casos de valores faltantes o intentar encontrar a simple vista alg\u00fan patr\u00f3n en los mismos que nos facilite la construcci\u00f3n del modelo. Entrenar el modelo . En esta etapa se entrenan los modelos con los datos que venimos procesando en las etapas anteriores. La idea es que los modelos puedan extraer informaci\u00f3n \u00fatil de los datos que le pasamos para luego poder hacer predicciones. Evaluar el modelo . Evaluamos que tan preciso es el modelo en sus predicciones y si no estamos muy conforme con su rendimiento, podemos volver a la etapa anterior y continuar entrenando el modelo cambiando algunos par\u00e1metros hasta lograr un rendimiento aceptable. Utilizar el modelo . En esta ultima etapa, ya ponemos a nuestro modelo a enfrentarse al problema real. Aqu\u00ed tambi\u00e9n podemos medir su rendimiento, lo que tal vez nos obligue a revisar todos los pasos anteriores. Los pasos 1,2,3 son los pasos que ya se han visto con detalle en este curso. Por otro lado, la etapa de modelamiento (entrenar, evaluar y predecir) ser\u00e1 necesario introducir nuevos conceptos. Referencias Basic Concepts in Machine Learning An Introduction to Machine Learning Theory and Its Applications: A Visual Tutorial with Examples","title":"Introducci\u00f3n"},{"location":"lectures/ml/introduccion_ml/introduccion/#introduccion","text":"","title":"Introducci\u00f3n"},{"location":"lectures/ml/introduccion_ml/introduccion/#que-es-el-machine-learning","text":"Podemos resumir Machine Learning en cuatro puntos: Estudia y construye sistemas que pueden aprender de los datos, m\u00e1s que seguir instrucciones expl\u00edcitamente programadas. Conjunto de t\u00e9cnicas y modelos que permiten el modelamiento predictivo de datos, reunidas a partir de la intersecci\u00f3n de elementos de probabilidad, estad\u00edstica e inteligencia artificial. T\u00edpicamente, alguien que trabaja en Machine Learning est\u00e1 en la Academia y busca realizar investigaci\u00f3n y publicar art\u00edculos. Pregunta fundamental: \u00bfQu\u00e9 conocimiento emerge a partir de los datos? \u00bfQu\u00e9 modelo/t\u00e9cnica otorga la mejor predicci\u00f3n para estos datos? Para poder avanzar en el estudio de machine learning es de vital importancia definir el concepto de modelo .","title":"\u00bfQu\u00e9 es el Machine learning?"},{"location":"lectures/ml/introduccion_ml/introduccion/#que-se-entiende-por-modelo","text":"Un modelo se entinde como: * Una representaci\u00f3n abstracta y conveniente de un sistema. * Una simplificaci\u00f3n del mundo real. * Un medio de exploraci\u00f3n y de explicaci\u00f3n para nuestro entendimiento de la realidad. Por otro lado, un modelo no es: Igual al mundo real. Un sustituto para mediciones o experimentos. Los modelos nos permiten: reproducir experimentos donde factores no pueden ser f\u00e1cilmente controlados. Ejemplo : Comportamiento de c\u00e1psula lunar en gravedad cero y al atravesar atm\u00f3sfera a gran velocidad. simplificar el entendimiento de sistemas complejos, al permitir el an\u00e1lisis y exploraci\u00f3n de cada componente del sistema por separado. Ejemplo : Adelgazamiento de la capa de hielo polar por calentamiento global.","title":"\u00bfQu\u00e9 se entiende por modelo?"},{"location":"lectures/ml/introduccion_ml/introduccion/#que-tipos-de-problemas-podemos-abordar-con-machine-learning","text":"Los problemas que se pueden resolver con machine learning se pueden englobar en tres tipos: aprendeizaje supervisado , aprendizaje no supervisado y aprendizaje reforzado .","title":"\u00bf Qu\u00e9 tipos de problemas podemos abordar con machine learning?"},{"location":"lectures/ml/introduccion_ml/introduccion/#aprendizaje-supervisado","text":"El sistema aprende en base a datos estructurados o no estructurados. Clasificados previamente (se conoce la respuesta). El algoritmo produce una funci\u00f3n que establece una correspondencia entre las entradas y las salidas deseadas del sistema.","title":"Aprendizaje supervisado"},{"location":"lectures/ml/introduccion_ml/introduccion/#aprendizaje-no-supervisado","text":"Modelo se construye usando un conjunto de datos como entrada, los cuales no han sido clasificados previamente. El sistema tiene que ser capaz de reconocer patrones para poder etiquetar las nuevas entradas.","title":"Aprendizaje no supervisado"},{"location":"lectures/ml/introduccion_ml/introduccion/#aprendizaje-por-refuerzo","text":"Aprendizaje por refuerzo o Aprendizaje reforzado es un \u00e1rea del aprendizaje autom\u00e1tico inspirada en la psicolog\u00eda conductista, cuya ocupaci\u00f3n es determinar qu\u00e9 acciones debe escoger un agente de software en un entorno dado con el fin de maximizar alguna noci\u00f3n de \"recompensa\" o premio acumulado.","title":"Aprendizaje por refuerzo"},{"location":"lectures/ml/introduccion_ml/introduccion/#algoritmos-mas-utilizados","text":"Los algoritmos que m\u00e1s se suelen utilizar en los problemas de Machine Learning son los siguientes: Regresi\u00f3n Lineal Regresi\u00f3n Log\u00edstica Arboles de Decision Random Forest SVM KNN K-means \u00bf Qu\u00e9 se necesita para aprender machine learning? Se necesita tener conocimientos de los siguientes t\u00f3picos. Algebra Lineal Probabilidad y estad\u00edstica Optimizaci\u00f3n","title":"Algoritmos m\u00e1s utilizados"},{"location":"lectures/ml/introduccion_ml/introduccion/#librerias-de-machine-learning-en-python","text":"Una de las grandes ventajas que ofrece python sobre otros lenguajes de programaci\u00f3n; es lo grande y prolifera que es la comunidad de desarrolladores que lo rodean; comunidad que ha contribuido con una gran variedad de librer\u00edas de primer nivel que extienden la funcionalidades del lenguaje. Para el caso de Machine Learning , las principales librer\u00edas que podemos utilizar son:","title":"Librer\u00edas de machine learning en python"},{"location":"lectures/ml/introduccion_ml/introduccion/#scikit-learn","text":"Scikit-learn es la principal librer\u00eda que existe para trabajar con Machine Learning, incluye la implementaci\u00f3n de un gran n\u00famero de algoritmos de aprendizaje. La podemos utilizar para clasificaciones , extraccion de caracter\u00edsticas , regresiones , agrupaciones , reducci\u00f3n de dimensiones , selecci\u00f3n de modelos , o preprocesamiento .","title":"Scikit-Learn"},{"location":"lectures/ml/introduccion_ml/introduccion/#statsmodels","text":"Statsmodels es otra gran librer\u00eda que hace foco en modelos estad\u00edsticos y se utiliza principalmente para an\u00e1lisis predictivos y exploratorios. Las pruebas estad\u00edsticas que ofrece son bastante amplias y abarcan tareas de validaci\u00f3n para la mayor\u00eda de los casos.","title":"Statsmodels"},{"location":"lectures/ml/introduccion_ml/introduccion/#conceptos-claves-en-machine-learning","text":"","title":"Conceptos claves en machine learning"},{"location":"lectures/ml/introduccion_ml/introduccion/#esquema-machine-learning","text":"EL proceso de machine learning se puede resumir a grandes rasgo por el siguiente esquema. Recolectar los datos . Podemos recolectar los datos desde muchas fuentes, podemos por ejemplo extraer los datos de un sitio web o obtener los datos utilizando una API o desde una base de datos. Preprocesar los datos . Es pr\u00e1cticamente inevitable tener que realizar varias tareas de preprocesamiento antes de poder utilizar los datos. Igualmente este punto suele ser mucho m\u00e1s sencillo que el paso anterior. Explorar los datos . Una vez que ya tenemos los datos y est\u00e1n con el formato correcto, podemos realizar un pre an\u00e1lisis para corregir los casos de valores faltantes o intentar encontrar a simple vista alg\u00fan patr\u00f3n en los mismos que nos facilite la construcci\u00f3n del modelo. Entrenar el modelo . En esta etapa se entrenan los modelos con los datos que venimos procesando en las etapas anteriores. La idea es que los modelos puedan extraer informaci\u00f3n \u00fatil de los datos que le pasamos para luego poder hacer predicciones. Evaluar el modelo . Evaluamos que tan preciso es el modelo en sus predicciones y si no estamos muy conforme con su rendimiento, podemos volver a la etapa anterior y continuar entrenando el modelo cambiando algunos par\u00e1metros hasta lograr un rendimiento aceptable. Utilizar el modelo . En esta ultima etapa, ya ponemos a nuestro modelo a enfrentarse al problema real. Aqu\u00ed tambi\u00e9n podemos medir su rendimiento, lo que tal vez nos obligue a revisar todos los pasos anteriores. Los pasos 1,2,3 son los pasos que ya se han visto con detalle en este curso. Por otro lado, la etapa de modelamiento (entrenar, evaluar y predecir) ser\u00e1 necesario introducir nuevos conceptos.","title":"Esquema machine learning"},{"location":"lectures/ml/introduccion_ml/introduccion/#referencias","text":"Basic Concepts in Machine Learning An Introduction to Machine Learning Theory and Its Applications: A Visual Tutorial with Examples","title":"Referencias"},{"location":"lectures/ml/overfitting/04_overfitting_underfitting/","text":"Overfitting Introducci\u00f3n El overfitting ocurre cuando el algoritmo de machine learning captura el ruido de los datos. Intuitivamente, el overfitting ocurre cuando el modelo o el algoritmo se ajusta demasiado bien a los datos. Espec\u00edficamente, el sobreajuste ocurre si el modelo o algoritmo muestra un sesgo bajo pero una varianza alta. El overfitting a menudo es el resultado de un modelo excesivamente complicado, y puede evitarse ajustando m\u00faltiples modelos y utilizando validaci\u00f3n o validaci\u00f3n cruzada para comparar sus precisiones predictivas en los datos de prueba. El underfitting ocurre cuando un modelo estad\u00edstico o un algoritmo de machine learning no pueden capturar la tendencia subyacente de los datos. Intuitivamente, el underfitting ocurre cuando el modelo o el algoritmo no se ajustan suficientemente a los datos. Espec\u00edficamente, el underfitting ocurre si el modelo o algoritmo muestra una varianza baja pero un sesgo alto. El underfitting suele ser el resultado de un modelo excesivamente simple. \u00bfC\u00f3mo escoger el mejor modelo? El sobreajuste va a estar relacionado con la complejidad del modelo, mientras m\u00e1s complejidad le agreguemos, mayor va a ser la tendencia a sobreajuste a los datos. No existe una regla general para establecer cual es el nivel ideal de complejidad que le podemos otorgar a nuestro modelo sin caer en el sobreajuste; pero podemos valernos de algunas herramientas anal\u00edticas para intentar entender como el modelo se ajusta a los datos y reconocer el sobreajuste. Para entender esto, veamos un ejemplo con el m\u00e9todo de \u00e1rboles de decisiones . Los \u00e1rboles de decisi\u00f3n ( DT ) son un m\u00e9todo de aprendizaje supervisado no param\u00e9trico utilizado para la clasificaci\u00f3n y la regresi\u00f3n. Ejemplo con \u00c1rboles de Decisi\u00f3n Los \u00c1rboles de Decisi\u00f3n pueden ser muchas veces una herramienta muy precisa, pero tambi\u00e9n con mucha tendencia al sobreajuste. Para construir estos modelos aplicamos un procedimiento recursivo para encontrar los atributos que nos proporcionan m\u00e1s informaci\u00f3n sobre distintos subconjuntos de datos, cada vez m\u00e1s peque\u00f1os. Si aplicamos este procedimiento en forma reiterada, eventualmente podemos llegar a un \u00e1rbol en el que cada hoja tenga una sola instancia de nuestra variable objetivo a clasificar. En este caso extremo, el \u00c1rbol de Decisi\u00f3n va a tener una pobre generalizaci\u00f3n y estar bastante sobreajustado; ya que cada instancia de los datos de entrenamiento va a encontrar el camino que lo lleve eventualmente a la hoja que lo contiene, alcanzando as\u00ed una precisi\u00f3n del 100% con los datos de entrenamiento. Ejemplo funci\u00f3n sinusoidal Veamos un ejemplo sencillo con la ayuda de python, tratemos de ajustar un modelo de DT sobre una funci\u00f3n senusoidal. # librerias import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.datasets import make_classification from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier , DecisionTreeRegressor import random random . seed ( 1982 ) # semilla # graficos incrustados % matplotlib inline # parametros esteticos de seaborn sns . set_palette ( \"deep\" , desat = .6 ) sns . set_context ( rc = { \"figure.figsize\" : ( 12 , 4 )}) # Create a random dataset rng = np . random . RandomState ( 1 ) X = np . sort ( 5 * rng . rand ( 80 , 1 ), axis = 0 ) y = np . sin ( X ) . ravel () y [:: 5 ] += 3 * ( 0.5 - rng . rand ( 16 )) # separ los datos en train y eval x_train , x_eval , y_train , y_eval = train_test_split ( X , y , test_size = 0.35 , train_size = 0.65 , random_state = 1982 ) # Fit regression model regr_1 = DecisionTreeRegressor ( max_depth = 2 ) regr_2 = DecisionTreeRegressor ( max_depth = 10 ) regr_1 . fit ( x_train , y_train ) regr_2 . fit ( x_train , y_train ) # Predict X_test = np . arange ( 0.0 , 5.0 , 0.01 )[:, np . newaxis ] y_1 = regr_1 . predict ( X_test ) y_2 = regr_2 . predict ( X_test ) y_3 = regr_1 . predict ( X_test ) # Plot the results fig , ax = plt . subplots ( figsize = ( 11 , 8.5 )) plt . scatter ( X , y , s = 20 , edgecolor = \"black\" , c = \"darkorange\" , label = \"data\" ) plt . plot ( X_test , y_1 , color = \"cornflowerblue\" , label = \"max_depth=2\" , linewidth = 2 ) plt . plot ( X_test , y_2 , color = \"yellowgreen\" , label = \"max_depth=10\" , linewidth = 2 ) plt . xlabel ( \"data\" ) plt . ylabel ( \"target\" ) plt . title ( \"Decision Tree Regression\" ) plt . legend () plt . show () Basado en los gr\u00e1ficos, el modelo de DT con profundidad 2, no se ajuste muy bien a los datos, mientras que el modelo DT con profundidad 10 se ajuste excesivamente demasiado a ellos. Para ver el ajuste de cada modelo, estudiaremos su precisi\u00f3n ( score ) sobre los conjunto de entrenamiento y de testeo. result = pd . DataFrame ({ 'model' : [ 'dt_depth_2' , 'dt_depth_10' ], 'train_score' : [ regr_1 . score ( x_train , y_train ), regr_2 . score ( x_train , y_train )], 'test_score' : [ regr_1 . score ( x_eval , y_eval ), regr_2 . score ( x_eval , y_eval )] }) result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model train_score test_score 0 dt_depth_2 0.766363 0.719137 1 dt_depth_10 1.000000 0.661186 Como es de esperar, para el modelo DT con profundidad 10, la precisi\u00f3n sobre el conjunto de entrenamiento es perfecta (igual a 1), no obstante, esta disminuye considerablemente al obtener la presici\u00f3n sobre los datos de testeo (igual a 0.66), por lo que esto es una evidencia para decir que el modelo tiene overfitting . Caso contrario es el modelo DT con profundidad 2, puesto que es un caso t\u00edpico de underfitting . Cabe destacar que el modelo de underfitting tiene una presici\u00f3n similar tanto para el conjunto de entrenamiento como para el conjunto de testo. Conclusiones del caso Ambos modelos no ajuste de la mejor manera, pero lo hacen de distintas perspectivas. Se debe poner mucho \u00e9nfasis al momento de separar el conjunto de entrenamiento y de testeo, puesto que los resultados se pueden ver altamente sesgado (caso del overfitting). Particularmente para este caso, el ajuste era complejo de realizar puesto que eliminabamos un monto de datos \"significativos\", que hacian que los modelos no captar\u00e1n la continuidad de la funci\u00f3n sinusoidal. Equilibrio en el ajuste de modelos A continuaci\u00f3n ocuparemos otro conjunto de entrenamientos ( make_classification ) para mostrar una forma de encoentrar un un equilibrio en la complejidad del modelo y su ajuste a los datos. Siguiendo con el ejemplo de los modelos de \u00e1rbol de decisi\u00f3n , analizaremos la presici\u00f3n ( score ) para distintas profundidades sobre los distintos conjuntos (entrenamiento y testeo). # Ejemplo en python - \u00e1rboles de decisi\u00f3n # dummy data con 100 atributos y 2 clases X , y = make_classification ( 10000 , 100 , n_informative = 3 , n_classes = 2 , random_state = 1982 ) # separ los datos en train y eval x_train , x_eval , y_train , y_eval = train_test_split ( X , y , test_size = 0.35 , train_size = 0.65 , random_state = 1982 ) # Grafico de ajuste del \u00e1rbol de decisi\u00f3n train_prec = [] eval_prec = [] max_deep_list = list ( range ( 2 , 20 )) # Entrenar con arboles de distinta profundidad for deep in max_deep_list : model = DecisionTreeClassifier ( max_depth = deep ) model . fit ( x_train , y_train ) train_prec . append ( model . score ( x_train , y_train )) eval_prec . append ( model . score ( x_eval , y_eval )) # graficar los resultados. sns . set ( rc = { 'figure.figsize' :( 12 , 9 )}) df1 = pd . DataFrame ({ 'numero_nodos' : max_deep_list , 'precision' : train_prec , 'datos' : 'entrenamiento' }) df2 = pd . DataFrame ({ 'numero_nodos' : max_deep_list , 'precision' : eval_prec , 'datos' : 'evaluacion' }) df_graph = pd . concat ([ df1 , df2 ]) sns . lineplot ( data = df_graph , x = 'numero_nodos' , y = 'precision' , hue = 'datos' , palette = \"Set1\" ) <AxesSubplot:xlabel='numero_nodos', ylabel='precision'> El gr\u00e1fico que acabamos de construir se llama gr\u00e1fico de ajuste y muestra la precisi\u00f3n del modelo en funci\u00f3n de su complejidad. El punto con mayor precisi\u00f3n, en los datos de evaluaci\u00f3n, lo obtenemos con un nivel de profundidad de aproximadamente 6 nodos; a partir de all\u00ed el modelo pierde en generalizaci\u00f3n y comienza a estar sobreajustado. Tambi\u00e9n podemos crear un gr\u00e1fico similar con la ayuda de Scikit-learn, utilizando validation_curve . # utilizando validation curve de sklearn from sklearn.model_selection import validation_curve train_prec , eval_prec = validation_curve ( estimator = model , X = x_train , y = y_train , param_name = 'max_depth' , param_range = max_deep_list , cv = 5 ) train_mean = np . mean ( train_prec , axis = 1 ) train_std = np . std ( train_prec , axis = 1 ) test_mean = np . mean ( eval_prec , axis = 1 ) test_std = np . std ( eval_prec , axis = 1 ) # graficando las curvas plt . plot ( max_deep_list , train_mean , color = 'r' , marker = 'o' , markersize = 5 , label = 'entrenamiento' ) plt . fill_between ( max_deep_list , train_mean + train_std , train_mean - train_std , alpha = 0.15 , color = 'r' ) plt . plot ( max_deep_list , test_mean , color = 'b' , linestyle = '--' , marker = 's' , markersize = 5 , label = 'evaluacion' ) plt . fill_between ( max_deep_list , test_mean + test_std , test_mean - test_std , alpha = 0.15 , color = 'b' ) plt . legend ( loc = 'center right' ) plt . xlabel ( 'numero_nodos' ) plt . ylabel ( 'precision' ) plt . show () Referencia Underfitting vs Underfitting Overfitting and Underfitting With Machine Learning Algorithms","title":"Overffiting"},{"location":"lectures/ml/overfitting/04_overfitting_underfitting/#overfitting","text":"","title":"Overfitting"},{"location":"lectures/ml/overfitting/04_overfitting_underfitting/#introduccion","text":"El overfitting ocurre cuando el algoritmo de machine learning captura el ruido de los datos. Intuitivamente, el overfitting ocurre cuando el modelo o el algoritmo se ajusta demasiado bien a los datos. Espec\u00edficamente, el sobreajuste ocurre si el modelo o algoritmo muestra un sesgo bajo pero una varianza alta. El overfitting a menudo es el resultado de un modelo excesivamente complicado, y puede evitarse ajustando m\u00faltiples modelos y utilizando validaci\u00f3n o validaci\u00f3n cruzada para comparar sus precisiones predictivas en los datos de prueba. El underfitting ocurre cuando un modelo estad\u00edstico o un algoritmo de machine learning no pueden capturar la tendencia subyacente de los datos. Intuitivamente, el underfitting ocurre cuando el modelo o el algoritmo no se ajustan suficientemente a los datos. Espec\u00edficamente, el underfitting ocurre si el modelo o algoritmo muestra una varianza baja pero un sesgo alto. El underfitting suele ser el resultado de un modelo excesivamente simple. \u00bfC\u00f3mo escoger el mejor modelo? El sobreajuste va a estar relacionado con la complejidad del modelo, mientras m\u00e1s complejidad le agreguemos, mayor va a ser la tendencia a sobreajuste a los datos. No existe una regla general para establecer cual es el nivel ideal de complejidad que le podemos otorgar a nuestro modelo sin caer en el sobreajuste; pero podemos valernos de algunas herramientas anal\u00edticas para intentar entender como el modelo se ajusta a los datos y reconocer el sobreajuste. Para entender esto, veamos un ejemplo con el m\u00e9todo de \u00e1rboles de decisiones . Los \u00e1rboles de decisi\u00f3n ( DT ) son un m\u00e9todo de aprendizaje supervisado no param\u00e9trico utilizado para la clasificaci\u00f3n y la regresi\u00f3n.","title":"Introducci\u00f3n"},{"location":"lectures/ml/overfitting/04_overfitting_underfitting/#ejemplo-con-arboles-de-decision","text":"Los \u00c1rboles de Decisi\u00f3n pueden ser muchas veces una herramienta muy precisa, pero tambi\u00e9n con mucha tendencia al sobreajuste. Para construir estos modelos aplicamos un procedimiento recursivo para encontrar los atributos que nos proporcionan m\u00e1s informaci\u00f3n sobre distintos subconjuntos de datos, cada vez m\u00e1s peque\u00f1os. Si aplicamos este procedimiento en forma reiterada, eventualmente podemos llegar a un \u00e1rbol en el que cada hoja tenga una sola instancia de nuestra variable objetivo a clasificar. En este caso extremo, el \u00c1rbol de Decisi\u00f3n va a tener una pobre generalizaci\u00f3n y estar bastante sobreajustado; ya que cada instancia de los datos de entrenamiento va a encontrar el camino que lo lleve eventualmente a la hoja que lo contiene, alcanzando as\u00ed una precisi\u00f3n del 100% con los datos de entrenamiento.","title":"Ejemplo con  \u00c1rboles de Decisi\u00f3n"},{"location":"lectures/ml/overfitting/04_overfitting_underfitting/#ejemplo-funcion-sinusoidal","text":"Veamos un ejemplo sencillo con la ayuda de python, tratemos de ajustar un modelo de DT sobre una funci\u00f3n senusoidal. # librerias import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.datasets import make_classification from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier , DecisionTreeRegressor import random random . seed ( 1982 ) # semilla # graficos incrustados % matplotlib inline # parametros esteticos de seaborn sns . set_palette ( \"deep\" , desat = .6 ) sns . set_context ( rc = { \"figure.figsize\" : ( 12 , 4 )}) # Create a random dataset rng = np . random . RandomState ( 1 ) X = np . sort ( 5 * rng . rand ( 80 , 1 ), axis = 0 ) y = np . sin ( X ) . ravel () y [:: 5 ] += 3 * ( 0.5 - rng . rand ( 16 )) # separ los datos en train y eval x_train , x_eval , y_train , y_eval = train_test_split ( X , y , test_size = 0.35 , train_size = 0.65 , random_state = 1982 ) # Fit regression model regr_1 = DecisionTreeRegressor ( max_depth = 2 ) regr_2 = DecisionTreeRegressor ( max_depth = 10 ) regr_1 . fit ( x_train , y_train ) regr_2 . fit ( x_train , y_train ) # Predict X_test = np . arange ( 0.0 , 5.0 , 0.01 )[:, np . newaxis ] y_1 = regr_1 . predict ( X_test ) y_2 = regr_2 . predict ( X_test ) y_3 = regr_1 . predict ( X_test ) # Plot the results fig , ax = plt . subplots ( figsize = ( 11 , 8.5 )) plt . scatter ( X , y , s = 20 , edgecolor = \"black\" , c = \"darkorange\" , label = \"data\" ) plt . plot ( X_test , y_1 , color = \"cornflowerblue\" , label = \"max_depth=2\" , linewidth = 2 ) plt . plot ( X_test , y_2 , color = \"yellowgreen\" , label = \"max_depth=10\" , linewidth = 2 ) plt . xlabel ( \"data\" ) plt . ylabel ( \"target\" ) plt . title ( \"Decision Tree Regression\" ) plt . legend () plt . show () Basado en los gr\u00e1ficos, el modelo de DT con profundidad 2, no se ajuste muy bien a los datos, mientras que el modelo DT con profundidad 10 se ajuste excesivamente demasiado a ellos. Para ver el ajuste de cada modelo, estudiaremos su precisi\u00f3n ( score ) sobre los conjunto de entrenamiento y de testeo. result = pd . DataFrame ({ 'model' : [ 'dt_depth_2' , 'dt_depth_10' ], 'train_score' : [ regr_1 . score ( x_train , y_train ), regr_2 . score ( x_train , y_train )], 'test_score' : [ regr_1 . score ( x_eval , y_eval ), regr_2 . score ( x_eval , y_eval )] }) result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model train_score test_score 0 dt_depth_2 0.766363 0.719137 1 dt_depth_10 1.000000 0.661186 Como es de esperar, para el modelo DT con profundidad 10, la precisi\u00f3n sobre el conjunto de entrenamiento es perfecta (igual a 1), no obstante, esta disminuye considerablemente al obtener la presici\u00f3n sobre los datos de testeo (igual a 0.66), por lo que esto es una evidencia para decir que el modelo tiene overfitting . Caso contrario es el modelo DT con profundidad 2, puesto que es un caso t\u00edpico de underfitting . Cabe destacar que el modelo de underfitting tiene una presici\u00f3n similar tanto para el conjunto de entrenamiento como para el conjunto de testo. Conclusiones del caso Ambos modelos no ajuste de la mejor manera, pero lo hacen de distintas perspectivas. Se debe poner mucho \u00e9nfasis al momento de separar el conjunto de entrenamiento y de testeo, puesto que los resultados se pueden ver altamente sesgado (caso del overfitting). Particularmente para este caso, el ajuste era complejo de realizar puesto que eliminabamos un monto de datos \"significativos\", que hacian que los modelos no captar\u00e1n la continuidad de la funci\u00f3n sinusoidal.","title":"Ejemplo funci\u00f3n sinusoidal"},{"location":"lectures/ml/overfitting/04_overfitting_underfitting/#equilibrio-en-el-ajuste-de-modelos","text":"A continuaci\u00f3n ocuparemos otro conjunto de entrenamientos ( make_classification ) para mostrar una forma de encoentrar un un equilibrio en la complejidad del modelo y su ajuste a los datos. Siguiendo con el ejemplo de los modelos de \u00e1rbol de decisi\u00f3n , analizaremos la presici\u00f3n ( score ) para distintas profundidades sobre los distintos conjuntos (entrenamiento y testeo). # Ejemplo en python - \u00e1rboles de decisi\u00f3n # dummy data con 100 atributos y 2 clases X , y = make_classification ( 10000 , 100 , n_informative = 3 , n_classes = 2 , random_state = 1982 ) # separ los datos en train y eval x_train , x_eval , y_train , y_eval = train_test_split ( X , y , test_size = 0.35 , train_size = 0.65 , random_state = 1982 ) # Grafico de ajuste del \u00e1rbol de decisi\u00f3n train_prec = [] eval_prec = [] max_deep_list = list ( range ( 2 , 20 )) # Entrenar con arboles de distinta profundidad for deep in max_deep_list : model = DecisionTreeClassifier ( max_depth = deep ) model . fit ( x_train , y_train ) train_prec . append ( model . score ( x_train , y_train )) eval_prec . append ( model . score ( x_eval , y_eval )) # graficar los resultados. sns . set ( rc = { 'figure.figsize' :( 12 , 9 )}) df1 = pd . DataFrame ({ 'numero_nodos' : max_deep_list , 'precision' : train_prec , 'datos' : 'entrenamiento' }) df2 = pd . DataFrame ({ 'numero_nodos' : max_deep_list , 'precision' : eval_prec , 'datos' : 'evaluacion' }) df_graph = pd . concat ([ df1 , df2 ]) sns . lineplot ( data = df_graph , x = 'numero_nodos' , y = 'precision' , hue = 'datos' , palette = \"Set1\" ) <AxesSubplot:xlabel='numero_nodos', ylabel='precision'> El gr\u00e1fico que acabamos de construir se llama gr\u00e1fico de ajuste y muestra la precisi\u00f3n del modelo en funci\u00f3n de su complejidad. El punto con mayor precisi\u00f3n, en los datos de evaluaci\u00f3n, lo obtenemos con un nivel de profundidad de aproximadamente 6 nodos; a partir de all\u00ed el modelo pierde en generalizaci\u00f3n y comienza a estar sobreajustado. Tambi\u00e9n podemos crear un gr\u00e1fico similar con la ayuda de Scikit-learn, utilizando validation_curve . # utilizando validation curve de sklearn from sklearn.model_selection import validation_curve train_prec , eval_prec = validation_curve ( estimator = model , X = x_train , y = y_train , param_name = 'max_depth' , param_range = max_deep_list , cv = 5 ) train_mean = np . mean ( train_prec , axis = 1 ) train_std = np . std ( train_prec , axis = 1 ) test_mean = np . mean ( eval_prec , axis = 1 ) test_std = np . std ( eval_prec , axis = 1 ) # graficando las curvas plt . plot ( max_deep_list , train_mean , color = 'r' , marker = 'o' , markersize = 5 , label = 'entrenamiento' ) plt . fill_between ( max_deep_list , train_mean + train_std , train_mean - train_std , alpha = 0.15 , color = 'r' ) plt . plot ( max_deep_list , test_mean , color = 'b' , linestyle = '--' , marker = 's' , markersize = 5 , label = 'evaluacion' ) plt . fill_between ( max_deep_list , test_mean + test_std , test_mean - test_std , alpha = 0.15 , color = 'b' ) plt . legend ( loc = 'center right' ) plt . xlabel ( 'numero_nodos' ) plt . ylabel ( 'precision' ) plt . show ()","title":"Equilibrio en el ajuste de modelos"},{"location":"lectures/ml/overfitting/04_overfitting_underfitting/#referencia","text":"Underfitting vs Underfitting Overfitting and Underfitting With Machine Learning Algorithms","title":"Referencia"},{"location":"lectures/ml/overfitting/04_reducir_overfitting/","text":"Reducir el overfitting Algunas de las t\u00e9cnicas que podemos utilizar para reducir el overfitting, son: Recolectar m\u00e1s datos. Introducir una penalizaci\u00f3n a la complejidad con alguna t\u00e9cnica de regularizaci\u00f3n. Utilizar modelos ensamblados. Utilizar validaci\u00f3n cruzada. Optimizar los par\u00e1metros del modelo con grid search . Reducir la dimensi\u00f3n de los datos. Aplicar t\u00e9cnicas de selecci\u00f3n de atributos. Veremos ejemplos de algunos m\u00e9todos para reducir el sobreajuste (overfitting). Validaci\u00f3n cruzada La validaci\u00f3n cruzada se inicia mediante el fraccionamiento de un conjunto de datos en un n\u00famero \\(k\\) de particiones (generalmente entre 5 y 10) llamadas pliegues . La validaci\u00f3n cruzada luego itera entre los datos de evaluaci\u00f3n y entrenamiento \\(k\\) veces, de un modo particular. En cada iteraci\u00f3n de la validaci\u00f3n cruzada, un pliegue diferente se elige como los datos de evaluaci\u00f3n . En esta iteraci\u00f3n, los otros pliegues \\(k-1\\) se combinan para formar los datos de entrenamiento . Por lo tanto, en cada iteraci\u00f3n tenemos \\((k-1) / k\\) de los datos utilizados para el entrenamiento y \\(1 / k\\) utilizado para la evaluaci\u00f3n . Cada iteraci\u00f3n produce un modelo, y por lo tanto una estimaci\u00f3n del rendimiento de la generalizaci\u00f3n , por ejemplo, una estimaci\u00f3n de la precisi\u00f3n. Una vez finalizada la validaci\u00f3n cruzada, todos los ejemplos se han utilizado s\u00f3lo una vez para evaluar pero \\(k -1\\) veces para entrenar . En este punto tenemos estimaciones de rendimiento de todos los pliegues y podemos calcular la media y la desviaci\u00f3n est\u00e1ndar de la precisi\u00f3n del modelo. Veamos un ejemplo en python, ocupando el conjunto de datos make_classification . # librerias import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.datasets import make_classification from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier , DecisionTreeRegressor import random random . seed ( 1982 ) # semilla # graficos incrustados % matplotlib inline # parametros esteticos de seaborn sns . set_palette ( \"deep\" , desat = .6 ) sns . set_context ( rc = { \"figure.figsize\" : ( 12 , 4 )}) # Ejemplo en python - \u00e1rboles de decisi\u00f3n # dummy data con 100 atributos y 2 clases X , y = make_classification ( 10000 , 100 , n_informative = 3 , n_classes = 2 , random_state = 1982 ) # separ los datos en train y eval x_train , x_eval , y_train , y_eval = train_test_split ( X , y , test_size = 0.35 , train_size = 0.65 , random_state = 1982 ) # Grafico de ajuste del \u00e1rbol de decisi\u00f3n train_prec = [] eval_prec = [] max_deep_list = list ( range ( 2 , 20 )) # Ejemplo cross-validation from sklearn.model_selection import cross_validate , StratifiedKFold # creando pliegues skf = StratifiedKFold ( n_splits = 20 ) precision = [] model = DecisionTreeClassifier ( criterion = 'entropy' , max_depth = 5 ) skf . get_n_splits ( x_train , y_train ) for k , ( train_index , test_index ) in enumerate ( skf . split ( X , y )): X_train , X_test = X [ train_index ], X [ test_index ] y_train , y_test = y [ train_index ], y [ test_index ] model . fit ( X_train , y_train ) score = model . score ( X_test , y_test ) precision . append ( score ) print ( 'Pliegue: {0:} , Dist Clase: {1:} , Prec: {2:.3f} ' . format ( k + 1 , np . bincount ( y_train ), score )) Pliegue: 1, Dist Clase: [4763 4737], Prec: 0.928 Pliegue: 2, Dist Clase: [4763 4737], Prec: 0.914 Pliegue: 3, Dist Clase: [4763 4737], Prec: 0.914 Pliegue: 4, Dist Clase: [4763 4737], Prec: 0.938 Pliegue: 5, Dist Clase: [4763 4737], Prec: 0.922 Pliegue: 6, Dist Clase: [4763 4737], Prec: 0.938 Pliegue: 7, Dist Clase: [4763 4737], Prec: 0.924 Pliegue: 8, Dist Clase: [4762 4738], Prec: 0.938 Pliegue: 9, Dist Clase: [4762 4738], Prec: 0.936 Pliegue: 10, Dist Clase: [4762 4738], Prec: 0.908 Pliegue: 11, Dist Clase: [4762 4738], Prec: 0.936 Pliegue: 12, Dist Clase: [4762 4738], Prec: 0.938 Pliegue: 13, Dist Clase: [4762 4738], Prec: 0.934 Pliegue: 14, Dist Clase: [4762 4738], Prec: 0.920 Pliegue: 15, Dist Clase: [4762 4738], Prec: 0.930 Pliegue: 16, Dist Clase: [4762 4738], Prec: 0.928 Pliegue: 17, Dist Clase: [4762 4738], Prec: 0.924 Pliegue: 18, Dist Clase: [4762 4738], Prec: 0.926 Pliegue: 19, Dist Clase: [4762 4738], Prec: 0.936 Pliegue: 20, Dist Clase: [4762 4738], Prec: 0.920 En este ejemplo, utilizamos el iterador StratifiedKFold que nos proporciona Scikit-learn. Este iterador es una versi\u00f3n mejorada de la validaci\u00f3n cruzada, ya que cada pliegue va a estar estratificado para mantener las proporciones entre las clases del conjunto de datos original, lo que suele dar mejores estimaciones del sesgo y la varianza del modelo. Tambi\u00e9n podr\u00edamos utilizar cross_val_score que ya nos proporciona los resultados de la precisi\u00f3n que tuvo el modelo en cada pliegue . # Ejemplo con cross_val_score from sklearn.model_selection import cross_val_score # separ los datos en train y eval x_train , x_eval , y_train , y_eval = train_test_split ( X , y , test_size = 0.35 , train_size = 0.65 , random_state = 1982 ) model = DecisionTreeClassifier ( criterion = 'entropy' , max_depth = 5 ) precision = cross_val_score ( estimator = model , X = x_train , y = y_train , cv = 20 ) precision = [ round ( x , 2 ) for x in precision ] print ( 'Precisiones: {} ' . format ( precision )) print ( 'Precision promedio: {0: .3f} +/- {1: .3f} ' . format ( np . mean ( precision ), np . std ( precision ))) Precisiones: [0.93, 0.94, 0.92, 0.94, 0.93, 0.9, 0.92, 0.94, 0.94, 0.93, 0.94, 0.92, 0.91, 0.9, 0.94, 0.94, 0.93, 0.93, 0.93, 0.94] Precision promedio: 0.928 +/- 0.013 M\u00e1s datos y curvas de aprendizaje Muchas veces, reducir el Sobreajuste es tan f\u00e1cil como conseguir m\u00e1s datos, dame m\u00e1s datos y te predecir\u00e9 el futuro!. En la vida real nunca es una tarea tan sencilla conseguir m\u00e1s datos. Una t\u00e9cnica para reducir el sobreajuste son las curvas de aprendizaje , las cuales grafican la precisi\u00f3n en funci\u00f3n del tama\u00f1o de los datos de entrenamiento. Para graficar las curvas de aprendizaje es necesario ocupar el comando de sklearn llamado learning_curve . # Ejemplo Curvas de aprendizaje from sklearn.model_selection import learning_curve train_sizes , train_scores , test_scores = learning_curve ( estimator = model , X = x_train , y = y_train , train_sizes = np . linspace ( 0.1 , 1.0 , 20 ), cv = 10 , n_jobs =- 1 ) # calculo de metricas train_mean = np . mean ( train_scores , axis = 1 ) train_std = np . std ( train_scores , axis = 1 ) test_mean = np . mean ( test_scores , axis = 1 ) test_std = np . std ( test_scores , axis = 1 ) Veamos que el comando learning_curve va creando conjunto de datos, pero de distintos tama\u00f1os. # tamano conjunto de entrenamiento for k in range ( len ( train_sizes )): print ( 'Tama\u00f1o Conjunto {} : {} ' . format ( k + 1 , train_sizes [ k ])) Tama\u00f1o Conjunto 1: 585 Tama\u00f1o Conjunto 2: 862 Tama\u00f1o Conjunto 3: 1139 Tama\u00f1o Conjunto 4: 1416 Tama\u00f1o Conjunto 5: 1693 Tama\u00f1o Conjunto 6: 1970 Tama\u00f1o Conjunto 7: 2247 Tama\u00f1o Conjunto 8: 2524 Tama\u00f1o Conjunto 9: 2801 Tama\u00f1o Conjunto 10: 3078 Tama\u00f1o Conjunto 11: 3356 Tama\u00f1o Conjunto 12: 3633 Tama\u00f1o Conjunto 13: 3910 Tama\u00f1o Conjunto 14: 4187 Tama\u00f1o Conjunto 15: 4464 Tama\u00f1o Conjunto 16: 4741 Tama\u00f1o Conjunto 17: 5018 Tama\u00f1o Conjunto 18: 5295 Tama\u00f1o Conjunto 19: 5572 Tama\u00f1o Conjunto 20: 5850 Finalmente, graficamos las precisiones tanto para el conjunto de entranamiento como de evaluaci\u00f3n para los distintos conjuntos de datos generados. # graficando las curvas plt . figure ( figsize = ( 12 , 8 )) plt . plot ( train_sizes , train_mean , color = 'r' , marker = 'o' , markersize = 5 , label = 'entrenamiento' ) plt . fill_between ( train_sizes , train_mean + train_std , train_mean - train_std , alpha = 0.15 , color = 'r' ) plt . plot ( train_sizes , test_mean , color = 'b' , linestyle = '--' , marker = 's' , markersize = 5 , label = 'evaluacion' ) plt . fill_between ( train_sizes , test_mean + test_std , test_mean - test_std , alpha = 0.15 , color = 'b' ) plt . grid () plt . title ( 'Curva de aprendizaje' ) plt . legend ( loc = 'upper right' ) plt . xlabel ( 'Cant de ejemplos de entrenamiento' ) plt . ylabel ( 'Precision' ) plt . show () En este gr\u00e1fico podemos concluir que: Con pocos datos la precisi\u00f3n entre los datos de entrenamiento y los de evaluaci\u00f3n son muy distintas y luego a medida que la cantidad de datos va aumentando, el modelo puede generalizar mucho mejor y las precisiones se comienzan a emparejar. Este gr\u00e1fico tambi\u00e9n puede ser importante a la hora de decidir invertir en la obtenci\u00f3n de m\u00e1s datos, ya que por ejemplo nos indica que a partir las 2500 muestras, el modelo ya no gana mucha m\u00e1s precisi\u00f3n a pesar de obtener m\u00e1s datos. Optimizaci\u00f3n de par\u00e1metros con Grid Search La mayor\u00eda de los modelos de Machine Learning cuentan con varios par\u00e1metros para ajustar su comportamiento, por lo tanto, otra alternativa que tenemos para reducir el Sobreajuste es optimizar estos par\u00e1metros por medio de un proceso conocido como grid search e intentar encontrar la combinaci\u00f3n ideal que nos proporcione mayor precisi\u00f3n. El enfoque que utiliza grid search es bastante simple, se trata de una b\u00fasqueda exhaustiva por el paradigma de fuerza bruta en el que se especifica una lista de valores para diferentes par\u00e1metros, y la computadora eval\u00faa el rendimiento del modelo para cada combinaci\u00f3n de \u00e9stos par\u00e1metros para obtener el conjunto \u00f3ptimo que nos brinda el mayor rendimiento. # Ejemplo de grid search con SVM. from sklearn.model_selection import GridSearchCV # creaci\u00f3n del modelo model = DecisionTreeClassifier () # rango de parametros rango_criterion = [ 'gini' , 'entropy' ] rango_max_depth = np . array ( [ 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 15 , 20 , 30 , 40 , 50 , 70 , 90 , 120 , 150 ]) param_grid = dict ( criterion = rango_criterion , max_depth = rango_max_depth ) param_grid {'criterion': ['gini', 'entropy'], 'max_depth': array([ 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 20, 30, 40, 50, 70, 90, 120, 150])} # aplicar greed search gs = GridSearchCV ( estimator = model , param_grid = param_grid , scoring = 'accuracy' , cv = 5 , n_jobs =- 1 ) gs = gs . fit ( x_train , y_train ) # imprimir resultados print ( gs . best_score_ ) print ( gs . best_params_ ) 0.9329230769230769 {'criterion': 'entropy', 'max_depth': 6} # utilizando el mejor modelo mejor_modelo = gs . best_estimator_ mejor_modelo . fit ( x_train , y_train ) print ( 'Precisi\u00f3n: {0:.3f} ' . format ( mejor_modelo . score ( x_eval , y_eval ))) Precisi\u00f3n: 0.938 En este ejemplo, primero utilizamos el objeto GridSearchCV que nos permite realizar grid search junto con validaci\u00f3n cruzada, luego comenzamos a ajustar el modelo con las diferentes combinaciones de los valores de los par\u00e1metros criterion y max_depth . Finalmente imprimimos el mejor resultado de precisi\u00f3n y los valores de los par\u00e1metros que utilizamos para obtenerlos; por \u00faltimo utilizamos este mejor modelo para realizar las predicciones con los datos de evaluaci\u00f3n . Podemos ver que la precisi\u00f3n que obtuvimos con los datos de evaluaci\u00f3n es casi id\u00e9ntica a la que nos indic\u00f3 grid search , lo que indica que el modelo generaliza muy bien. Reducci\u00f3n de dimensionalidad La reducci\u00f3n de dimensiones es frecuentemente usada como una etapa de preproceso en el entrenamiento de sistemas, y consiste en escoger un subconjunto de variables, de tal manera, que el espacio de caracter\u00edsticas quede \u00f3ptimamente reducido de acuerdo a un criterio de evaluaci\u00f3n, cuyo fin es distinguir el subconjunto que representa mejor el espacio inicial de entrenamiento. Como cada caracter\u00edstica que se incluye en el an\u00e1lisis, puede incrementar el costo y el tiempo de proceso de los sistemas, hay una fuerte motivaci\u00f3n para dise\u00f1ar e implementar sistemas con peque\u00f1os conjuntos de caracter\u00edsticas. Sin dejar de lado, que al mismo tiempo, hay una opuesta necesidad de incluir un conjunto suficiente de caracter\u00edsticas para lograr un alto rendimiento. La reducci\u00f3n de dimensionalidad se puede separar en dos tipos: Extracci\u00f3n de atributos y Selecci\u00f3n de aributos . Extracci\u00f3n de atributos La extracci\u00f3n de atributos comienza a partir de un conjunto inicial de datos medidos y crea valores derivados (caracter\u00edsticas) destinados a ser informativos y no redundantes, lo que facilita los pasos de aprendizaje y generalizaci\u00f3n posteriores, y en algunos casos conduce a a mejores interpretaciones humanas. Cuando los datos de entrada a un algoritmo son demasiado grandes para ser procesados y se sospecha que son redundantes (por ejemplo, la misma medici\u00f3n en pies y metros, o la repetitividad de las im\u00e1genes presentadas como p\u00edxeles), entonces se puede transformar en un conjunto reducido de caracter\u00edsticas (tambi\u00e9n denominado un vector de caracter\u00edsticas). Estos algoritmos fueron analizados con profundidad en la secci\u00f3n de An\u00e1lisis no supervisados - Reducci\u00f3n de la dimensionalidad . Selecci\u00f3n de atributos Proceso por el cual seleccionamos un subconjunto de atributos (representados por cada una de las columnas en un datasetde forma tabular) que son m\u00e1s relevantes para la construcci\u00f3n del modelo predictivo sobre el que estamos trabajando. El objetivo de la selecci\u00f3n de atributos es : * mejorar la capacidad predictiva de nuestro modelo, * proporcionando modelos predictivos m\u00e1s r\u00e1pidos y eficientes, * proporcionar una mejor comprensi\u00f3n del proceso subyacente que gener\u00f3 los datos. Los m\u00e9todos de selecci\u00f3n de atributos se pueden utilizar para identificar y eliminar los atributos innecesarios, irrelevantes y redundantes que no contribuyen a la exactitud del modelo predictivo o incluso puedan disminuir su precisi\u00f3n. Algoritmos para selecci\u00f3n de atributos Podemos encontrar dos clases generales de algoritmos de selecci\u00f3n de atributos : los m\u00e9todos de filtrado, y los m\u00e9todos empaquetados. M\u00e9todos de filtrado : Estos m\u00e9todos aplican una medida estad\u00edstica para asignar una puntuaci\u00f3n a cada atributo. Los atributos luego son clasificados de acuerdo a su puntuaci\u00f3n y son, o bien seleccionados para su conservaci\u00f3n o eliminados del conjunto de datos. Los m\u00e9todos de filtrado son a menudo univariantes y consideran a cada atributo en forma independiente, o con respecto a la variable dependiente. Ejemplos : prueba de Chi cuadrado , prueba F de Fisher , ratio de ganancia de informaci\u00f3n y los coeficientes de correlaci\u00f3n . M\u00e9todos empaquetados : Estos m\u00e9todos consideran la selecci\u00f3n de un conjunto de atributos como un problema de b\u00fasqueda, en donde las diferentes combinaciones son evaluadas y comparadas. Para hacer estas evaluaciones se utiliza un modelo predictivo y luego se asigna una puntuaci\u00f3n a cada combinaci\u00f3n basada en la precisi\u00f3n del modelo. Un ejemplo de este m\u00e9todo es el algoritmo de eliminaci\u00f3n recursiva de atributos. Un m\u00e9todo popular en sklearn es el m\u00e9todo SelectKBest , el cual selecciona las caracter\u00edsticas de acuerdo con las \\(k\\) puntuaciones m\u00e1s altas (de acuerdo al criterio escogido). Para entender este conceptos, transformemos el conjunto de datos anterior a formato pandas DataFrame. # Datos X , y = make_classification ( 10000 , 100 , n_informative = 3 , n_classes = 2 , random_state = 1982 ) df = pd . DataFrame ( X ) df . columns = [ f 'V { k } ' for k in range ( 1 , X . shape [ 1 ] + 1 )] df [ 'y' ] = y df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 ... V92 V93 V94 V95 V96 V97 V98 V99 V100 y 0 0.949283 -1.075706 -0.105733 -0.000047 -0.278974 0.510083 -0.778030 -1.976158 -1.201534 -1.047384 ... -0.630209 -0.331225 -0.202422 -1.786323 1.540031 1.119424 0.507775 -0.848286 -0.027485 1 1 0.183904 0.524554 -1.561357 -1.950628 1.077846 -0.598287 0.153160 -1.206113 0.673170 -0.843770 ... -1.015067 0.319214 0.240570 -2.205400 -0.430933 -0.313175 0.752012 -0.070265 1.390394 0 2 0.499151 -0.625950 2.977037 0.612030 -0.102034 2.076814 1.661343 1.310895 -1.115465 -0.544276 ... 0.311830 -1.130865 0.247865 -0.499241 -1.595737 -0.496805 -0.917257 0.976909 -1.518979 0 3 -0.172063 -0.599516 0.154253 -0.593797 0.931374 0.939714 1.107241 0.146723 -0.446275 0.095896 ... -1.641808 -1.170021 0.815094 -0.722564 -0.263476 -0.715898 1.962313 1.076288 -2.259682 0 4 -0.396408 0.876210 -0.791795 0.999677 0.046859 -0.166211 -0.549437 0.344644 0.349981 -0.207106 ... 1.307020 0.876912 0.882497 -0.704791 -0.743942 -0.075060 0.622693 0.751576 0.907325 0 5 rows \u00d7 101 columns Comencemos con un simple algoritmo univariante que aplica el m\u00e9todo de filtrado. Para esto vamos a utilizar los objetos SelectKBest y f_classif del paquete sklearn.feature_selection . Este algoritmo selecciona a los mejores atributos bas\u00e1ndose en una prueba estad\u00edstica univariante . Al objeto SelectKBest le pasamos la prueba estad\u00edstica que vamos a a aplicar, en este caso una prueba F definida por el objeto f_classif , junto con el n\u00famero de atributos a seleccionar. El algoritmo va a aplicar la prueba a todos los atributos y va a seleccionar los que mejor resultado obtuvieron. from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import f_classif # Separamos las columnas objetivo x_training = df . drop ([ 'y' ,], axis = 1 ) y_training = df [ 'y' ] # Aplicando el algoritmo univariante de prueba F. k = 15 # n\u00famero de atributos a seleccionar columnas = list ( x_training . columns . values ) seleccionadas = SelectKBest ( f_classif , k = k ) . fit ( x_training , y_training ) catrib = seleccionadas . get_support () atributos = [ columnas [ i ] for i in list ( catrib . nonzero ()[ 0 ])] atributos ['V1', 'V42', 'V46', 'V49', 'V62', 'V64', 'V66', 'V68', 'V69', 'V75', 'V82', 'V86', 'V89', 'V98', 'V100'] Como podemos ver, el algoritmo nos seleccion\u00f3 la cantidad de atributos que le indicamos; en este ejemplo decidimos seleccionar solo 15; obviamente, cuando armemos nuestro modelo final vamos a tomar un n\u00famero mayor de atributos. Ahora se proceder\u00e1 a comparar los resultados de entrenar un modelo en particular con todas las variables y el subconjunto de variables seleccionadas. from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from metrics_classification import summary_metrics %% timeit # Entrenamiento con todas las variables X = df . drop ( 'y' , axis = 1 ) Y = df [ 'y' ] # split dataset X_train , X_test , Y_train , Y_test = train_test_split ( X , Y , test_size = 0.2 , random_state = 2 ) # Creando el modelo rlog = LogisticRegression () rlog . fit ( X_train , Y_train ) # ajustando el modelo predicciones = rlog . predict ( X_test ) df_pred = pd . DataFrame ({ 'y' : Y_test , 'yhat' : predicciones }) df_s1 = summary_metrics ( df_pred ) . assign ( name = 'Todas las variables' ) 82.4 ms \u00b1 6.51 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) %% timeit # Entrenamiento con las variables seleccionadas X = df [ atributos ] Y = df [ 'y' ] # split dataset X_train , X_test , Y_train , Y_test = train_test_split ( X , Y , test_size = 0.2 , random_state = 2 ) # Creando el modelo rlog = LogisticRegression () rlog . fit ( X_train , Y_train ) # ajustando el modelo predicciones = rlog . predict ( X_test ) df_pred = pd . DataFrame ({ 'y' : Y_test , 'yhat' : predicciones }) df_s2 = summary_metrics ( df_pred ) . assign ( name = 'Variables Seleccionadas' ) 60.4 ms \u00b1 8.62 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) Juntando ambos resultados: # juntar resultados en formato dataframe pd . concat ([ df_s1 , df_s2 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } accuracy recall precision fscore name 0 0.8905 0.8906 0.8907 0.8905 Todas las variables 0 0.8985 0.8986 0.8986 0.8985 Variables Seleccionadas Las m\u00e9tricas para ambos casos son parecidas y el tiempo de ejecuci\u00f3n del modelo con menos variable resulta ser menor (lo cual era algo esperable). Lo cual nos muestra que trabajando con menos variables, se puede captar las caracter\u00edsticas m\u00e1s relevante del problema, y en la medida que se trabaje con m\u00e1s datos, las mejoras a nivel de capacidad de c\u00f3mputo tendr\u00e1n un mejor desempe\u00f1o. Referencia K-Fold Cross Validation Cross Validation and Grid Search for Model Selection in Python Feature selection for supervised models using SelectKBest","title":"Reducir Overfiting"},{"location":"lectures/ml/overfitting/04_reducir_overfitting/#reducir-el-overfitting","text":"Algunas de las t\u00e9cnicas que podemos utilizar para reducir el overfitting, son: Recolectar m\u00e1s datos. Introducir una penalizaci\u00f3n a la complejidad con alguna t\u00e9cnica de regularizaci\u00f3n. Utilizar modelos ensamblados. Utilizar validaci\u00f3n cruzada. Optimizar los par\u00e1metros del modelo con grid search . Reducir la dimensi\u00f3n de los datos. Aplicar t\u00e9cnicas de selecci\u00f3n de atributos. Veremos ejemplos de algunos m\u00e9todos para reducir el sobreajuste (overfitting).","title":"Reducir el overfitting"},{"location":"lectures/ml/overfitting/04_reducir_overfitting/#validacion-cruzada","text":"La validaci\u00f3n cruzada se inicia mediante el fraccionamiento de un conjunto de datos en un n\u00famero \\(k\\) de particiones (generalmente entre 5 y 10) llamadas pliegues . La validaci\u00f3n cruzada luego itera entre los datos de evaluaci\u00f3n y entrenamiento \\(k\\) veces, de un modo particular. En cada iteraci\u00f3n de la validaci\u00f3n cruzada, un pliegue diferente se elige como los datos de evaluaci\u00f3n . En esta iteraci\u00f3n, los otros pliegues \\(k-1\\) se combinan para formar los datos de entrenamiento . Por lo tanto, en cada iteraci\u00f3n tenemos \\((k-1) / k\\) de los datos utilizados para el entrenamiento y \\(1 / k\\) utilizado para la evaluaci\u00f3n . Cada iteraci\u00f3n produce un modelo, y por lo tanto una estimaci\u00f3n del rendimiento de la generalizaci\u00f3n , por ejemplo, una estimaci\u00f3n de la precisi\u00f3n. Una vez finalizada la validaci\u00f3n cruzada, todos los ejemplos se han utilizado s\u00f3lo una vez para evaluar pero \\(k -1\\) veces para entrenar . En este punto tenemos estimaciones de rendimiento de todos los pliegues y podemos calcular la media y la desviaci\u00f3n est\u00e1ndar de la precisi\u00f3n del modelo. Veamos un ejemplo en python, ocupando el conjunto de datos make_classification . # librerias import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.datasets import make_classification from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier , DecisionTreeRegressor import random random . seed ( 1982 ) # semilla # graficos incrustados % matplotlib inline # parametros esteticos de seaborn sns . set_palette ( \"deep\" , desat = .6 ) sns . set_context ( rc = { \"figure.figsize\" : ( 12 , 4 )}) # Ejemplo en python - \u00e1rboles de decisi\u00f3n # dummy data con 100 atributos y 2 clases X , y = make_classification ( 10000 , 100 , n_informative = 3 , n_classes = 2 , random_state = 1982 ) # separ los datos en train y eval x_train , x_eval , y_train , y_eval = train_test_split ( X , y , test_size = 0.35 , train_size = 0.65 , random_state = 1982 ) # Grafico de ajuste del \u00e1rbol de decisi\u00f3n train_prec = [] eval_prec = [] max_deep_list = list ( range ( 2 , 20 )) # Ejemplo cross-validation from sklearn.model_selection import cross_validate , StratifiedKFold # creando pliegues skf = StratifiedKFold ( n_splits = 20 ) precision = [] model = DecisionTreeClassifier ( criterion = 'entropy' , max_depth = 5 ) skf . get_n_splits ( x_train , y_train ) for k , ( train_index , test_index ) in enumerate ( skf . split ( X , y )): X_train , X_test = X [ train_index ], X [ test_index ] y_train , y_test = y [ train_index ], y [ test_index ] model . fit ( X_train , y_train ) score = model . score ( X_test , y_test ) precision . append ( score ) print ( 'Pliegue: {0:} , Dist Clase: {1:} , Prec: {2:.3f} ' . format ( k + 1 , np . bincount ( y_train ), score )) Pliegue: 1, Dist Clase: [4763 4737], Prec: 0.928 Pliegue: 2, Dist Clase: [4763 4737], Prec: 0.914 Pliegue: 3, Dist Clase: [4763 4737], Prec: 0.914 Pliegue: 4, Dist Clase: [4763 4737], Prec: 0.938 Pliegue: 5, Dist Clase: [4763 4737], Prec: 0.922 Pliegue: 6, Dist Clase: [4763 4737], Prec: 0.938 Pliegue: 7, Dist Clase: [4763 4737], Prec: 0.924 Pliegue: 8, Dist Clase: [4762 4738], Prec: 0.938 Pliegue: 9, Dist Clase: [4762 4738], Prec: 0.936 Pliegue: 10, Dist Clase: [4762 4738], Prec: 0.908 Pliegue: 11, Dist Clase: [4762 4738], Prec: 0.936 Pliegue: 12, Dist Clase: [4762 4738], Prec: 0.938 Pliegue: 13, Dist Clase: [4762 4738], Prec: 0.934 Pliegue: 14, Dist Clase: [4762 4738], Prec: 0.920 Pliegue: 15, Dist Clase: [4762 4738], Prec: 0.930 Pliegue: 16, Dist Clase: [4762 4738], Prec: 0.928 Pliegue: 17, Dist Clase: [4762 4738], Prec: 0.924 Pliegue: 18, Dist Clase: [4762 4738], Prec: 0.926 Pliegue: 19, Dist Clase: [4762 4738], Prec: 0.936 Pliegue: 20, Dist Clase: [4762 4738], Prec: 0.920 En este ejemplo, utilizamos el iterador StratifiedKFold que nos proporciona Scikit-learn. Este iterador es una versi\u00f3n mejorada de la validaci\u00f3n cruzada, ya que cada pliegue va a estar estratificado para mantener las proporciones entre las clases del conjunto de datos original, lo que suele dar mejores estimaciones del sesgo y la varianza del modelo. Tambi\u00e9n podr\u00edamos utilizar cross_val_score que ya nos proporciona los resultados de la precisi\u00f3n que tuvo el modelo en cada pliegue . # Ejemplo con cross_val_score from sklearn.model_selection import cross_val_score # separ los datos en train y eval x_train , x_eval , y_train , y_eval = train_test_split ( X , y , test_size = 0.35 , train_size = 0.65 , random_state = 1982 ) model = DecisionTreeClassifier ( criterion = 'entropy' , max_depth = 5 ) precision = cross_val_score ( estimator = model , X = x_train , y = y_train , cv = 20 ) precision = [ round ( x , 2 ) for x in precision ] print ( 'Precisiones: {} ' . format ( precision )) print ( 'Precision promedio: {0: .3f} +/- {1: .3f} ' . format ( np . mean ( precision ), np . std ( precision ))) Precisiones: [0.93, 0.94, 0.92, 0.94, 0.93, 0.9, 0.92, 0.94, 0.94, 0.93, 0.94, 0.92, 0.91, 0.9, 0.94, 0.94, 0.93, 0.93, 0.93, 0.94] Precision promedio: 0.928 +/- 0.013","title":"Validaci\u00f3n cruzada"},{"location":"lectures/ml/overfitting/04_reducir_overfitting/#mas-datos-y-curvas-de-aprendizaje","text":"Muchas veces, reducir el Sobreajuste es tan f\u00e1cil como conseguir m\u00e1s datos, dame m\u00e1s datos y te predecir\u00e9 el futuro!. En la vida real nunca es una tarea tan sencilla conseguir m\u00e1s datos. Una t\u00e9cnica para reducir el sobreajuste son las curvas de aprendizaje , las cuales grafican la precisi\u00f3n en funci\u00f3n del tama\u00f1o de los datos de entrenamiento. Para graficar las curvas de aprendizaje es necesario ocupar el comando de sklearn llamado learning_curve . # Ejemplo Curvas de aprendizaje from sklearn.model_selection import learning_curve train_sizes , train_scores , test_scores = learning_curve ( estimator = model , X = x_train , y = y_train , train_sizes = np . linspace ( 0.1 , 1.0 , 20 ), cv = 10 , n_jobs =- 1 ) # calculo de metricas train_mean = np . mean ( train_scores , axis = 1 ) train_std = np . std ( train_scores , axis = 1 ) test_mean = np . mean ( test_scores , axis = 1 ) test_std = np . std ( test_scores , axis = 1 ) Veamos que el comando learning_curve va creando conjunto de datos, pero de distintos tama\u00f1os. # tamano conjunto de entrenamiento for k in range ( len ( train_sizes )): print ( 'Tama\u00f1o Conjunto {} : {} ' . format ( k + 1 , train_sizes [ k ])) Tama\u00f1o Conjunto 1: 585 Tama\u00f1o Conjunto 2: 862 Tama\u00f1o Conjunto 3: 1139 Tama\u00f1o Conjunto 4: 1416 Tama\u00f1o Conjunto 5: 1693 Tama\u00f1o Conjunto 6: 1970 Tama\u00f1o Conjunto 7: 2247 Tama\u00f1o Conjunto 8: 2524 Tama\u00f1o Conjunto 9: 2801 Tama\u00f1o Conjunto 10: 3078 Tama\u00f1o Conjunto 11: 3356 Tama\u00f1o Conjunto 12: 3633 Tama\u00f1o Conjunto 13: 3910 Tama\u00f1o Conjunto 14: 4187 Tama\u00f1o Conjunto 15: 4464 Tama\u00f1o Conjunto 16: 4741 Tama\u00f1o Conjunto 17: 5018 Tama\u00f1o Conjunto 18: 5295 Tama\u00f1o Conjunto 19: 5572 Tama\u00f1o Conjunto 20: 5850 Finalmente, graficamos las precisiones tanto para el conjunto de entranamiento como de evaluaci\u00f3n para los distintos conjuntos de datos generados. # graficando las curvas plt . figure ( figsize = ( 12 , 8 )) plt . plot ( train_sizes , train_mean , color = 'r' , marker = 'o' , markersize = 5 , label = 'entrenamiento' ) plt . fill_between ( train_sizes , train_mean + train_std , train_mean - train_std , alpha = 0.15 , color = 'r' ) plt . plot ( train_sizes , test_mean , color = 'b' , linestyle = '--' , marker = 's' , markersize = 5 , label = 'evaluacion' ) plt . fill_between ( train_sizes , test_mean + test_std , test_mean - test_std , alpha = 0.15 , color = 'b' ) plt . grid () plt . title ( 'Curva de aprendizaje' ) plt . legend ( loc = 'upper right' ) plt . xlabel ( 'Cant de ejemplos de entrenamiento' ) plt . ylabel ( 'Precision' ) plt . show () En este gr\u00e1fico podemos concluir que: Con pocos datos la precisi\u00f3n entre los datos de entrenamiento y los de evaluaci\u00f3n son muy distintas y luego a medida que la cantidad de datos va aumentando, el modelo puede generalizar mucho mejor y las precisiones se comienzan a emparejar. Este gr\u00e1fico tambi\u00e9n puede ser importante a la hora de decidir invertir en la obtenci\u00f3n de m\u00e1s datos, ya que por ejemplo nos indica que a partir las 2500 muestras, el modelo ya no gana mucha m\u00e1s precisi\u00f3n a pesar de obtener m\u00e1s datos.","title":"M\u00e1s datos y curvas de aprendizaje"},{"location":"lectures/ml/overfitting/04_reducir_overfitting/#optimizacion-de-parametros-con-grid-search","text":"La mayor\u00eda de los modelos de Machine Learning cuentan con varios par\u00e1metros para ajustar su comportamiento, por lo tanto, otra alternativa que tenemos para reducir el Sobreajuste es optimizar estos par\u00e1metros por medio de un proceso conocido como grid search e intentar encontrar la combinaci\u00f3n ideal que nos proporcione mayor precisi\u00f3n. El enfoque que utiliza grid search es bastante simple, se trata de una b\u00fasqueda exhaustiva por el paradigma de fuerza bruta en el que se especifica una lista de valores para diferentes par\u00e1metros, y la computadora eval\u00faa el rendimiento del modelo para cada combinaci\u00f3n de \u00e9stos par\u00e1metros para obtener el conjunto \u00f3ptimo que nos brinda el mayor rendimiento. # Ejemplo de grid search con SVM. from sklearn.model_selection import GridSearchCV # creaci\u00f3n del modelo model = DecisionTreeClassifier () # rango de parametros rango_criterion = [ 'gini' , 'entropy' ] rango_max_depth = np . array ( [ 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 15 , 20 , 30 , 40 , 50 , 70 , 90 , 120 , 150 ]) param_grid = dict ( criterion = rango_criterion , max_depth = rango_max_depth ) param_grid {'criterion': ['gini', 'entropy'], 'max_depth': array([ 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 20, 30, 40, 50, 70, 90, 120, 150])} # aplicar greed search gs = GridSearchCV ( estimator = model , param_grid = param_grid , scoring = 'accuracy' , cv = 5 , n_jobs =- 1 ) gs = gs . fit ( x_train , y_train ) # imprimir resultados print ( gs . best_score_ ) print ( gs . best_params_ ) 0.9329230769230769 {'criterion': 'entropy', 'max_depth': 6} # utilizando el mejor modelo mejor_modelo = gs . best_estimator_ mejor_modelo . fit ( x_train , y_train ) print ( 'Precisi\u00f3n: {0:.3f} ' . format ( mejor_modelo . score ( x_eval , y_eval ))) Precisi\u00f3n: 0.938 En este ejemplo, primero utilizamos el objeto GridSearchCV que nos permite realizar grid search junto con validaci\u00f3n cruzada, luego comenzamos a ajustar el modelo con las diferentes combinaciones de los valores de los par\u00e1metros criterion y max_depth . Finalmente imprimimos el mejor resultado de precisi\u00f3n y los valores de los par\u00e1metros que utilizamos para obtenerlos; por \u00faltimo utilizamos este mejor modelo para realizar las predicciones con los datos de evaluaci\u00f3n . Podemos ver que la precisi\u00f3n que obtuvimos con los datos de evaluaci\u00f3n es casi id\u00e9ntica a la que nos indic\u00f3 grid search , lo que indica que el modelo generaliza muy bien.","title":"Optimizaci\u00f3n de par\u00e1metros con Grid Search"},{"location":"lectures/ml/overfitting/04_reducir_overfitting/#reduccion-de-dimensionalidad","text":"La reducci\u00f3n de dimensiones es frecuentemente usada como una etapa de preproceso en el entrenamiento de sistemas, y consiste en escoger un subconjunto de variables, de tal manera, que el espacio de caracter\u00edsticas quede \u00f3ptimamente reducido de acuerdo a un criterio de evaluaci\u00f3n, cuyo fin es distinguir el subconjunto que representa mejor el espacio inicial de entrenamiento. Como cada caracter\u00edstica que se incluye en el an\u00e1lisis, puede incrementar el costo y el tiempo de proceso de los sistemas, hay una fuerte motivaci\u00f3n para dise\u00f1ar e implementar sistemas con peque\u00f1os conjuntos de caracter\u00edsticas. Sin dejar de lado, que al mismo tiempo, hay una opuesta necesidad de incluir un conjunto suficiente de caracter\u00edsticas para lograr un alto rendimiento. La reducci\u00f3n de dimensionalidad se puede separar en dos tipos: Extracci\u00f3n de atributos y Selecci\u00f3n de aributos .","title":"Reducci\u00f3n de dimensionalidad"},{"location":"lectures/ml/overfitting/04_reducir_overfitting/#extraccion-de-atributos","text":"La extracci\u00f3n de atributos comienza a partir de un conjunto inicial de datos medidos y crea valores derivados (caracter\u00edsticas) destinados a ser informativos y no redundantes, lo que facilita los pasos de aprendizaje y generalizaci\u00f3n posteriores, y en algunos casos conduce a a mejores interpretaciones humanas. Cuando los datos de entrada a un algoritmo son demasiado grandes para ser procesados y se sospecha que son redundantes (por ejemplo, la misma medici\u00f3n en pies y metros, o la repetitividad de las im\u00e1genes presentadas como p\u00edxeles), entonces se puede transformar en un conjunto reducido de caracter\u00edsticas (tambi\u00e9n denominado un vector de caracter\u00edsticas). Estos algoritmos fueron analizados con profundidad en la secci\u00f3n de An\u00e1lisis no supervisados - Reducci\u00f3n de la dimensionalidad .","title":"Extracci\u00f3n de atributos"},{"location":"lectures/ml/overfitting/04_reducir_overfitting/#seleccion-de-atributos","text":"Proceso por el cual seleccionamos un subconjunto de atributos (representados por cada una de las columnas en un datasetde forma tabular) que son m\u00e1s relevantes para la construcci\u00f3n del modelo predictivo sobre el que estamos trabajando. El objetivo de la selecci\u00f3n de atributos es : * mejorar la capacidad predictiva de nuestro modelo, * proporcionando modelos predictivos m\u00e1s r\u00e1pidos y eficientes, * proporcionar una mejor comprensi\u00f3n del proceso subyacente que gener\u00f3 los datos. Los m\u00e9todos de selecci\u00f3n de atributos se pueden utilizar para identificar y eliminar los atributos innecesarios, irrelevantes y redundantes que no contribuyen a la exactitud del modelo predictivo o incluso puedan disminuir su precisi\u00f3n. Algoritmos para selecci\u00f3n de atributos Podemos encontrar dos clases generales de algoritmos de selecci\u00f3n de atributos : los m\u00e9todos de filtrado, y los m\u00e9todos empaquetados. M\u00e9todos de filtrado : Estos m\u00e9todos aplican una medida estad\u00edstica para asignar una puntuaci\u00f3n a cada atributo. Los atributos luego son clasificados de acuerdo a su puntuaci\u00f3n y son, o bien seleccionados para su conservaci\u00f3n o eliminados del conjunto de datos. Los m\u00e9todos de filtrado son a menudo univariantes y consideran a cada atributo en forma independiente, o con respecto a la variable dependiente. Ejemplos : prueba de Chi cuadrado , prueba F de Fisher , ratio de ganancia de informaci\u00f3n y los coeficientes de correlaci\u00f3n . M\u00e9todos empaquetados : Estos m\u00e9todos consideran la selecci\u00f3n de un conjunto de atributos como un problema de b\u00fasqueda, en donde las diferentes combinaciones son evaluadas y comparadas. Para hacer estas evaluaciones se utiliza un modelo predictivo y luego se asigna una puntuaci\u00f3n a cada combinaci\u00f3n basada en la precisi\u00f3n del modelo. Un ejemplo de este m\u00e9todo es el algoritmo de eliminaci\u00f3n recursiva de atributos. Un m\u00e9todo popular en sklearn es el m\u00e9todo SelectKBest , el cual selecciona las caracter\u00edsticas de acuerdo con las \\(k\\) puntuaciones m\u00e1s altas (de acuerdo al criterio escogido). Para entender este conceptos, transformemos el conjunto de datos anterior a formato pandas DataFrame. # Datos X , y = make_classification ( 10000 , 100 , n_informative = 3 , n_classes = 2 , random_state = 1982 ) df = pd . DataFrame ( X ) df . columns = [ f 'V { k } ' for k in range ( 1 , X . shape [ 1 ] + 1 )] df [ 'y' ] = y df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 ... V92 V93 V94 V95 V96 V97 V98 V99 V100 y 0 0.949283 -1.075706 -0.105733 -0.000047 -0.278974 0.510083 -0.778030 -1.976158 -1.201534 -1.047384 ... -0.630209 -0.331225 -0.202422 -1.786323 1.540031 1.119424 0.507775 -0.848286 -0.027485 1 1 0.183904 0.524554 -1.561357 -1.950628 1.077846 -0.598287 0.153160 -1.206113 0.673170 -0.843770 ... -1.015067 0.319214 0.240570 -2.205400 -0.430933 -0.313175 0.752012 -0.070265 1.390394 0 2 0.499151 -0.625950 2.977037 0.612030 -0.102034 2.076814 1.661343 1.310895 -1.115465 -0.544276 ... 0.311830 -1.130865 0.247865 -0.499241 -1.595737 -0.496805 -0.917257 0.976909 -1.518979 0 3 -0.172063 -0.599516 0.154253 -0.593797 0.931374 0.939714 1.107241 0.146723 -0.446275 0.095896 ... -1.641808 -1.170021 0.815094 -0.722564 -0.263476 -0.715898 1.962313 1.076288 -2.259682 0 4 -0.396408 0.876210 -0.791795 0.999677 0.046859 -0.166211 -0.549437 0.344644 0.349981 -0.207106 ... 1.307020 0.876912 0.882497 -0.704791 -0.743942 -0.075060 0.622693 0.751576 0.907325 0 5 rows \u00d7 101 columns Comencemos con un simple algoritmo univariante que aplica el m\u00e9todo de filtrado. Para esto vamos a utilizar los objetos SelectKBest y f_classif del paquete sklearn.feature_selection . Este algoritmo selecciona a los mejores atributos bas\u00e1ndose en una prueba estad\u00edstica univariante . Al objeto SelectKBest le pasamos la prueba estad\u00edstica que vamos a a aplicar, en este caso una prueba F definida por el objeto f_classif , junto con el n\u00famero de atributos a seleccionar. El algoritmo va a aplicar la prueba a todos los atributos y va a seleccionar los que mejor resultado obtuvieron. from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import f_classif # Separamos las columnas objetivo x_training = df . drop ([ 'y' ,], axis = 1 ) y_training = df [ 'y' ] # Aplicando el algoritmo univariante de prueba F. k = 15 # n\u00famero de atributos a seleccionar columnas = list ( x_training . columns . values ) seleccionadas = SelectKBest ( f_classif , k = k ) . fit ( x_training , y_training ) catrib = seleccionadas . get_support () atributos = [ columnas [ i ] for i in list ( catrib . nonzero ()[ 0 ])] atributos ['V1', 'V42', 'V46', 'V49', 'V62', 'V64', 'V66', 'V68', 'V69', 'V75', 'V82', 'V86', 'V89', 'V98', 'V100'] Como podemos ver, el algoritmo nos seleccion\u00f3 la cantidad de atributos que le indicamos; en este ejemplo decidimos seleccionar solo 15; obviamente, cuando armemos nuestro modelo final vamos a tomar un n\u00famero mayor de atributos. Ahora se proceder\u00e1 a comparar los resultados de entrenar un modelo en particular con todas las variables y el subconjunto de variables seleccionadas. from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from metrics_classification import summary_metrics %% timeit # Entrenamiento con todas las variables X = df . drop ( 'y' , axis = 1 ) Y = df [ 'y' ] # split dataset X_train , X_test , Y_train , Y_test = train_test_split ( X , Y , test_size = 0.2 , random_state = 2 ) # Creando el modelo rlog = LogisticRegression () rlog . fit ( X_train , Y_train ) # ajustando el modelo predicciones = rlog . predict ( X_test ) df_pred = pd . DataFrame ({ 'y' : Y_test , 'yhat' : predicciones }) df_s1 = summary_metrics ( df_pred ) . assign ( name = 'Todas las variables' ) 82.4 ms \u00b1 6.51 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) %% timeit # Entrenamiento con las variables seleccionadas X = df [ atributos ] Y = df [ 'y' ] # split dataset X_train , X_test , Y_train , Y_test = train_test_split ( X , Y , test_size = 0.2 , random_state = 2 ) # Creando el modelo rlog = LogisticRegression () rlog . fit ( X_train , Y_train ) # ajustando el modelo predicciones = rlog . predict ( X_test ) df_pred = pd . DataFrame ({ 'y' : Y_test , 'yhat' : predicciones }) df_s2 = summary_metrics ( df_pred ) . assign ( name = 'Variables Seleccionadas' ) 60.4 ms \u00b1 8.62 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) Juntando ambos resultados: # juntar resultados en formato dataframe pd . concat ([ df_s1 , df_s2 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } accuracy recall precision fscore name 0 0.8905 0.8906 0.8907 0.8905 Todas las variables 0 0.8985 0.8986 0.8986 0.8985 Variables Seleccionadas Las m\u00e9tricas para ambos casos son parecidas y el tiempo de ejecuci\u00f3n del modelo con menos variable resulta ser menor (lo cual era algo esperable). Lo cual nos muestra que trabajando con menos variables, se puede captar las caracter\u00edsticas m\u00e1s relevante del problema, y en la medida que se trabaje con m\u00e1s datos, las mejoras a nivel de capacidad de c\u00f3mputo tendr\u00e1n un mejor desempe\u00f1o.","title":"Selecci\u00f3n de atributos"},{"location":"lectures/ml/overfitting/04_reducir_overfitting/#referencia","text":"K-Fold Cross Validation Cross Validation and Grid Search for Model Selection in Python Feature selection for supervised models using SelectKBest","title":"Referencia"},{"location":"lectures/ml/time_series/05_series_temporales/","text":"Series Temporales Introducci\u00f3n Una serie temporal o cronol\u00f3gica es una sucesi\u00f3n de datos medidos en determinados momentos y ordenados cronol\u00f3gicamente. Los datos pueden estar espaciados a intervalos iguales (como la temperatura en un observatorio meteorol\u00f3gico en d\u00edas sucesivos al mediod\u00eda) o desiguales (como el peso de una persona en sucesivas mediciones en el consultorio m\u00e9dico, la farmacia, etc.). Uno de los usos m\u00e1s habituales de las series de datos temporales es su an\u00e1lisis para predicci\u00f3n y pron\u00f3stico (as\u00ed se hace por ejemplo con los datos clim\u00e1ticos, las acciones de bolsa, o las series de datos demogr\u00e1ficos). Resulta dif\u00edcil imaginar una rama de las ciencias en la que no aparezcan datos que puedan ser considerados como series temporales. Las series temporales se estudian en estad\u00edstica, procesamiento de se\u00f1ales, econometr\u00eda y muchas otras \u00e1reas. An\u00e1lisis de series temporales Para entender mejor el comportamiento de una serie temporal, nos iremos ayudando con python. Descripci\u00f3n del problema El conjunto de datos se llama AirPassengers.csv , el cual contiene la informaci\u00f3n del total de pasajeros (a nivel de mes) entre los a\u00f1o 1949 y 1960. En terminos estad\u00edsticos, el problema puede ser presentado por la serie temporal \\(\\left \\{ X_t: t \\in T \\right \\}\\) , donde: \\(X_{t}\\) : corresponde al total de pasajeros en el tiempo t \\(t\\) : tiempo medido a nivel de mes. Comparando la serie temporal con un dataframe, este ser\u00eda un dataframe de una sola columna, en cuyo \u00edndice se encuentra las distintas fechas. El objetivo es poder desarrollar un modelo predictivo que me indique el n\u00famero de pasajeros para los pr\u00f3ximos dos a\u00f1os. Antes de ajustar el modelo, se debe entender el comportamiento de la serie de tiempo en estudio y con esta informaci\u00f3n, encontrar el modelo que mejor se puede ajustar (en caso que exista). Como son muchos los conceptos que se presentar\u00e1n, es necesario ir apoyandose con alguna herramienta de programaci\u00f3n, en nuestro caso python . Dentro de python, la librer\u00eda statsmodels es ideal para hacer este tipo de an\u00e1lisis. Lo primero es cargar, transformar y visualizar el conjunto de datos. # librerias import os import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns # graficos incrustados plt . style . use ( 'fivethirtyeight' ) % matplotlib inline # parametros esteticos de seaborn sns . set_palette ( \"deep\" , desat = .6 ) sns . set_context ( rc = { \"figure.figsize\" : ( 12 , 4 )}) # cargar datos df = pd . read_csv ( os . path . join ( \"data\" , \"AirPassengers.csv\" ), sep = \",\" ) df . columns = [ 'date' , 'passengers' ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date passengers 0 1949-01 112 1 1949-02 118 2 1949-03 132 3 1949-04 129 4 1949-05 121 # resumen df . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } passengers count 144.000000 mean 280.298611 std 119.966317 min 104.000000 25% 180.000000 50% 265.500000 75% 360.500000 max 622.000000 # fechas print ( 'Fecha Inicio: {} \\n Fecha Fin: {} ' . format ( df . date . min (), df . date . max ())) Fecha Inicio: 1949-01 Fecha Fin: 1960-12 # formato datetime de las fechas df [ 'date' ] = pd . to_datetime ( df [ 'date' ], format = '%Y-%m' ) # dejar en formato ts y = df . set_index ( 'date' ) . resample ( 'M' ) . mean () y . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } passengers date 1949-01-31 112 1949-02-28 118 1949-03-31 132 1949-04-30 129 1949-05-31 121 # graficar datos y . plot ( figsize = ( 15 , 3 ), color = 'blue' ) plt . show () Basado en el gr\u00e1fico, uno podria decir que tiene un comportamiento estacionario en el tiempo, es decir, que se repita cada cierto instante de tiempo. Adem\u00e1s, la demanda ha tendido a incrementar a\u00f1o a a\u00f1o. Por otro lado, podemos agregar valor a nuestro entendimiento de nuestra serie temporal, realizando un diagrama de box-plot a los distintos a\u00f1os. # diagrama de caja y bigotes fig , ax = plt . subplots ( figsize = ( 15 , 6 )) sns . boxplot ( y . passengers . index . year , y . passengers , ax = ax ) plt . show () Descomposici\u00f3n STL Una serie temporal la podemos descomponer en tres componentes: tendencia ( \\(T\\) ): trayectoria de los datos en el tiempo (direcci\u00f3n positiva o negativa). estacionalidad ( \\(S\\) ): fluctuaciones regulares y predecibles en un periodo determinado (anual, semestral,etc.) ruido ( \\(e\\) ): error intr\u00ednsico al tomar una serie temporal (instrumenos, medici\u00f3n humana, etc.) Cuando un descompone la serie temporla en sus tres componenctes (tendencia, estacionalidad, ruido) se habla de descompocisi\u00f3n STL . En muchas ocasiones no es posible descomponer adecuadamente la serie temporal, puesto que la muestra obtenida no presenta un comportamiento ciclico o repetitivo en el periodo de tiempo analizado. Por otro lado, esta descomposici\u00f3n se puede realizar de dos maneras diferentes: Aditiva : \\( \\(X_{t} = T_{t} + S_{t} + e_{t}\\) \\) Multiplicativa : \\( \\(X_{t} = T_{t} * S_{t} * e_{t}\\) \\) Por suepuesto esta no es la \u00fanica forma de descomponer una serie, pero sirve como punto de partida para comprender nuestra serie en estudio. Realizaremos un descomposici\u00f3n del tipo multiplicativa, ocupando el comando de statsmodels seasonal_decompose . from pylab import rcParams import statsmodels.api as sm import matplotlib.pyplot as plt rcParams [ 'figure.figsize' ] = 18 , 8 decomposition = sm . tsa . seasonal_decompose ( y , model = 'multiplicative' ) fig = decomposition . plot () plt . show () Analicemos cada uno de estos gr\u00e1ficos: gr\u00e1fico 01 (serie original): este gr\u00e1fico simplemente nos muestra la serie original graficada en el tiempo. gr\u00e1fico 02 (tendencia): este gr\u00e1fico nos muestra la tendencia de la serie, para este caso, se tiene una tendencial lineal positiva. gr\u00e1fico 03 (estacionariedad): este gr\u00e1fico nos muestra la estacionariedad de la serie, para este caso, se muestra una estacionariedad a\u00f1o a a\u00f1o, esta estacionariedad se puede ver como una curva invertida (funci\u00f3n cuadr\u00e1tica negativa), en donde a aumenta hasta hasta a mediados de a\u00f1os (o un poco m\u00e1s) y luego esta cae suavemente a final de a\u00f1o. gr\u00e1fico 04 (error): este gr\u00e1fico nos muestra el error de la serie, para este caso, el error oscila entre 0 y 1. En general se busca que el error sea siempre lo m\u00e1s peque\u00f1o posible y que tenga el comportamiento de una distribuci\u00f3n normal. Cuando el error sigue una distribuci\u00f3n normal con media cero y varianza 1, se dice que el error es un ruido blanco . \u00bf c\u00f3mo es un ruido blanco?, mostremos un ruido blanco en python y veamos como este luce: # grafico: lineplot np . random . seed ( 42 ) # fijar semilla mean = 0 std = 1 num_samples = 300 samples = np . random . normal ( mean , std , size = num_samples ) plt . plot ( samples ) plt . title ( \"Ruido blanco: N(0,1)\" ) plt . show () Observemos que el ruido blanco oscila sobre el valor 0 y tiene una varianza constante (igual a 1). Veamos su histograma: # grafico: histograma plt . hist ( samples , bins = 10 ) plt . title ( \"Ruido blanco: N(0,1)\" ) plt . show () EL histograma de una variable normal, se caracteriza por esa forma de campana sim\u00e9trica entorno a un valor, en este caso, entorno al valor 0. Series Estacionarias Un concepto importante para el \u00e1nalisis de series temporales, es el concepto de estacionariedad. Definici\u00f3n Sea \\(\\left \\{ X_t: t \\in T \\right \\}\\) una serie temporal. Se dice que una serie temporal es d\u00e9bilmente estacionaria si: \\(\\mathbb{V}(X_t) < \\infty\\) , para todo \\(t \\in T\\) . \\(\\mu_{X}(t)= \\mu\\) , para todo \\(t \\in T\\) . $\\gamma_{X}(r,s)= \\gamma_{X}(r+h,s+h)=\\gamma_{X}(h) $, para todo \\(r,s,h\\in T\\) . En palabras simple, una serie temporal es d\u00e9bilmente estacionaria si var\u00eda poco respecto a su propia media. Veamos las siguientes im\u00e1genes: La imagen A : Este es un excelente ejemplo de una serie estacionaria, a simple vista se puede ver que los valores se encuentran oscilando en un rango acotado (tendencia constante) y la variabilidad es constante. Las im\u00e1genes B , C y D no son estacionarias porque: imagen B : existe una una tendencia no contante, para este caso lineal (similar al caso que estamos analizando). imagen C : existe varianza no constante, si bien varia dentro de valores acotados, la oscilaciones son err\u00e1ticas. imagen D : existe codependencia entre los distintos instantes de tiempo. \u00bf Por qu\u00e9 es importante este concepto ? Supuesto base de muchos modelos de series temporales (desde el punto de vista estad\u00edstico) No se requiere muchas complicaciones extras para que las predicciones sean \"buenas\". Formas de detectar la estacionariedad La manera m\u00e1s simple es gr\u00e1ficarla e inferir el comportamiento de esta. La ventaja que este m\u00e9todo es r\u00e1pido, sin embargo, se encuentra sesgado por el criterio del ojo humano. Por otro lado existen algunas alternativas que aqu\u00ed presentamos: Autocorrelaci\u00f3n (ACF) y autocorrelaci\u00f3n parcial PACF Definamos a grandes rasgos estos conceptos: Funci\u00f3n de autocorrelaci\u00f3n (ACF). En el retardo \\(k\\) , es la autocorrelaci\u00f3n entre los valores de las series que se encuentran a \\(k\\) intervalos de distancia. Funci\u00f3n de autocorrelaci\u00f3n parcial (PACF). En el retardo \\(k\\) , es la autocorrelaci\u00f3n entre los valores de las series que se encuentran a \\(k\\) intervalos de distancia, teniendo en cuenta los valores de los intervalos intermedios. Si la serie temporal es estacionaria, los gr\u00e1ficos ACF / PACF mostrar\u00e1n una r\u00e1pida disminuci\u00f3n de la correlaci\u00f3n despu\u00e9s de un peque\u00f1o retraso entre los puntos. Gr\u00e1fiquemos la acf y pacf de nuestra serie temporal ocupando los comandos plot_acf y plot_pacf , respectivamente. from statsmodels.graphics.tsaplots import plot_acf from statsmodels.graphics.tsaplots import plot_pacf from matplotlib import pyplot pyplot . figure ( figsize = ( 12 , 9 )) # acf pyplot . subplot ( 211 ) plot_acf ( y . passengers , ax = pyplot . gca (), lags = 30 ) #pacf pyplot . subplot ( 212 ) plot_pacf ( y . passengers , ax = pyplot . gca (), lags = 30 ) pyplot . show () Se observa de ambas imagenes, que estas no decaen r\u00e1pidamente a cero, por lo cual se puede decir que la serie en estudio no es estacionaria. Prueba Dickey-Fuller En estad\u00edstica, la prueba Dickey-Fuller prueba la hip\u00f3tesis nula de que una ra\u00edz unitaria est\u00e1 presente en un modelo autorregresivo. La hip\u00f3tesis alternativa es diferente seg\u00fan la versi\u00f3n de la prueba utilizada, pero generalmente es estacionariedad o tendencia-estacionaria. Lleva el nombre de los estad\u00edsticos David Dickey y Wayne Fuller, quienes desarrollaron la prueba en 1979. Para efectos pr\u00e1ticos, para el caso de estacionariedad se puede definir el test como: Hip\u00f3tesis nula : la serie temporal no es estacionaria. Hip\u00f3tesis alternativa : la serie temporal es alternativa. Rechazar la hip\u00f3tesis nula (es decir, un valor p muy bajo) indicar\u00e1 estacionariedad from statsmodels.tsa.stattools import adfuller #test Dickey-Fulle: print ( 'Resultados del test de Dickey-Fuller:' ) dftest = adfuller ( y . passengers , autolag = 'AIC' ) dfoutput = pd . Series ( dftest [ 0 : 4 ], index = [ 'Test Statistic' , 'p-value' , '#Lags Used' , 'Number of Observations Used' ]) print ( dfoutput ) Resultados del test de Dickey-Fuller: Test Statistic 0.815369 p-value 0.991880 #Lags Used 13.000000 Number of Observations Used 130.000000 dtype: float64 Dado que el p-value es 0.991880, se concluye que la serie temporal no es estacionaria. \u00bf Qu\u00e9 se puede hacer si la serie no es etacionaria ? Nuestro caso en estudio resulto ser una serie no estacionaria, no obstante, se pueden realizar tranformaciones para solucionar este problema. Como es de esperar, estas tranformaciones deben cumplir ciertas porpiedades (inyectividad, diferenciables, etc.). A continuaci\u00f3n, se presentan algunas de las tranformaciones m\u00e1s ocupadas en el \u00e1mbito de series temporales: Sea \\(X_{t}\\) una serie temporal, entonces uno puede definir una nueva serie temporal \\(Y_{t}\\) de la siguiente manera: diferenciaci\u00f3n : \\(Y_{t} =\\Delta X_{t} =X_{t}-X_{t-1}\\) . transformaciones de Box-Cox : \\( \\(Y_{t} = \\left\\{\\begin{matrix} \\dfrac{X_{t}^{\\lambda}-1}{\\lambda}, \\ \\ \\textrm{si } \\lambda > 0\\\\ \\textrm{log}(X_{t}), \\ \\ \\textrm{si } \\lambda = 0 \\end{matrix}\\right.\\) \\) Ayudemonos con python para ver el resultado de las distintas transformaciones. # diferenciacion Y_diff = y . diff () rcParams [ 'figure.figsize' ] = 12 , 4 plt . plot ( Y_diff ) plt . title ( \"Serie diferenciada\" ) plt . show () def box_transformations ( y , param ): if param > 0 : return y . apply ( lambda x : ( x ** ( param ) - 1 ) / param ) elif param == 0 : return np . log ( y ) else : print ( \"lambda es negativo, se devulve la serie original\" ) return y # logaritmo Y_log = box_transformations ( y , 0 ) rcParams [ 'figure.figsize' ] = 12 , 4 plt . plot ( Y_log ) plt . title ( \"Serie logaritmica\" ) plt . show () # cuadratica Y_quad = box_transformations ( y , 2 ) rcParams [ 'figure.figsize' ] = 12 , 4 plt . plot ( Y_log ) plt . title ( \"Serie cuadratica\" ) plt . show () Modelos de forecast (Pron\u00f3stico) Para realizar el pron\u00f3stico de series, existen varios modelos cl\u00e1sicos para analizar series temporales: Modelos con variabilidad (varianza) constante Modelos de media m\u00f3vil (MA(d)): el modelo queda en funci\u00f3n de los ruidos \\(e_{1},e_{2},...,e_{d}\\) Modelos autorregresivos (AR(q)): el modelo queda en funci\u00f3n de los ruidos \\(X_{1},X_{2},...,X_{q}\\) Modelos ARMA (ARMA(p,q)): Mezcla de los modelos \\(MA(d)\\) y \\(AR(q)\\) Modelos ARIMA (ARIMA(p,d,q)):: Mezcla de los modelos \\(MA(d)\\) y \\(AR(q)\\) sobre la serie diferenciada \\(d\\) veces. Modelos SARIMA (SARIMA(p,d,q)x(P,D,Q,S)): Mezcla de los modelos ARIMA(p,d,q) agregando componentes de estacionariedad ( \\(S\\) ). Dentro de estos modelos, se tiene que uno son un caso particular de otro m\u00e1s general: \\[MA(d),AR(q) \\subset ARMA(p,q) \\subset ARIMA(p,d,q) \\subset SARIMA(p,d,q)x(P,D,Q,S) \\] Modelos de volatibilidad Arch Garch Modelos de espacio estado Se deja como tarea al lector profundizar m\u00e1s en estos conceptos. Realizar pron\u00f3stico con statsmodels El pron\u00f3stico lo realizaremos ocupando los modelos disponible en statsmodels, particularmen los modelos \\(SARIMA(p,d,q)x(P,D,Q,S)\\) . Como todo buen proceso de machine learning es necesario separar nuestro conjunto de datos en dos (entrenamiento y testeo). \u00bf C\u00f3mo se realiza esto con series temporales ?. El camino correcto para considerar una fecha objetivo ( target_date ), el cual separ\u00e9 en dos conjuntos, de la siguiente manera: y_train : todos los datos hasta la fecha target_date y_test : todos los datos despu\u00e9s la fecha target_date target_date = '1958-01-01' # crear conjunto de entrenamiento y de testeo mask_ds = y . index < target_date y_train = y [ mask_ds ] y_test = y [ ~ mask_ds ] #plotting the data y_train [ 'passengers' ] . plot () y_test [ 'passengers' ] . plot () plt . show () Una pregunta natural que surgue es: \u00bf por qu\u00e9 no se toman datos de manera aleatoria?. La respuesta es que como se trabaja el la variable tiempo , por lo tanto los datos siguen un orden natural de los sucesos, en cambio, en los problemas de regresi\u00f3n no existe orden en los sucesos, por cada par de punto es de cierta forma independiente uno con otros. Adem\u00e1s, si se sacar\u00e1n puntos de testeo de manera aleatoria, podr\u00eda romper con la tendencia y estacionariedad original de la serie. Veamos un ejemplo sensillo de este caso en python: from statsmodels.tsa.statespace.sarimax import SARIMAX from metrics_regression import * # parametros param = [( 1 , 0 , 0 ),( 0 , 0 , 0 , 12 )] # modelo model = SARIMAX ( y_train , order = param [ 0 ], seasonal_order = param [ 1 ], enforce_stationarity = False , enforce_invertibility = False ) # ajustar modelo model_fit = model . fit ( disp = 0 ) # fecha de las predicciones start_index = y_test . index . min () end_index = y_test . index . max () preds = model_fit . get_prediction ( start = start_index , end = end_index , dynamic = False ) df_temp = pd . DataFrame ( { 'y' : y_test [ 'passengers' ], 'yhat' : preds . predicted_mean } ) # resultados del ajuste df_temp . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } y yhat date 1958-01-31 340 336.756041 1958-02-28 318 337.513783 1958-03-31 362 338.273230 1958-04-30 348 339.034386 1958-05-31 363 339.797254 # resultados de las m\u00e9tricas df_metrics = summary_metrics ( df_temp ) df_metrics [ 'model' ] = f \"SARIMA_ { param [ 0 ] } X { param [ 1 ] } \" . replace ( ' ' , '' ) df_metrics .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse mape maape wmape mmape smape model 0 81.8529 11619.4305 107.7935 0.1701 0.166 0.191 0.1697 0.195 SARIMA_(1,0,0)X(0,0,0,12) # graficamos resultados preds = df_temp [ 'yhat' ] ax = y [ '1949' :] . plot ( label = 'observed' ) preds . plot ( ax = ax , label = 'Forecast' , alpha = .7 , figsize = ( 14 , 7 )) ax . set_xlabel ( 'Date' ) ax . set_ylabel ( 'Passengers' ) plt . legend () plt . show () Observamos a simple vista que el ajuste no es tan bueno que digamos (analizando las m\u00e9tricas y el gr\u00e1fico). Entonces, \u00bf qu\u00e9 se puede hacer?. La respuesta es simple \u00a1 probar varios modelos sarima !. Ahora, \u00bf c\u00f3mo lo hacemos?. Lo primero es definir una clase llamada SarimaModels que automatice el proceso anterior, y nos quedamos con aquel modelo que minimice alguna de las m\u00e9tricas propuestas, por ejemplo, minimizar las m\u00e9tricas de mae y mape # creando clase SarimaModels class SarimaModels : def __init__ ( self , params ): self . params = params @property def name_model ( self ): return f \"SARIMA_ { self . params [ 0 ] } X { self . params [ 1 ] } \" . replace ( ' ' , '' ) @staticmethod def test_train_model ( y , date ): mask_ds = y . index < date y_train = y [ mask_ds ] y_test = y [ ~ mask_ds ] return y_train , y_test def fit_model ( self , y , date ): y_train , y_test = self . test_train_model ( y , date ) model = SARIMAX ( y_train , order = self . params [ 0 ], seasonal_order = self . params [ 1 ], enforce_stationarity = False , enforce_invertibility = False ) model_fit = model . fit ( disp = 0 ) return model_fit def df_testig ( self , y , date ): y_train , y_test = self . test_train_model ( y , date ) model = SARIMAX ( y_train , order = self . params [ 0 ], seasonal_order = self . params [ 1 ], enforce_stationarity = False , enforce_invertibility = False ) model_fit = model . fit ( disp = 0 ) start_index = y_test . index . min () end_index = y_test . index . max () preds = model_fit . get_prediction ( start = start_index , end = end_index , dynamic = False ) df_temp = pd . DataFrame ( { 'y' : y_test [ 'passengers' ], 'yhat' : preds . predicted_mean } ) return df_temp def metrics ( self , y , date ): df_temp = self . df_testig ( y , date ) df_metrics = summary_metrics ( df_temp ) df_metrics [ 'model' ] = self . name_model return df_metrics # definir parametros import itertools p = d = q = range ( 0 , 2 ) pdq = list ( itertools . product ( p , d , q )) seasonal_pdq = [( x [ 0 ], x [ 1 ], x [ 2 ], 12 ) for x in list ( itertools . product ( p , d , q ))] params = list ( itertools . product ( pdq , seasonal_pdq )) target_date = '1958-01-01' # iterar para los distintos escenarios frames = [] for param in params : try : sarima_model = SarimaModels ( param ) df_metrics = sarima_model . metrics ( y , target_date ) frames . append ( df_metrics ) except : pass # juntar resultados de las m\u00e9tricas y comparar df_metrics_result = pd . concat ( frames ) df_metrics_result . sort_values ([ 'mae' , 'mape' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse mape maape wmape mmape smape model 0 16.4272 406.5114 20.1621 0.0410 0.0409 0.0383 0.0409 0.0398 SARIMA_(0,1,0)X(1,0,0,12) 0 17.7173 469.8340 21.6757 0.0420 0.0419 0.0413 0.0419 0.0414 SARIMA_(0,1,1)X(1,1,1,12) 0 17.7204 480.9764 21.9312 0.0414 0.0414 0.0414 0.0413 0.0411 SARIMA_(1,1,0)X(1,1,1,12) 0 17.8053 501.0603 22.3844 0.0412 0.0411 0.0416 0.0411 0.0410 SARIMA_(1,1,1)X(0,1,0,12) 0 17.8056 505.4167 22.4815 0.0411 0.0410 0.0416 0.0410 0.0409 SARIMA_(0,1,0)X(0,1,0,12) ... ... ... ... ... ... ... ... ... ... 0 94.9444 14674.5556 121.1386 0.1989 0.1933 0.2216 0.1984 0.2320 SARIMA_(0,1,0)X(0,0,0,12) 0 360.7115 150709.0664 388.2127 0.8203 0.6687 0.8418 0.8184 1.5247 SARIMA_(0,0,1)X(0,0,1,12) 0 366.5303 153175.7993 391.3768 0.8370 0.6811 0.8554 0.8351 1.5623 SARIMA_(0,0,0)X(0,0,1,12) 0 422.9626 187068.9748 432.5147 0.9837 0.7745 0.9871 0.9814 1.9589 SARIMA_(0,0,1)X(0,0,0,12) 0 428.5000 189730.5556 435.5807 1.0000 0.7854 1.0000 0.9976 2.0000 SARIMA_(0,0,0)X(0,0,0,12) 64 rows \u00d7 9 columns En este caso el mejor modelo resulta ser el modelo \\(SARIMA(0,1,0)X(1,0,0,12)\\) . Veamos gr\u00e1ficamente que tal el ajuste de este modelo. # ajustar mejor modelo param = [( 0 , 1 , 0 ),( 1 , 0 , 0 , 12 )] sarima_model = SarimaModels ( param ) model_fit = sarima_model . fit_model ( y , target_date ) best_model = sarima_model . df_testig ( y , target_date ) best_model . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } y yhat date 1958-01-31 340 345.765806 1958-02-28 318 330.574552 1958-03-31 362 390.254476 1958-04-30 348 381.573760 1958-05-31 363 389.169386 # graficar mejor modelo preds = best_model [ 'yhat' ] ax = y [ '1949' :] . plot ( label = 'observed' ) preds . plot ( ax = ax , label = 'Forecast' , alpha = .7 , figsize = ( 14 , 7 )) ax . set_xlabel ( 'Date' ) ax . set_ylabel ( 'Passengers' ) plt . legend () plt . show () Para este caso, el mejor modelo encontrado se ajusta bastante bien a los datos. Finalmente, vemos algunos resultados del error asosciado al modelo. Para esto ocupamos la herramienta plot_diagnostics , el cual nos arroja cuatro gr\u00e1ficos analizando el error de diferentes manera. # resultados del error model_fit . plot_diagnostics ( figsize = ( 16 , 8 )) plt . show () gr\u00e1fico 01 (standarized residual): Este gr\u00e1fico nos muestra el error estandarizado en el tiempo. En este caso se observa que esta nueva serie de tiempo corresponde a una serie estacionaria que oscila entorno al cero, es decir, un ruido blanco. gr\u00e1fico 02 (histogram plus estimated density): Este gr\u00e1fico nos muestra el histograma del error. En este caso, el histograma es muy similar al histograma de una variable \\(\\mathcal{N}(0,1)\\) (ruido blanco). gr\u00e1fico 03 (normal QQ): el gr\u00e1fico Q-Q (\"Q\" viene de cuantil) es un m\u00e9todo gr\u00e1fico para el diagn\u00f3stico de diferencias entre la distribuci\u00f3n de probabilidad de una poblaci\u00f3n de la que se ha extra\u00eddo una muestra aleatoria y una distribuci\u00f3n usada para la comparaci\u00f3n. En este caso se comparar la distribuci\u00f3n del error versus una distribuci\u00f3n normal. Cuando mejor es el ajuste lineal sobre los puntos, m\u00e1s parecida es la distribuci\u00f3n entre la muestra obtenida y la distribuci\u00f3n de prueba (distribuci\u00f3n normal). gr\u00e1fico 04 (correlogram): Este gr\u00e1fico nos muestra el gr\u00e1fico de autocorrelaci\u00f3n entre las variables del error, se observa que no hay correlaci\u00f3n entre ninguna de las variables, por lo que se puedan dar indicios de independencia entre las variables. En conclusi\u00f3n, el error asociado al modelo en estudio corresponde a un ruido blanco. Conclusi\u00f3n Este fue una introducci\u00f3n amigable a los conceptos claves de series temporales, a medida que m\u00e1s se profundice en la teor\u00eda, mejor ser\u00e1n las t\u00e9cnicas empleadas sobre la serie temporal en fin de obtener el mejor pron\u00f3stico posible. En esta secci\u00f3n nos limitamos a algunos modelos y algunos criterios de verificaci\u00f3n de estacionariedad. En la literatura existen muchas m\u00e1s, pero con los mostrados de momento y un poco de expertice en el tema, se pueden abordar casi todos los problemas de series temporales. Referencia A comprehensive beginner\u2019s guide to create a Time Series Forecast (with Codes in Python and R) A Gentle Introduction to SARIMA for Time Series Forecasting in Python An Introductory Study on Time Series Modeling and Forecasting","title":"Series Temporales"},{"location":"lectures/ml/time_series/05_series_temporales/#series-temporales","text":"","title":"Series Temporales"},{"location":"lectures/ml/time_series/05_series_temporales/#introduccion","text":"Una serie temporal o cronol\u00f3gica es una sucesi\u00f3n de datos medidos en determinados momentos y ordenados cronol\u00f3gicamente. Los datos pueden estar espaciados a intervalos iguales (como la temperatura en un observatorio meteorol\u00f3gico en d\u00edas sucesivos al mediod\u00eda) o desiguales (como el peso de una persona en sucesivas mediciones en el consultorio m\u00e9dico, la farmacia, etc.). Uno de los usos m\u00e1s habituales de las series de datos temporales es su an\u00e1lisis para predicci\u00f3n y pron\u00f3stico (as\u00ed se hace por ejemplo con los datos clim\u00e1ticos, las acciones de bolsa, o las series de datos demogr\u00e1ficos). Resulta dif\u00edcil imaginar una rama de las ciencias en la que no aparezcan datos que puedan ser considerados como series temporales. Las series temporales se estudian en estad\u00edstica, procesamiento de se\u00f1ales, econometr\u00eda y muchas otras \u00e1reas.","title":"Introducci\u00f3n"},{"location":"lectures/ml/time_series/05_series_temporales/#analisis-de-series-temporales","text":"Para entender mejor el comportamiento de una serie temporal, nos iremos ayudando con python.","title":"An\u00e1lisis de series temporales"},{"location":"lectures/ml/time_series/05_series_temporales/#descripcion-del-problema","text":"El conjunto de datos se llama AirPassengers.csv , el cual contiene la informaci\u00f3n del total de pasajeros (a nivel de mes) entre los a\u00f1o 1949 y 1960. En terminos estad\u00edsticos, el problema puede ser presentado por la serie temporal \\(\\left \\{ X_t: t \\in T \\right \\}\\) , donde: \\(X_{t}\\) : corresponde al total de pasajeros en el tiempo t \\(t\\) : tiempo medido a nivel de mes. Comparando la serie temporal con un dataframe, este ser\u00eda un dataframe de una sola columna, en cuyo \u00edndice se encuentra las distintas fechas. El objetivo es poder desarrollar un modelo predictivo que me indique el n\u00famero de pasajeros para los pr\u00f3ximos dos a\u00f1os. Antes de ajustar el modelo, se debe entender el comportamiento de la serie de tiempo en estudio y con esta informaci\u00f3n, encontrar el modelo que mejor se puede ajustar (en caso que exista). Como son muchos los conceptos que se presentar\u00e1n, es necesario ir apoyandose con alguna herramienta de programaci\u00f3n, en nuestro caso python . Dentro de python, la librer\u00eda statsmodels es ideal para hacer este tipo de an\u00e1lisis. Lo primero es cargar, transformar y visualizar el conjunto de datos. # librerias import os import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns # graficos incrustados plt . style . use ( 'fivethirtyeight' ) % matplotlib inline # parametros esteticos de seaborn sns . set_palette ( \"deep\" , desat = .6 ) sns . set_context ( rc = { \"figure.figsize\" : ( 12 , 4 )}) # cargar datos df = pd . read_csv ( os . path . join ( \"data\" , \"AirPassengers.csv\" ), sep = \",\" ) df . columns = [ 'date' , 'passengers' ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date passengers 0 1949-01 112 1 1949-02 118 2 1949-03 132 3 1949-04 129 4 1949-05 121 # resumen df . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } passengers count 144.000000 mean 280.298611 std 119.966317 min 104.000000 25% 180.000000 50% 265.500000 75% 360.500000 max 622.000000 # fechas print ( 'Fecha Inicio: {} \\n Fecha Fin: {} ' . format ( df . date . min (), df . date . max ())) Fecha Inicio: 1949-01 Fecha Fin: 1960-12 # formato datetime de las fechas df [ 'date' ] = pd . to_datetime ( df [ 'date' ], format = '%Y-%m' ) # dejar en formato ts y = df . set_index ( 'date' ) . resample ( 'M' ) . mean () y . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } passengers date 1949-01-31 112 1949-02-28 118 1949-03-31 132 1949-04-30 129 1949-05-31 121 # graficar datos y . plot ( figsize = ( 15 , 3 ), color = 'blue' ) plt . show () Basado en el gr\u00e1fico, uno podria decir que tiene un comportamiento estacionario en el tiempo, es decir, que se repita cada cierto instante de tiempo. Adem\u00e1s, la demanda ha tendido a incrementar a\u00f1o a a\u00f1o. Por otro lado, podemos agregar valor a nuestro entendimiento de nuestra serie temporal, realizando un diagrama de box-plot a los distintos a\u00f1os. # diagrama de caja y bigotes fig , ax = plt . subplots ( figsize = ( 15 , 6 )) sns . boxplot ( y . passengers . index . year , y . passengers , ax = ax ) plt . show ()","title":"Descripci\u00f3n del problema"},{"location":"lectures/ml/time_series/05_series_temporales/#descomposicion-stl","text":"Una serie temporal la podemos descomponer en tres componentes: tendencia ( \\(T\\) ): trayectoria de los datos en el tiempo (direcci\u00f3n positiva o negativa). estacionalidad ( \\(S\\) ): fluctuaciones regulares y predecibles en un periodo determinado (anual, semestral,etc.) ruido ( \\(e\\) ): error intr\u00ednsico al tomar una serie temporal (instrumenos, medici\u00f3n humana, etc.) Cuando un descompone la serie temporla en sus tres componenctes (tendencia, estacionalidad, ruido) se habla de descompocisi\u00f3n STL . En muchas ocasiones no es posible descomponer adecuadamente la serie temporal, puesto que la muestra obtenida no presenta un comportamiento ciclico o repetitivo en el periodo de tiempo analizado. Por otro lado, esta descomposici\u00f3n se puede realizar de dos maneras diferentes: Aditiva : \\( \\(X_{t} = T_{t} + S_{t} + e_{t}\\) \\) Multiplicativa : \\( \\(X_{t} = T_{t} * S_{t} * e_{t}\\) \\) Por suepuesto esta no es la \u00fanica forma de descomponer una serie, pero sirve como punto de partida para comprender nuestra serie en estudio. Realizaremos un descomposici\u00f3n del tipo multiplicativa, ocupando el comando de statsmodels seasonal_decompose . from pylab import rcParams import statsmodels.api as sm import matplotlib.pyplot as plt rcParams [ 'figure.figsize' ] = 18 , 8 decomposition = sm . tsa . seasonal_decompose ( y , model = 'multiplicative' ) fig = decomposition . plot () plt . show () Analicemos cada uno de estos gr\u00e1ficos: gr\u00e1fico 01 (serie original): este gr\u00e1fico simplemente nos muestra la serie original graficada en el tiempo. gr\u00e1fico 02 (tendencia): este gr\u00e1fico nos muestra la tendencia de la serie, para este caso, se tiene una tendencial lineal positiva. gr\u00e1fico 03 (estacionariedad): este gr\u00e1fico nos muestra la estacionariedad de la serie, para este caso, se muestra una estacionariedad a\u00f1o a a\u00f1o, esta estacionariedad se puede ver como una curva invertida (funci\u00f3n cuadr\u00e1tica negativa), en donde a aumenta hasta hasta a mediados de a\u00f1os (o un poco m\u00e1s) y luego esta cae suavemente a final de a\u00f1o. gr\u00e1fico 04 (error): este gr\u00e1fico nos muestra el error de la serie, para este caso, el error oscila entre 0 y 1. En general se busca que el error sea siempre lo m\u00e1s peque\u00f1o posible y que tenga el comportamiento de una distribuci\u00f3n normal. Cuando el error sigue una distribuci\u00f3n normal con media cero y varianza 1, se dice que el error es un ruido blanco . \u00bf c\u00f3mo es un ruido blanco?, mostremos un ruido blanco en python y veamos como este luce: # grafico: lineplot np . random . seed ( 42 ) # fijar semilla mean = 0 std = 1 num_samples = 300 samples = np . random . normal ( mean , std , size = num_samples ) plt . plot ( samples ) plt . title ( \"Ruido blanco: N(0,1)\" ) plt . show () Observemos que el ruido blanco oscila sobre el valor 0 y tiene una varianza constante (igual a 1). Veamos su histograma: # grafico: histograma plt . hist ( samples , bins = 10 ) plt . title ( \"Ruido blanco: N(0,1)\" ) plt . show () EL histograma de una variable normal, se caracteriza por esa forma de campana sim\u00e9trica entorno a un valor, en este caso, entorno al valor 0.","title":"Descomposici\u00f3n STL"},{"location":"lectures/ml/time_series/05_series_temporales/#series-estacionarias","text":"Un concepto importante para el \u00e1nalisis de series temporales, es el concepto de estacionariedad.","title":"Series Estacionarias"},{"location":"lectures/ml/time_series/05_series_temporales/#definicion","text":"Sea \\(\\left \\{ X_t: t \\in T \\right \\}\\) una serie temporal. Se dice que una serie temporal es d\u00e9bilmente estacionaria si: \\(\\mathbb{V}(X_t) < \\infty\\) , para todo \\(t \\in T\\) . \\(\\mu_{X}(t)= \\mu\\) , para todo \\(t \\in T\\) . $\\gamma_{X}(r,s)= \\gamma_{X}(r+h,s+h)=\\gamma_{X}(h) $, para todo \\(r,s,h\\in T\\) . En palabras simple, una serie temporal es d\u00e9bilmente estacionaria si var\u00eda poco respecto a su propia media. Veamos las siguientes im\u00e1genes: La imagen A : Este es un excelente ejemplo de una serie estacionaria, a simple vista se puede ver que los valores se encuentran oscilando en un rango acotado (tendencia constante) y la variabilidad es constante. Las im\u00e1genes B , C y D no son estacionarias porque: imagen B : existe una una tendencia no contante, para este caso lineal (similar al caso que estamos analizando). imagen C : existe varianza no constante, si bien varia dentro de valores acotados, la oscilaciones son err\u00e1ticas. imagen D : existe codependencia entre los distintos instantes de tiempo. \u00bf Por qu\u00e9 es importante este concepto ? Supuesto base de muchos modelos de series temporales (desde el punto de vista estad\u00edstico) No se requiere muchas complicaciones extras para que las predicciones sean \"buenas\".","title":"Definici\u00f3n"},{"location":"lectures/ml/time_series/05_series_temporales/#formas-de-detectar-la-estacionariedad","text":"La manera m\u00e1s simple es gr\u00e1ficarla e inferir el comportamiento de esta. La ventaja que este m\u00e9todo es r\u00e1pido, sin embargo, se encuentra sesgado por el criterio del ojo humano. Por otro lado existen algunas alternativas que aqu\u00ed presentamos: Autocorrelaci\u00f3n (ACF) y autocorrelaci\u00f3n parcial PACF Definamos a grandes rasgos estos conceptos: Funci\u00f3n de autocorrelaci\u00f3n (ACF). En el retardo \\(k\\) , es la autocorrelaci\u00f3n entre los valores de las series que se encuentran a \\(k\\) intervalos de distancia. Funci\u00f3n de autocorrelaci\u00f3n parcial (PACF). En el retardo \\(k\\) , es la autocorrelaci\u00f3n entre los valores de las series que se encuentran a \\(k\\) intervalos de distancia, teniendo en cuenta los valores de los intervalos intermedios. Si la serie temporal es estacionaria, los gr\u00e1ficos ACF / PACF mostrar\u00e1n una r\u00e1pida disminuci\u00f3n de la correlaci\u00f3n despu\u00e9s de un peque\u00f1o retraso entre los puntos. Gr\u00e1fiquemos la acf y pacf de nuestra serie temporal ocupando los comandos plot_acf y plot_pacf , respectivamente. from statsmodels.graphics.tsaplots import plot_acf from statsmodels.graphics.tsaplots import plot_pacf from matplotlib import pyplot pyplot . figure ( figsize = ( 12 , 9 )) # acf pyplot . subplot ( 211 ) plot_acf ( y . passengers , ax = pyplot . gca (), lags = 30 ) #pacf pyplot . subplot ( 212 ) plot_pacf ( y . passengers , ax = pyplot . gca (), lags = 30 ) pyplot . show () Se observa de ambas imagenes, que estas no decaen r\u00e1pidamente a cero, por lo cual se puede decir que la serie en estudio no es estacionaria. Prueba Dickey-Fuller En estad\u00edstica, la prueba Dickey-Fuller prueba la hip\u00f3tesis nula de que una ra\u00edz unitaria est\u00e1 presente en un modelo autorregresivo. La hip\u00f3tesis alternativa es diferente seg\u00fan la versi\u00f3n de la prueba utilizada, pero generalmente es estacionariedad o tendencia-estacionaria. Lleva el nombre de los estad\u00edsticos David Dickey y Wayne Fuller, quienes desarrollaron la prueba en 1979. Para efectos pr\u00e1ticos, para el caso de estacionariedad se puede definir el test como: Hip\u00f3tesis nula : la serie temporal no es estacionaria. Hip\u00f3tesis alternativa : la serie temporal es alternativa. Rechazar la hip\u00f3tesis nula (es decir, un valor p muy bajo) indicar\u00e1 estacionariedad from statsmodels.tsa.stattools import adfuller #test Dickey-Fulle: print ( 'Resultados del test de Dickey-Fuller:' ) dftest = adfuller ( y . passengers , autolag = 'AIC' ) dfoutput = pd . Series ( dftest [ 0 : 4 ], index = [ 'Test Statistic' , 'p-value' , '#Lags Used' , 'Number of Observations Used' ]) print ( dfoutput ) Resultados del test de Dickey-Fuller: Test Statistic 0.815369 p-value 0.991880 #Lags Used 13.000000 Number of Observations Used 130.000000 dtype: float64 Dado que el p-value es 0.991880, se concluye que la serie temporal no es estacionaria. \u00bf Qu\u00e9 se puede hacer si la serie no es etacionaria ? Nuestro caso en estudio resulto ser una serie no estacionaria, no obstante, se pueden realizar tranformaciones para solucionar este problema. Como es de esperar, estas tranformaciones deben cumplir ciertas porpiedades (inyectividad, diferenciables, etc.). A continuaci\u00f3n, se presentan algunas de las tranformaciones m\u00e1s ocupadas en el \u00e1mbito de series temporales: Sea \\(X_{t}\\) una serie temporal, entonces uno puede definir una nueva serie temporal \\(Y_{t}\\) de la siguiente manera: diferenciaci\u00f3n : \\(Y_{t} =\\Delta X_{t} =X_{t}-X_{t-1}\\) . transformaciones de Box-Cox : \\( \\(Y_{t} = \\left\\{\\begin{matrix} \\dfrac{X_{t}^{\\lambda}-1}{\\lambda}, \\ \\ \\textrm{si } \\lambda > 0\\\\ \\textrm{log}(X_{t}), \\ \\ \\textrm{si } \\lambda = 0 \\end{matrix}\\right.\\) \\) Ayudemonos con python para ver el resultado de las distintas transformaciones. # diferenciacion Y_diff = y . diff () rcParams [ 'figure.figsize' ] = 12 , 4 plt . plot ( Y_diff ) plt . title ( \"Serie diferenciada\" ) plt . show () def box_transformations ( y , param ): if param > 0 : return y . apply ( lambda x : ( x ** ( param ) - 1 ) / param ) elif param == 0 : return np . log ( y ) else : print ( \"lambda es negativo, se devulve la serie original\" ) return y # logaritmo Y_log = box_transformations ( y , 0 ) rcParams [ 'figure.figsize' ] = 12 , 4 plt . plot ( Y_log ) plt . title ( \"Serie logaritmica\" ) plt . show () # cuadratica Y_quad = box_transformations ( y , 2 ) rcParams [ 'figure.figsize' ] = 12 , 4 plt . plot ( Y_log ) plt . title ( \"Serie cuadratica\" ) plt . show ()","title":"Formas de detectar la estacionariedad"},{"location":"lectures/ml/time_series/05_series_temporales/#modelos-de-forecast-pronostico","text":"Para realizar el pron\u00f3stico de series, existen varios modelos cl\u00e1sicos para analizar series temporales: Modelos con variabilidad (varianza) constante Modelos de media m\u00f3vil (MA(d)): el modelo queda en funci\u00f3n de los ruidos \\(e_{1},e_{2},...,e_{d}\\) Modelos autorregresivos (AR(q)): el modelo queda en funci\u00f3n de los ruidos \\(X_{1},X_{2},...,X_{q}\\) Modelos ARMA (ARMA(p,q)): Mezcla de los modelos \\(MA(d)\\) y \\(AR(q)\\) Modelos ARIMA (ARIMA(p,d,q)):: Mezcla de los modelos \\(MA(d)\\) y \\(AR(q)\\) sobre la serie diferenciada \\(d\\) veces. Modelos SARIMA (SARIMA(p,d,q)x(P,D,Q,S)): Mezcla de los modelos ARIMA(p,d,q) agregando componentes de estacionariedad ( \\(S\\) ). Dentro de estos modelos, se tiene que uno son un caso particular de otro m\u00e1s general: \\[MA(d),AR(q) \\subset ARMA(p,q) \\subset ARIMA(p,d,q) \\subset SARIMA(p,d,q)x(P,D,Q,S) \\] Modelos de volatibilidad Arch Garch Modelos de espacio estado Se deja como tarea al lector profundizar m\u00e1s en estos conceptos.","title":"Modelos de forecast  (Pron\u00f3stico)"},{"location":"lectures/ml/time_series/05_series_temporales/#realizar-pronostico-con-statsmodels","text":"El pron\u00f3stico lo realizaremos ocupando los modelos disponible en statsmodels, particularmen los modelos \\(SARIMA(p,d,q)x(P,D,Q,S)\\) . Como todo buen proceso de machine learning es necesario separar nuestro conjunto de datos en dos (entrenamiento y testeo). \u00bf C\u00f3mo se realiza esto con series temporales ?. El camino correcto para considerar una fecha objetivo ( target_date ), el cual separ\u00e9 en dos conjuntos, de la siguiente manera: y_train : todos los datos hasta la fecha target_date y_test : todos los datos despu\u00e9s la fecha target_date target_date = '1958-01-01' # crear conjunto de entrenamiento y de testeo mask_ds = y . index < target_date y_train = y [ mask_ds ] y_test = y [ ~ mask_ds ] #plotting the data y_train [ 'passengers' ] . plot () y_test [ 'passengers' ] . plot () plt . show () Una pregunta natural que surgue es: \u00bf por qu\u00e9 no se toman datos de manera aleatoria?. La respuesta es que como se trabaja el la variable tiempo , por lo tanto los datos siguen un orden natural de los sucesos, en cambio, en los problemas de regresi\u00f3n no existe orden en los sucesos, por cada par de punto es de cierta forma independiente uno con otros. Adem\u00e1s, si se sacar\u00e1n puntos de testeo de manera aleatoria, podr\u00eda romper con la tendencia y estacionariedad original de la serie. Veamos un ejemplo sensillo de este caso en python: from statsmodels.tsa.statespace.sarimax import SARIMAX from metrics_regression import * # parametros param = [( 1 , 0 , 0 ),( 0 , 0 , 0 , 12 )] # modelo model = SARIMAX ( y_train , order = param [ 0 ], seasonal_order = param [ 1 ], enforce_stationarity = False , enforce_invertibility = False ) # ajustar modelo model_fit = model . fit ( disp = 0 ) # fecha de las predicciones start_index = y_test . index . min () end_index = y_test . index . max () preds = model_fit . get_prediction ( start = start_index , end = end_index , dynamic = False ) df_temp = pd . DataFrame ( { 'y' : y_test [ 'passengers' ], 'yhat' : preds . predicted_mean } ) # resultados del ajuste df_temp . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } y yhat date 1958-01-31 340 336.756041 1958-02-28 318 337.513783 1958-03-31 362 338.273230 1958-04-30 348 339.034386 1958-05-31 363 339.797254 # resultados de las m\u00e9tricas df_metrics = summary_metrics ( df_temp ) df_metrics [ 'model' ] = f \"SARIMA_ { param [ 0 ] } X { param [ 1 ] } \" . replace ( ' ' , '' ) df_metrics .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse mape maape wmape mmape smape model 0 81.8529 11619.4305 107.7935 0.1701 0.166 0.191 0.1697 0.195 SARIMA_(1,0,0)X(0,0,0,12) # graficamos resultados preds = df_temp [ 'yhat' ] ax = y [ '1949' :] . plot ( label = 'observed' ) preds . plot ( ax = ax , label = 'Forecast' , alpha = .7 , figsize = ( 14 , 7 )) ax . set_xlabel ( 'Date' ) ax . set_ylabel ( 'Passengers' ) plt . legend () plt . show () Observamos a simple vista que el ajuste no es tan bueno que digamos (analizando las m\u00e9tricas y el gr\u00e1fico). Entonces, \u00bf qu\u00e9 se puede hacer?. La respuesta es simple \u00a1 probar varios modelos sarima !. Ahora, \u00bf c\u00f3mo lo hacemos?. Lo primero es definir una clase llamada SarimaModels que automatice el proceso anterior, y nos quedamos con aquel modelo que minimice alguna de las m\u00e9tricas propuestas, por ejemplo, minimizar las m\u00e9tricas de mae y mape # creando clase SarimaModels class SarimaModels : def __init__ ( self , params ): self . params = params @property def name_model ( self ): return f \"SARIMA_ { self . params [ 0 ] } X { self . params [ 1 ] } \" . replace ( ' ' , '' ) @staticmethod def test_train_model ( y , date ): mask_ds = y . index < date y_train = y [ mask_ds ] y_test = y [ ~ mask_ds ] return y_train , y_test def fit_model ( self , y , date ): y_train , y_test = self . test_train_model ( y , date ) model = SARIMAX ( y_train , order = self . params [ 0 ], seasonal_order = self . params [ 1 ], enforce_stationarity = False , enforce_invertibility = False ) model_fit = model . fit ( disp = 0 ) return model_fit def df_testig ( self , y , date ): y_train , y_test = self . test_train_model ( y , date ) model = SARIMAX ( y_train , order = self . params [ 0 ], seasonal_order = self . params [ 1 ], enforce_stationarity = False , enforce_invertibility = False ) model_fit = model . fit ( disp = 0 ) start_index = y_test . index . min () end_index = y_test . index . max () preds = model_fit . get_prediction ( start = start_index , end = end_index , dynamic = False ) df_temp = pd . DataFrame ( { 'y' : y_test [ 'passengers' ], 'yhat' : preds . predicted_mean } ) return df_temp def metrics ( self , y , date ): df_temp = self . df_testig ( y , date ) df_metrics = summary_metrics ( df_temp ) df_metrics [ 'model' ] = self . name_model return df_metrics # definir parametros import itertools p = d = q = range ( 0 , 2 ) pdq = list ( itertools . product ( p , d , q )) seasonal_pdq = [( x [ 0 ], x [ 1 ], x [ 2 ], 12 ) for x in list ( itertools . product ( p , d , q ))] params = list ( itertools . product ( pdq , seasonal_pdq )) target_date = '1958-01-01' # iterar para los distintos escenarios frames = [] for param in params : try : sarima_model = SarimaModels ( param ) df_metrics = sarima_model . metrics ( y , target_date ) frames . append ( df_metrics ) except : pass # juntar resultados de las m\u00e9tricas y comparar df_metrics_result = pd . concat ( frames ) df_metrics_result . sort_values ([ 'mae' , 'mape' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse mape maape wmape mmape smape model 0 16.4272 406.5114 20.1621 0.0410 0.0409 0.0383 0.0409 0.0398 SARIMA_(0,1,0)X(1,0,0,12) 0 17.7173 469.8340 21.6757 0.0420 0.0419 0.0413 0.0419 0.0414 SARIMA_(0,1,1)X(1,1,1,12) 0 17.7204 480.9764 21.9312 0.0414 0.0414 0.0414 0.0413 0.0411 SARIMA_(1,1,0)X(1,1,1,12) 0 17.8053 501.0603 22.3844 0.0412 0.0411 0.0416 0.0411 0.0410 SARIMA_(1,1,1)X(0,1,0,12) 0 17.8056 505.4167 22.4815 0.0411 0.0410 0.0416 0.0410 0.0409 SARIMA_(0,1,0)X(0,1,0,12) ... ... ... ... ... ... ... ... ... ... 0 94.9444 14674.5556 121.1386 0.1989 0.1933 0.2216 0.1984 0.2320 SARIMA_(0,1,0)X(0,0,0,12) 0 360.7115 150709.0664 388.2127 0.8203 0.6687 0.8418 0.8184 1.5247 SARIMA_(0,0,1)X(0,0,1,12) 0 366.5303 153175.7993 391.3768 0.8370 0.6811 0.8554 0.8351 1.5623 SARIMA_(0,0,0)X(0,0,1,12) 0 422.9626 187068.9748 432.5147 0.9837 0.7745 0.9871 0.9814 1.9589 SARIMA_(0,0,1)X(0,0,0,12) 0 428.5000 189730.5556 435.5807 1.0000 0.7854 1.0000 0.9976 2.0000 SARIMA_(0,0,0)X(0,0,0,12) 64 rows \u00d7 9 columns En este caso el mejor modelo resulta ser el modelo \\(SARIMA(0,1,0)X(1,0,0,12)\\) . Veamos gr\u00e1ficamente que tal el ajuste de este modelo. # ajustar mejor modelo param = [( 0 , 1 , 0 ),( 1 , 0 , 0 , 12 )] sarima_model = SarimaModels ( param ) model_fit = sarima_model . fit_model ( y , target_date ) best_model = sarima_model . df_testig ( y , target_date ) best_model . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } y yhat date 1958-01-31 340 345.765806 1958-02-28 318 330.574552 1958-03-31 362 390.254476 1958-04-30 348 381.573760 1958-05-31 363 389.169386 # graficar mejor modelo preds = best_model [ 'yhat' ] ax = y [ '1949' :] . plot ( label = 'observed' ) preds . plot ( ax = ax , label = 'Forecast' , alpha = .7 , figsize = ( 14 , 7 )) ax . set_xlabel ( 'Date' ) ax . set_ylabel ( 'Passengers' ) plt . legend () plt . show () Para este caso, el mejor modelo encontrado se ajusta bastante bien a los datos. Finalmente, vemos algunos resultados del error asosciado al modelo. Para esto ocupamos la herramienta plot_diagnostics , el cual nos arroja cuatro gr\u00e1ficos analizando el error de diferentes manera. # resultados del error model_fit . plot_diagnostics ( figsize = ( 16 , 8 )) plt . show () gr\u00e1fico 01 (standarized residual): Este gr\u00e1fico nos muestra el error estandarizado en el tiempo. En este caso se observa que esta nueva serie de tiempo corresponde a una serie estacionaria que oscila entorno al cero, es decir, un ruido blanco. gr\u00e1fico 02 (histogram plus estimated density): Este gr\u00e1fico nos muestra el histograma del error. En este caso, el histograma es muy similar al histograma de una variable \\(\\mathcal{N}(0,1)\\) (ruido blanco). gr\u00e1fico 03 (normal QQ): el gr\u00e1fico Q-Q (\"Q\" viene de cuantil) es un m\u00e9todo gr\u00e1fico para el diagn\u00f3stico de diferencias entre la distribuci\u00f3n de probabilidad de una poblaci\u00f3n de la que se ha extra\u00eddo una muestra aleatoria y una distribuci\u00f3n usada para la comparaci\u00f3n. En este caso se comparar la distribuci\u00f3n del error versus una distribuci\u00f3n normal. Cuando mejor es el ajuste lineal sobre los puntos, m\u00e1s parecida es la distribuci\u00f3n entre la muestra obtenida y la distribuci\u00f3n de prueba (distribuci\u00f3n normal). gr\u00e1fico 04 (correlogram): Este gr\u00e1fico nos muestra el gr\u00e1fico de autocorrelaci\u00f3n entre las variables del error, se observa que no hay correlaci\u00f3n entre ninguna de las variables, por lo que se puedan dar indicios de independencia entre las variables. En conclusi\u00f3n, el error asociado al modelo en estudio corresponde a un ruido blanco.","title":"Realizar pron\u00f3stico con statsmodels"},{"location":"lectures/ml/time_series/05_series_temporales/#conclusion","text":"Este fue una introducci\u00f3n amigable a los conceptos claves de series temporales, a medida que m\u00e1s se profundice en la teor\u00eda, mejor ser\u00e1n las t\u00e9cnicas empleadas sobre la serie temporal en fin de obtener el mejor pron\u00f3stico posible. En esta secci\u00f3n nos limitamos a algunos modelos y algunos criterios de verificaci\u00f3n de estacionariedad. En la literatura existen muchas m\u00e1s, pero con los mostrados de momento y un poco de expertice en el tema, se pueden abordar casi todos los problemas de series temporales.","title":"Conclusi\u00f3n"},{"location":"lectures/ml/time_series/05_series_temporales/#referencia","text":"A comprehensive beginner\u2019s guide to create a Time Series Forecast (with Codes in Python and R) A Gentle Introduction to SARIMA for Time Series Forecasting in Python An Introductory Study on Time Series Modeling and Forecasting","title":"Referencia"}]}